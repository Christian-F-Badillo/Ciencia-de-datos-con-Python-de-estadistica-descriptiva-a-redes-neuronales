{
  "hash": "e48bb67f9da1fbb75d1771897da05477",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Redes Neuronales Artificiales en Python\"\nauthor: \"Christian Badillo\"\nformat: html\ndate: today\norder: 3\n---\n\nExisten muchas librerías en Python que permiten implementar redes neuronales artificiales. En este tutorial, vamos a utilizar la librería `scikit-learn`, `keras` y `tensorflow` para implementar una red neuronal artificial para resolver problemas de clasificación y regresión.\n\n## `scikit-learn`\n\n`scikit-learn` es una librería de Python que permite implementar algoritmos de aprendizaje supervisado y no supervisado. En particular, `scikit-learn` incluye una clase llamada `MLPClassifier` que permite implementar redes neuronales artificiales para problemas de clasificación.\n\n### Ejemplo de clasificación\n\nEn este ejemplo, vamos a utilizar la base de datos `iris` para entrenar una red neuronal artificial que permita clasificar las flores en tres categorías: `setosa`, `versicolor` y `virginica`.\n\n::: {#f6fe799c .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\n\n# Cargar la base de datos iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\nprint(iris.target_names)\nprint(iris.feature_names)\nprint(X.shape)\nprint(y.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['setosa' 'versicolor' 'virginica']\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n(150, 4)\n(150,)\n```\n:::\n:::\n\n\nImplementamos la red neuronal artificial y la entrenamos con la base de datos `iris`.\n\n::: {#74ef1f81 .cell execution_count=2}\n``` {.python .cell-code}\n# Dividir la base de datos en entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Crear la red neuronal artificial\nclf = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42, \n                    activation='relu', solver='sgd', learning_rate='adaptive', \n                    learning_rate_init=0.001)\n\n# Entrenar la red neuronal artificial\nclf.fit(X_train, y_train)\n\n# Evaluar la red neuronal artificial\nscore = clf.score(X_test, y_test)\nprint(score)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9333333333333333\n```\n:::\n:::\n\n\nLa clase `MLPClassifier` tiene varios parámetros que permiten configurar la red neuronal artificial. En este ejemplo, utilizamos los siguientes parámetros:\n\n- `hidden_layer_sizes`: Número de neuronas en cada capa oculta.\n- `max_iter`: Número máximo de iteraciones para entrenar la red neuronal artificial.\n- `random_state`: Semilla para la generación de números aleatorios.\n- `solver`: Algoritmo de optimización para entrenar la red neuronal artificial.\n- `activation`: Función de activación para las neuronas en las capas ocultas.\n- `learning_rate`: Tasa de aprendizaje para actualizar los pesos de la red neuronal artificial.\n- `learning_rate_init`: Tasa de aprendizaje inicial para actualizar los pesos de la red neuronal artificial.\n\nEl método `score` permite evaluar la precisión de la red neuronal artificial en la base de datos de prueba.\n\nPodemos obtener las predicciones de la red neuronal artificial para la base de datos de prueba.\n\n::: {#998761c6 .cell execution_count=3}\n``` {.python .cell-code}\n# Obtener las predicciones de la red neuronal artificial\ny_pred = clf.predict(X_test)\nprint(y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 0 2 1 1 0 1 2 2 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 1 2 2 2 0 0]\n```\n:::\n:::\n\n\nPara evaluar la red neuronal artificial, podemos calcular la matriz de confusión.\n\n::: {#07167829 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\n\n# Calcular la matriz de confusión\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[10  0  0]\n [ 0  8  1]\n [ 0  1 10]]\n```\n:::\n:::\n\n\nPodemos visualizar la matriz de confusión utilizando la librería `seaborn`.\n\n::: {#02c2daa2 .cell execution_count=5}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Visualizar la matriz de confusión\nsns.heatmap(cm, annot=True, fmt='d', \n            xticklabels=iris.target_names, \n            yticklabels=iris.target_names,\n            cmap='Blues')\nplt.xlabel('Predicción')\nplt.ylabel('Real')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ann_python_files/figure-html/cell-6-output-1.png){width=547 height=434}\n:::\n:::\n\n\nCon este mapa de color podemos visualizar la matriz de confusión. Las filas representan las categorías reales y las columnas representan las categorías predichas. Los valores en la diagonal principal representan las predicciones correctas y esperamos que un buen modelo tenga valores altos en la diagonal principal.\n\nPOdemos usar el área bajo la curva ROC para evaluar la red neuronal artificial.\n\n::: {#11ce3ee5 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_auc_score\n\n# Calcular el área bajo la curva ROC\ny_prob = clf.predict_proba(X_test)\nroc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\nprint(roc_auc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9966414352379265\n```\n:::\n:::\n\n\nAquí el parámetro `multi_class` se refiere a la estrategia de codificación de clases. En este caso, utilizamos `ovr` que significa \"one-vs-rest\", que evalúa cada clase en comparación con el resto de las clases y es sensible a datos desequilibrados.\n\nEntre más cercano a 1 sea el valor del área bajo la curva ROC, mejor será el modelo, ya que significa que el modelo es capaz de distinguir entre las diferentes clases.\n\n## `PyTorch`\n\n`PyTorch` es una librería de Python que permite implementar redes neuronales artificiales de manera eficiente. En particular, `PyTorch` incluye una clase llamada `nn.Module` que permite definir la arquitectura de la red neuronal artificial.\n\n### Uso básico de PyTorch\n\nPara utilizar `PyTorch`, primero debemos instalar la librería, el comando que recomienda la página de `PyTorch` es el siguiente para Windows y sin el uso de GPU.\n\n\n```{bash}\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n```\n\n\n#### Tensores\n\nEn `PyTorch`, los datos se almacenan en tensores, que son arreglos multidimensionales similares a los arreglos de `NumPy`. Podemos crear tensores en `PyTorch` de la siguiente manera.\n\n::: {#c1998f10 .cell execution_count=7}\n``` {.python .cell-code}\nimport torch\n\n# Crear un tensor de ceros\nx = torch.zeros(2, 3)\n\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n```\n:::\n:::\n\n\nPodemos crear tensores a partir de arreglos de `NumPy`.\n\n::: {#576a56e5 .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\n\n# Crear un arreglo de NumPy\narr = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Crear un tensor de PyTorch a partir de un arreglo de NumPy\nx = torch.tensor(arr)\n\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n```\n:::\n:::\n\n\nLos tensores pueden ser de diferentes tipos de datos, como `float32`, `int64`, `bool`, etc.\n\n::: {#bb213812 .cell execution_count=9}\n``` {.python .cell-code}\n# Crear un tensor de enteros\nx = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int64)\n\nprint(x)\n\n# Crear un tensor de booleanos\nx = torch.tensor([[True, False], [False, True]])\n\nprint(x)\n\n# Crear un tensor de punto flotante\nx = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)\n\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntensor([[ True, False],\n        [False,  True]])\ntensor([[1., 2.],\n        [3., 4.]])\n```\n:::\n:::\n\n\nPodemos tener tensores de cualquier dimensión.\n\n::: {#91c8aa28 .cell execution_count=10}\n``` {.python .cell-code}\n# Crear un tensor de 3 dimensiones\nx = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[[1, 2],\n         [3, 4]],\n\n        [[5, 6],\n         [7, 8]]])\n```\n:::\n:::\n\n\n#### Operaciones con tensores\n\nPodemos realizar operaciones matemáticas con tensores en `PyTorch`.\n\n::: {#b181b320 .cell execution_count=11}\n``` {.python .cell-code}\n# Crear dos tensores\nx = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\ny = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n\n# Sumar los tensores\nz = x + y\n\nprint(z)\n\n# Multiplicar los tensores\nz = x * y\nprint(z)\n\n# Multiplicar un tensor por un escalar\nz = 2 * x\nprint(z)\n\n# Calcular la transpuesta de un tensor\nz = x.t()\nprint(z)\n\n# Calcular el producto punto de dos tensores\nz = torch.dot(x.view(-1), y.view(-1))\nprint(z)\n\n# Calcular el producto matricial de dos tensores\nz = torch.mm(x, y)\n\nprint(z)\n\n# Calcular la inversa de un tensor\nz = torch.inverse(x)\n\nprint(z)\n\n# Calcular la norma de un tensor\nz = torch.norm(x)\n\nprint(z)\n\n# Calcular la media de un tensor\nz = torch.mean(x)\n\nprint(z)\n\n# Calcular la desviación estándar de un tensor\nz = torch.std(x)\n\nprint(z)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[ 6.,  8.],\n        [10., 12.]])\ntensor([[ 5., 12.],\n        [21., 32.]])\ntensor([[2., 4.],\n        [6., 8.]])\ntensor([[1., 3.],\n        [2., 4.]])\ntensor(70.)\ntensor([[19., 22.],\n        [43., 50.]])\ntensor([[-2.0000,  1.0000],\n        [ 1.5000, -0.5000]])\ntensor(5.4772)\ntensor(2.5000)\ntensor(1.2910)\n```\n:::\n:::\n\n\n#### Métodos de tensores\n\nLos tensores en `PyTorch` tienen varios métodos útiles.\n\n::: {#fb9d56a8 .cell execution_count=12}\n``` {.python .cell-code}\n# Crear un tensor de 3 dimensiones\nx = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n# Obtener la forma del tensor\nprint(f\"Shape: {x.shape}\")\n\n# Obtener el número de elementos en el tensor\nprint(f\"Size: {x.size()}\")\n\n# Obtener el tipo de datos del tensor\nprint(f\"Data type: {x.dtype}\")\n\n# Obtener el dispositivo en el que se almacena el tensor\nprint(f\"Device: {x.device}\")\n\n# Obtener el número de dimensiones del tensor\nprint(f\"Number of dimensions: {x.dim()}\")\n\n# Obtener el número de elementos en una dimensión específica\nprint(f\"Number of elements in the first dimension: {x.size(0)}\")\n\n# Obtener el elemento en una posición específica\nprint(f\"Element at position (0, 1, 1): {x[0, 1, 1]}\")\n\n# Obtener un subtensor\nprint(f\"Subtensor: {x[0, :, :]}\")\n\n# Cambiar la forma del tensor\nprint(f\"Reshape: {x.view(2, 4)}\")\n\n# Aplanar el tensor\nprint(f\"Flatten: {x.view(-1)}\")\n\n# Concatenar tensores\ny = torch.tensor([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\nz = torch.cat((x, y), dim=0)\n\nprint(z)\n\n# Dividir un tensor\nz1, z2 = torch.chunk(z, 2, dim=0)\n\nprint(f\"z1: {z1}\")\nprint(f\"z2: {z2}\")\n\n# Calcular la suma acumulada de un tensor\nz = torch.cumsum(x, dim=0)\n\nprint(f\"Cumulative sum: {z}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape: torch.Size([2, 2, 2])\nSize: torch.Size([2, 2, 2])\nData type: torch.int64\nDevice: cpu\nNumber of dimensions: 3\nNumber of elements in the first dimension: 2\nElement at position (0, 1, 1): 4\nSubtensor: tensor([[1, 2],\n        [3, 4]])\nReshape: tensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])\nFlatten: tensor([1, 2, 3, 4, 5, 6, 7, 8])\ntensor([[[ 1,  2],\n         [ 3,  4]],\n\n        [[ 5,  6],\n         [ 7,  8]],\n\n        [[ 9, 10],\n         [11, 12]],\n\n        [[13, 14],\n         [15, 16]]])\nz1: tensor([[[1, 2],\n         [3, 4]],\n\n        [[5, 6],\n         [7, 8]]])\nz2: tensor([[[ 9, 10],\n         [11, 12]],\n\n        [[13, 14],\n         [15, 16]]])\nCumulative sum: tensor([[[ 1,  2],\n         [ 3,  4]],\n\n        [[ 6,  8],\n         [10, 12]]])\n```\n:::\n:::\n\n\n### Estructura de una red neuronal artificial en PyTorch\n\nVeamos las distintas partes de una red neuronal artificial en `PyTorch`.\n\n#### Ejemplo de clasificación\n\nEn este ejemplo, vamos a utilizar la base de datos `iris` para entrenar una red neuronal artificial que permita clasificar las flores en tres categorías: `setosa`, `versicolor` y `virginica`.\n\n::: {#90bce513 .cell execution_count=13}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\n# Crear la red neuronal artificial\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(4, 10)\n        self.fc2 = nn.Linear(10, 10)\n        self.fc3 = nn.Linear(10, 3)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        x = self.softmax(x)\n        return x\n\nmodel = MLP()\n\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMLP(\n  (fc1): Linear(in_features=4, out_features=10, bias=True)\n  (fc2): Linear(in_features=10, out_features=10, bias=True)\n  (fc3): Linear(in_features=10, out_features=3, bias=True)\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n)\n```\n:::\n:::\n\n\nPara crear la red neuronal artificial, definimos una clase llamada `MLP` que hereda de `nn.Module`. En el método `__init__`, definimos las capas de la red neuronal artificial y las funciones de activación. En el método `forward`, definimos la arquitectura de la red neuronal artificial.\n\nDentro del método `__init__`, usamos la función `super` para inicializar la clase base `nn.Module`. Luego, definimos las capas de la red neuronal artificial utilizando la clase `nn.Linear` que representa una capa de neuronas completamente conectada. En este caso, definimos tres capas de neuronas con 4, 10 y 3 neuronas respectivamente.\n\nTambién definimos las funciones de activación `nn.ReLU` y `nn.Softmax` para las capas ocultas y de salida respectivamente. La función de activación `ReLU` se utiliza para introducir no linealidades en la red neuronal artificial, mientras que la función de activación `Softmax` se utiliza para obtener probabilidades de las clases y recive como argumento la dimensión en la que se calcula el softmax.\n\nEn el método `forward`, definimos la arquitectura de la red neuronal artificial. Primero aplicamos la capa `fc1` seguida de la función de activación `ReLU`. Luego aplicamos la capa `fc2` seguida de la función de activación `ReLU`. Finalmente aplicamos la capa `fc3` seguida de la función de activación `Softmax`.\n\nLa función `Linear` aplicará una transformación lineal a los datos de entrada: $y = xA^T + b$, donde $x$ es la entrada, $A$ es la matriz de pesos y $b$ es el vector de sesgos.\n\nEs importante que las neuronas entre capas tengan la misma cantidad de neuronas que la capa anterior y la capa siguiente. En este caso, la capa de entrada tiene 4 neuronas, la capa oculta tiene 10 neuronas y la capa de salida tiene 3 neuronas.\n\nVamos a usar la base de datos `iris` para ver que resultados obtenemos.\n\n::: {#49672b2e .cell execution_count=14}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Cargar la base de datos iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\nmodel = MLP()\n\n# Convertir los datos a tensores de PyTorch\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.long)\n\nprint(X.shape)\nprint(y.shape)\n\n# Obtener las predicciones de la red neuronal artificial\ny_pred = model(X)\n\nprint(y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([150, 4])\ntorch.Size([150])\ntensor([[0.3815, 0.2707, 0.3478],\n        [0.3707, 0.2763, 0.3529],\n        [0.3767, 0.2748, 0.3485],\n        [0.3761, 0.2767, 0.3472],\n        [0.3848, 0.2700, 0.3452],\n        [0.3857, 0.2675, 0.3468],\n        [0.3805, 0.2740, 0.3455],\n        [0.3804, 0.2723, 0.3473],\n        [0.3722, 0.2792, 0.3486],\n        [0.3762, 0.2751, 0.3487],\n        [0.3846, 0.2679, 0.3474],\n        [0.3825, 0.2732, 0.3442],\n        [0.3740, 0.2762, 0.3498],\n        [0.3763, 0.2772, 0.3465],\n        [0.3872, 0.2628, 0.3499],\n        [0.3947, 0.2610, 0.3442],\n        [0.3835, 0.2665, 0.3500],\n        [0.3790, 0.2712, 0.3498],\n        [0.3833, 0.2669, 0.3498],\n        [0.3871, 0.2685, 0.3445],\n        [0.3782, 0.2714, 0.3503],\n        [0.3820, 0.2700, 0.3479],\n        [0.3859, 0.2704, 0.3438],\n        [0.3707, 0.2750, 0.3542],\n        [0.3841, 0.2740, 0.3418],\n        [0.3710, 0.2765, 0.3525],\n        [0.3759, 0.2736, 0.3505],\n        [0.3813, 0.2706, 0.3481],\n        [0.3782, 0.2713, 0.3505],\n        [0.3783, 0.2756, 0.3461],\n        [0.3751, 0.2763, 0.3487],\n        [0.3722, 0.2719, 0.3559],\n        [0.3988, 0.2640, 0.3371],\n        [0.3958, 0.2623, 0.3418],\n        [0.3737, 0.2756, 0.3506],\n        [0.3738, 0.2735, 0.3527],\n        [0.3777, 0.2690, 0.3532],\n        [0.3881, 0.2698, 0.3420],\n        [0.3741, 0.2779, 0.3480],\n        [0.3796, 0.2719, 0.3485],\n        [0.3793, 0.2713, 0.3494],\n        [0.3538, 0.2849, 0.3613],\n        [0.3791, 0.2759, 0.3450],\n        [0.3735, 0.2737, 0.3529],\n        [0.3867, 0.2701, 0.3432],\n        [0.3691, 0.2772, 0.3537],\n        [0.3901, 0.2682, 0.3417],\n        [0.3781, 0.2754, 0.3465],\n        [0.3854, 0.2683, 0.3463],\n        [0.3773, 0.2730, 0.3496],\n        [0.3250, 0.2925, 0.3825],\n        [0.3263, 0.2936, 0.3801],\n        [0.3203, 0.2957, 0.3840],\n        [0.3146, 0.3016, 0.3837],\n        [0.3154, 0.3000, 0.3847],\n        [0.3272, 0.2929, 0.3800],\n        [0.3270, 0.2936, 0.3794],\n        [0.3322, 0.2924, 0.3754],\n        [0.3230, 0.2938, 0.3832],\n        [0.3244, 0.2970, 0.3785],\n        [0.3196, 0.2988, 0.3816],\n        [0.3244, 0.2961, 0.3795],\n        [0.3173, 0.2974, 0.3853],\n        [0.3239, 0.2944, 0.3816],\n        [0.3294, 0.2935, 0.3772],\n        [0.3242, 0.2940, 0.3818],\n        [0.3271, 0.2943, 0.3786],\n        [0.3342, 0.2883, 0.3776],\n        [0.3119, 0.3071, 0.3809],\n        [0.3256, 0.2940, 0.3803],\n        [0.3213, 0.2986, 0.3800],\n        [0.3233, 0.2955, 0.3811],\n        [0.3136, 0.3026, 0.3839],\n        [0.3275, 0.2911, 0.3814],\n        [0.3241, 0.2940, 0.3818],\n        [0.3223, 0.2953, 0.3824],\n        [0.3160, 0.2979, 0.3861],\n        [0.3157, 0.3008, 0.3834],\n        [0.3213, 0.2972, 0.3815],\n        [0.3307, 0.2913, 0.3780],\n        [0.3234, 0.2956, 0.3810],\n        [0.3272, 0.2933, 0.3795],\n        [0.3259, 0.2940, 0.3801],\n        [0.3147, 0.3010, 0.3843],\n        [0.3287, 0.2937, 0.3776],\n        [0.3317, 0.2916, 0.3767],\n        [0.3216, 0.2956, 0.3828],\n        [0.3131, 0.3023, 0.3846],\n        [0.3331, 0.2904, 0.3765],\n        [0.3200, 0.2984, 0.3816],\n        [0.3268, 0.2932, 0.3801],\n        [0.3265, 0.2931, 0.3804],\n        [0.3234, 0.2953, 0.3813],\n        [0.3282, 0.2944, 0.3774],\n        [0.3248, 0.2950, 0.3802],\n        [0.3364, 0.2879, 0.3757],\n        [0.3294, 0.2921, 0.3784],\n        [0.3257, 0.2934, 0.3809],\n        [0.3285, 0.2949, 0.3766],\n        [0.3266, 0.2940, 0.3794],\n        [0.3149, 0.3109, 0.3742],\n        [0.3139, 0.3066, 0.3794],\n        [0.3139, 0.3074, 0.3787],\n        [0.3150, 0.3018, 0.3833],\n        [0.3141, 0.3085, 0.3774],\n        [0.3133, 0.3066, 0.3801],\n        [0.3142, 0.3049, 0.3809],\n        [0.3139, 0.3020, 0.3841],\n        [0.3121, 0.3071, 0.3808],\n        [0.3163, 0.3076, 0.3762],\n        [0.3162, 0.3037, 0.3801],\n        [0.3134, 0.3073, 0.3793],\n        [0.3143, 0.3078, 0.3779],\n        [0.3125, 0.3112, 0.3763],\n        [0.3129, 0.3155, 0.3716],\n        [0.3153, 0.3091, 0.3757],\n        [0.3154, 0.3012, 0.3834],\n        [0.3179, 0.2985, 0.3836],\n        [0.3100, 0.3151, 0.3749],\n        [0.3120, 0.3055, 0.3826],\n        [0.3147, 0.3090, 0.3762],\n        [0.3145, 0.3075, 0.3780],\n        [0.3123, 0.3070, 0.3807],\n        [0.3139, 0.3061, 0.3800],\n        [0.3162, 0.3033, 0.3805],\n        [0.3160, 0.2988, 0.3852],\n        [0.3146, 0.3049, 0.3805],\n        [0.3159, 0.3019, 0.3821],\n        [0.3133, 0.3093, 0.3774],\n        [0.3154, 0.2978, 0.3868],\n        [0.3130, 0.3059, 0.3811],\n        [0.3184, 0.2957, 0.3859],\n        [0.3130, 0.3113, 0.3757],\n        [0.3172, 0.2980, 0.3848],\n        [0.3172, 0.2967, 0.3861],\n        [0.3127, 0.3121, 0.3752],\n        [0.3162, 0.3076, 0.3762],\n        [0.3161, 0.2998, 0.3841],\n        [0.3160, 0.3020, 0.3820],\n        [0.3148, 0.3070, 0.3782],\n        [0.3140, 0.3122, 0.3739],\n        [0.3143, 0.3118, 0.3739],\n        [0.3139, 0.3066, 0.3794],\n        [0.3148, 0.3083, 0.3769],\n        [0.3149, 0.3114, 0.3737],\n        [0.3138, 0.3124, 0.3738],\n        [0.3123, 0.3104, 0.3773],\n        [0.3149, 0.3060, 0.3791],\n        [0.3167, 0.3059, 0.3775],\n        [0.3164, 0.3010, 0.3826]], grad_fn=<SoftmaxBackward0>)\n```\n:::\n:::\n\n\nQue nos regresa un tensor con las probabilidades de cada clase para cada observación. Para obtener la clase predicha, podemos usar la función `argmax` de PyTorch.\n\n::: {#7867662a .cell execution_count=15}\n``` {.python .cell-code}\n# Obtener la clase predicha\ny_pred = torch.argmax(y_pred, dim=1)\n\nprint(y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n        0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2])\n```\n:::\n:::\n\n\n#### Funciones de activación en PyTorch\n\nEn `PyTorch`, las funciones de activación se pueden utilizar como capas en una red neuronal artificial. Las funciones de activación más comunes son `ReLU`, `Sigmoid` y `Softmax`.\n\n::: {#50f7f92d .cell execution_count=16}\n``` {.python .cell-code}\nimport torch\n\n# Crear un tensor de entrada\nx = torch.tensor([[1.0, -1.0], [-1.0, 1.0]])\n\n# Función de activación ReLU\nrelu = torch.nn.ReLU()\ny = relu(x)\n\nprint(f\"ReLU: {y}\")\n\n# Función de activación Sigmoid\nsigmoid = torch.nn.Sigmoid()\ny = sigmoid(x)\n\nprint(f\"Sigmoid: {y}\")\n\n# Función de activación Softmax\nsoftmax = torch.nn.Softmax(dim=1)\ny = softmax(x)\n\nprint(f\"Softmax: {y}\")\n\n# Función de activación Tanh\ntanh = torch.nn.Tanh()\ny = tanh(x)\n\nprint(f\"Tanh: {y}\")\n\n# Función de activación LeakyReLU\nleaky_relu = torch.nn.LeakyReLU(negative_slope=0.01)\ny = leaky_relu(x)\n\nprint(f\"LeakyReLU: {y}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReLU: tensor([[1., 0.],\n        [0., 1.]])\nSigmoid: tensor([[0.7311, 0.2689],\n        [0.2689, 0.7311]])\nSoftmax: tensor([[0.8808, 0.1192],\n        [0.1192, 0.8808]])\nTanh: tensor([[ 0.7616, -0.7616],\n        [-0.7616,  0.7616]])\nLeakyReLU: tensor([[ 1.0000, -0.0100],\n        [-0.0100,  1.0000]])\n```\n:::\n:::\n\n\n### Entrenamiento de la red neuronal\n\nPara entrenar la red neuronal artificial, primero debemos definir una función de pérdida y un optimizador.\n\n#### Función de pérdida\n\nLa función de pérdida mide la diferencia entre las predicciones de la red neuronal artificial y las etiquetas reales. Existen diversas funciones de pérdida para problemas de clasificación y regresión.\n\n- Funciones de pérdida para problemas de clasificación:\n    - `nn.CrossEntropyLoss`: Utilizada para problemas de clasificación multiclase.\n    - `nn.BCELoss`: Utilizada para problemas de clasificación binaria.\n    - `nn.NLLLoss`: Utilizada para problemas de clasificación multiclase con salida logarítmica.\n- Funciones de pérdida para problemas de regresión:\n    - `nn.MSELoss`: Utilizada para problemas de regresión de mínimos cuadrados.\n    - `nn.L1Loss`: Utilizada para problemas de regresión de mínimos absolutos.\n    - `nn.SmoothL1Loss`: Utilizada para problemas de regresión de mínimos suavizados.\n\n::: {#9b1ab40e .cell execution_count=17}\n``` {.python .cell-code}\nimport torch.nn as nn\n\ny_pred = torch.tensor([[0.1, 0.2, 0.7], [0.8, 0.1, 0.1], [0.2, 0.6, 0.2]])\ny = torch.tensor([2, 0, 1])\n\n# Crear la función de pérdida\ncriterion = nn.CrossEntropyLoss()\n\n# Calcular la pérdida\nloss = criterion(y_pred, y)\n\nprint(loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor(0.7694)\n```\n:::\n:::\n\n\nNos regresa el error de la red neuronal. Para minimizar la función de pérdida, utilizamos un optimizador.\n\n#### Optimizador\n\nEl optimizador ajusta los pesos de la red neuronal artificial para minimizar la función de pérdida. Existen diversos optimizadores que se pueden utilizar para entrenar una red neuronal artificial.\n\n- Optimizadores basados en gradiente:\n    - `torch.optim.SGD`: Descenso de gradiente estocástico.\n    - `torch.optim.Adam`: Algoritmo de optimización basado en el método de Adam.\n    - `torch.optim.RMSprop`: Algoritmo de optimización basado en el método de RMSprop.\n\nCadad optimizador tiene sus propios hiperparámetros que se pueden ajustar para mejorar el rendimiento de la red neuronal artificial.\n\n::: {#1cdfc55a .cell execution_count=18}\n``` {.python .cell-code}\nimport torch.optim as optim\n\n# Crear el optimizador\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Realizar la retropropagación\noptimizer.zero_grad()\n\n# loss.backward()\n\n# Actualizar los pesos\noptimizer.step()\n```\n:::\n\n\nEn este caso, utilizamos el optimizador `SGD` con una tasa de aprendizaje de 0.01. Primero, llamamos al método `zero_grad` para restablecer los gradientes de los pesos de la red neuronal artificial. Luego, llamamos al método `backward` para calcular los gradientes de la función de pérdida con respecto a los pesos. Finalmente, llamamos al método `step` para actualizar los pesos de la red neuronal artificial utilizando el algoritmo de optimización.\n\nEstos pasos se repiten varias veces para entrenar la red neuronal artificial en un conjunto de datos.\n\n### Preparación de los datos\n\nPara entrenar una red neuronal artificial, primero debemos preparar los datos en tensores de PyTorch.\n\n::: {#c66894fb .cell execution_count=19}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Cargar la base de datos iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Dividir la base de datos en entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1014)\n\n# Convertir los datos a tensores de PyTorch\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.long)\ny_test = torch.tensor(y_test, dtype=torch.long)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([120, 4])\ntorch.Size([30, 4])\ntorch.Size([120])\ntorch.Size([30])\n```\n:::\n:::\n\n\nLos datos tipo `torch.long` es equivalente a `int64` en `NumPy` y se utiliza para representar las etiquetas de las clases. \n\n\n### Red Neuronal con PyTorch\n\nVamos a definir una red neuronal de 2 capas ocultas con 4 neuronas cada capa y una capa de salida con 3 neuronas.\n\n::: {#ffda0bcc .cell execution_count=20}\n``` {.python .cell-code}\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(4, 4)\n        self.fc2 = nn.Linear(4, 4)\n        self.fc3 = nn.Linear(4, 3)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        x = self.softmax(x)\n        return x\n    \n    def predict(self, x):\n        with torch.no_grad():\n            y = self.forward(x)\n            return torch.argmax(y, dim=1)\n    \nmodel = NeuralNetwork()\n\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNeuralNetwork(\n  (fc1): Linear(in_features=4, out_features=4, bias=True)\n  (fc2): Linear(in_features=4, out_features=4, bias=True)\n  (fc3): Linear(in_features=4, out_features=3, bias=True)\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n)\n```\n:::\n:::\n\n\nAhora creemos una función para entrenar la red neuronal.\n\n::: {#46635f26 .cell execution_count=21}\n``` {.python .cell-code}\ndef train(model, X_train, y_train, criterion, optimizer, epochs):\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        y_pred = model(X_train)\n        loss = criterion(y_pred, y_train)\n        loss.backward()\n        optimizer.step()\n        \n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n```\n:::\n\n\nAhora vamos a entrenar la red neuronal.\n\n::: {#0da59443 .cell execution_count=22}\n``` {.python .cell-code}\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\ntrain(model, X_train, y_train, criterion, optimizer, epochs=1000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 50/1000, Loss: 1.0754790306091309\nEpoch 100/1000, Loss: 1.07102370262146\nEpoch 150/1000, Loss: 1.067678689956665\nEpoch 200/1000, Loss: 1.0643763542175293\nEpoch 250/1000, Loss: 1.0608757734298706\nEpoch 300/1000, Loss: 1.0570424795150757\nEpoch 350/1000, Loss: 1.052795648574829\nEpoch 400/1000, Loss: 1.0484215021133423\nEpoch 450/1000, Loss: 1.043760895729065\nEpoch 500/1000, Loss: 1.0385349988937378\nEpoch 550/1000, Loss: 1.0325710773468018\nEpoch 600/1000, Loss: 1.0256762504577637\nEpoch 650/1000, Loss: 1.0176128149032593\nEpoch 700/1000, Loss: 1.0081112384796143\nEpoch 750/1000, Loss: 0.9968506693840027\nEpoch 800/1000, Loss: 0.9835156798362732\nEpoch 850/1000, Loss: 0.9678962826728821\nEpoch 900/1000, Loss: 0.9499657154083252\nEpoch 950/1000, Loss: 0.9237939119338989\nEpoch 1000/1000, Loss: 0.9007318615913391\n```\n:::\n:::\n\n\nFinalmente, vamos a evaluar la red neuronal en la base de datos de prueba.\n\n::: {#eb998cc5 .cell execution_count=23}\n``` {.python .cell-code}\ny_pred = model.predict(X_test)\n\nprint(y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2,\n        0, 2, 2, 0, 2, 2])\n```\n:::\n:::\n\n\nCalculamos la precisión de la red neuronal.\n\n::: {#84307d2b .cell execution_count=24}\n``` {.python .cell-code}\naccuracy = torch.sum(y_pred == y_test).item() / len(y_test)\n\nprint(f\"Accuracy: {accuracy}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.5\n```\n:::\n:::\n\n\nComo vemos, la red tiene un precisión muy baja, esto se debe a que la red neuronal es muy compleja para el problema que estamos tratando de resolver. Dismunuyamos la complejidad de la red neuronal.\n\n::: {#90e67930 .cell execution_count=25}\n``` {.python .cell-code}\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(4, 2)\n        self.fc2 = nn.Linear(2, 3)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n    \n    def predict(self, x):\n        with torch.no_grad():\n            y = self.forward(x)\n            return torch.argmax(y, dim=1)\n\nmodel = NeuralNetwork()\n\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNeuralNetwork(\n  (fc1): Linear(in_features=4, out_features=2, bias=True)\n  (fc2): Linear(in_features=2, out_features=3, bias=True)\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n)\n```\n:::\n:::\n\n\nEntrenamos la red neuronal.\n\n::: {#26a6895a .cell execution_count=26}\n``` {.python .cell-code}\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\ntrain(model, X_train, y_train, criterion, optimizer, epochs=1500)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 50/1500, Loss: 1.0786900520324707\nEpoch 100/1500, Loss: 1.0306339263916016\nEpoch 150/1500, Loss: 0.9199546575546265\nEpoch 200/1500, Loss: 0.8805767297744751\nEpoch 250/1500, Loss: 0.8552428483963013\nEpoch 300/1500, Loss: 0.8381394147872925\nEpoch 350/1500, Loss: 0.8256418704986572\nEpoch 400/1500, Loss: 0.8157903552055359\nEpoch 450/1500, Loss: 0.8073647022247314\nEpoch 500/1500, Loss: 0.7996431589126587\nEpoch 550/1500, Loss: 0.792230486869812\nEpoch 600/1500, Loss: 0.7848939299583435\nEpoch 650/1500, Loss: 0.7775012850761414\nEpoch 700/1500, Loss: 0.7699931859970093\nEpoch 750/1500, Loss: 0.7623674273490906\nEpoch 800/1500, Loss: 0.7546666860580444\nEpoch 850/1500, Loss: 0.7469660043716431\nEpoch 900/1500, Loss: 0.7393587231636047\nEpoch 950/1500, Loss: 0.7319421172142029\nEpoch 1000/1500, Loss: 0.7248038053512573\nEpoch 1050/1500, Loss: 0.7180129289627075\nEpoch 1100/1500, Loss: 0.711614727973938\nEpoch 1150/1500, Loss: 0.7056317925453186\nEpoch 1200/1500, Loss: 0.7000667452812195\nEpoch 1250/1500, Loss: 0.6949079632759094\nEpoch 1300/1500, Loss: 0.6901333332061768\nEpoch 1350/1500, Loss: 0.6857159733772278\nEpoch 1400/1500, Loss: 0.6816268563270569\nEpoch 1450/1500, Loss: 0.6778366565704346\nEpoch 1500/1500, Loss: 0.6743176579475403\n```\n:::\n:::\n\n\nEvaluamos la red neuronal.\n\n::: {#d8676d00 .cell execution_count=27}\n``` {.python .cell-code}\ny_pred = model.predict(X_test)\n\naccuracy = torch.sum(y_pred == y_test).item() / len(y_test)\n\nprint(f\"Accuracy: {accuracy}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.9666666666666667\n```\n:::\n:::\n\n\nEsto mejora la precisión de la red neuronal. Podemos visualizar la matriz de confusión.\n\n::: {#4c67a58b .cell execution_count=28}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, y_pred)\n\nsns.heatmap(cm, annot=True, fmt='d', \n            xticklabels=iris.target_names, \n            yticklabels=iris.target_names,\n            cmap='Blues')\n```\n\n::: {.cell-output .cell-output-display}\n![](ann_python_files/figure-html/cell-29-output-1.png){width=529 height=416}\n:::\n:::\n\n\n# Aspectos importantes\n\nLa inicialización de los pesos de la red neuronal puede afectar significativamente el rendimiento del modelo. En general, es importante inicializar los pesos de la red neuronal de manera que no se saturen las neuronas y se evite el problema del desvanecimiento o explosión del gradiente. Existen diversas técnicas de inicialización de pesos que se pueden utilizar para mejorar el rendimiento de la red neuronal artificial. Una de ellas es la **inicialización de Xavier**, que ajusta los pesos de la red neuronal de manera que la varianza de las salidas de las neuronas sea igual a la varianza de las entradas de las neuronas.\n\nOtro aspecto importante es la **regularización de la red neuronal**, que se utiliza para evitar el sobreajuste de la red neuronal a los datos de entrenamiento. Existen diversas técnicas de regularización que se pueden utilizar para mejorar el rendimiento de la red neuronal artificial. Una de ellas es la **regularización L2**, que penaliza los pesos de la red neuronal para evitar que se vuelvan demasiado grandes.\n\nAdemás, es importante ajustar los hiperparámetros de la red neuronal, como el número de capas ocultas, el número de neuronas en cada capa oculta, la tasa de aprendizaje, el número de épocas de entrenamiento, etc. para mejorar el rendimiento de la red neuronal artificial.\n\nEs importante la escala de los datos, ya que si los datos no están normalizados, la red neuronal puede tener dificultades para converger. Por lo tanto, es importante normalizar los datos antes de entrenar la red neuronal artificial.\n\nExisten diversas formas de normalizar los datos, como la normalización min-max, la normalización z-score, la normalización por rango, etc. En python, podemos utilizar la librería `scikit-learn` para normalizar los datos.\n\n::: {#3f5c77fb .cell execution_count=29}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\n# Normalizar los datos utilizando la normalización min-max\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ndf = pd.DataFrame(X_train, columns=iris.feature_names)\nprint(df.head())\n\n# Normalizar los datos utilizando la normalización z-score\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ndf = pd.DataFrame(X_train, columns=iris.feature_names)\nprint(df.head())\n\n# Normalizar los datos utilizando la normalización por rango\nscaler = RobustScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ndf = pd.DataFrame(X_train, columns=iris.feature_names)\nprint(df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0           0.250000          0.590909           0.084746          0.041667\n1           0.583333          0.454545           0.593220          0.583333\n2           0.472222          0.545455           0.593220          0.625000\n3           0.500000          0.318182           0.627119          0.541667\n4           0.388889          0.363636           0.542373          0.458333\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0          -0.813744          1.000766          -1.257121         -1.298710\n1           0.611793          0.288759           0.384982          0.359218\n2           0.136614          0.763431           0.384982          0.486751\n3           0.255408         -0.423249           0.494455          0.231685\n4          -0.219770         -0.185913           0.220772         -0.023381\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0          -0.461539          0.833333          -0.805369         -0.786885\n1           0.461538          0.333333           0.000000          0.065574\n2           0.153846          0.666667           0.000000          0.131148\n3           0.230769         -0.166666           0.053691          0.000000\n4          -0.076923          0.000000          -0.080537         -0.131147\n```\n:::\n:::\n\n\nEs importante realizar este proceso **después** de haber dividido los datos en entrenamiento y prueba, para evitar el sesgo en la evaluación del modelo, ya que si se realiza antes de dividir los datos, se estaría utilizando información de la base de datos de prueba para normalizar los datos de entrenamiento y dar de forma indirecta información de la base de datos de prueba a la red neuronal.\n\n# `TensorFlow`\n\n`TensorFlow` es una librería de Python que permite implementar redes neuronales artificiales de manera eficiente. En particular, `TensorFlow` incluye una clase llamada `tf.keras.Sequential` que permite definir la arquitectura de la red neuronal artificial.\n\n### Uso básico de TensorFlow\n\nPara utilizar `TensorFlow`, primero debemos instalar la librería.\n\n\n```{bash}\n!pip install tensorflow\n```\n\n\n`Tensorflow` se basa igualmente en el uso de tensores para realizar operaciones matemáticas.\n\n::: {#26f250b8 .cell execution_count=30}\n``` {.python .cell-code}\nimport tensorflow as tf\n\n# Crear un tensor de ceros\nx = tf.zeros((2, 3))\n\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[0. 0. 0.]\n [0. 0. 0.]], shape=(2, 3), dtype=float32)\n```\n:::\n:::\n\n\nPodemos realizar operaciones matemáticas con tensores en `TensorFlow`.\n\n::: {#a3e10dd2 .cell execution_count=31}\n``` {.python .cell-code}\n# Crear dos tensores\nx = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\ny = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n\n# Sumar los tensores\nz = x + y\n\nprint(z)\n\n# Multiplicar los tensores\nz = x * y\n\nprint(z)\n\n# Multiplicar un tensor por un escalar\nz = 2 * x\n\nprint(z)\n\n# Calcular la transpuesta de un tensor\nz = tf.transpose(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 6.  8.]\n [10. 12.]], shape=(2, 2), dtype=float32)\ntf.Tensor(\n[[ 5. 12.]\n [21. 32.]], shape=(2, 2), dtype=float32)\ntf.Tensor(\n[[2. 4.]\n [6. 8.]], shape=(2, 2), dtype=float32)\n```\n:::\n:::\n\n\nTodo las operaciones que se pueden realizar en `PyTorch` se pueden realizar en `TensorFlow`.\n\n### Estructura de una red neuronal artificial en TensorFlow\n\nVeamos las distintas partes de una red neuronal artificial en `TensorFlow`.\n\n#### Ejemplo de clasificación\n\nEn este ejemplo, vamos a utilizar la base de datos `iris` para entrenar una red neuronal artificial que permita clasificar las flores en tres categorías: `setosa`, `versicolor` y `virginica`.\n\n::: {#8cfc0efa .cell execution_count=32}\n``` {.python .cell-code}\nimport tensorflow as tf\n\n# Crear la red neuronal artificial\nmodel = tf.keras.Sequential([\n    tf.keras.Input(shape=(4,)),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(3, activation='softmax')\n])\n\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> (772.00 B)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> (772.00 B)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n```\n:::\n:::\n\n\nCon `TensorFlow` se puede definir la red neuronal de manera más sencilla que con `PyTorch`. En este caso, utilizamos la clase `tf.keras.Sequential` para definir la arquitectura de la red neuronal artificial. La clase `tf.keras.layers.Dense` representa una capa de neuronas completamente conectada. En este caso, definimos tres capas de neuronas con 10, 10 y 3 neuronas respectivamente. La función `tf.keras.Input` define la capa de entrada de la red neuronal.\n\nVamos a entrenar la red neuronal.\n\n::: {#7e42a860 .cell execution_count=33}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Cargar la base de datos iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Dividir la base de datos en entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1014)\n\n# Convertir los datos a tensores de TensorFlow\nX_train = tf.constant(X_train, dtype=tf.float32)\nX_test = tf.constant(X_test, dtype=tf.float32)\n\ny_train = tf.constant(y_train, dtype=tf.int64)\ny_test = tf.constant(y_test, dtype=tf.int64)\n\nprint(X_train.shape)\nprint(X_test.shape)\n\nprint(y_train.shape)\nprint(y_test.shape)\n\n# Compilar la red neuronal\nmodel.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Entrenar la red neuronal\nmodel.fit(X_train, y_train, epochs=1000, verbose=0)\n\n# Evaluar la red neuronal\nloss, accuracy = model.evaluate(X_test, y_test)\n\nprint(f\"Loss: {loss}, Accuracy: {accuracy}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(120, 4)\n(30, 4)\n(120,)\n(30,)\n\r1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - accuracy: 1.0000 - loss: 0.0373\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - accuracy: 1.0000 - loss: 0.0373\nLoss: 0.03730132058262825, Accuracy: 1.0\n```\n:::\n:::\n\n\nCon el método `compile` se compila la red neuronal con un optimizador, una función de pérdida y métricas de evaluación. En este caso, utilizamos el optimizador `sgd` (descenso de gradiente estocástico), la función de pérdida `sparse_categorical_crossentropy` y la métrica `accuracy` para evaluar la precisión de la red neuronal.\n\nCon el método `fit` se entrena la red neuronal con los datos de entrenamiento. En este caso, utilizamos 1000 épocas de entrenamiento. El argumento `verbose=0` indica que no se mostrará información sobre el entrenamiento, para mostrar la información se puede cambiar a `verbose=1`.\n\nCon el método `evaluate` se evalúa la red neuronal con los datos de prueba. El método `evaluate` devuelve la pérdida y la precisión de la red neuronal en los datos de prueba. \n\nPodemos obtener las predicciones de la red neuronal para la base de datos de prueba.\n\n::: {#25e9d1d4 .cell execution_count=34}\n``` {.python .cell-code}\ny_pred = model.predict(X_test)\n\nprint(y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step\n[[9.3780188e-03 9.9053556e-01 8.6380802e-05]\n [2.6691245e-05 6.1251414e-01 3.8745919e-01]\n [2.8749557e-07 5.9596896e-02 9.4040269e-01]\n [9.9821705e-01 1.7829464e-03 7.0882692e-12]\n [3.1476901e-04 9.9696678e-01 2.7184207e-03]\n [9.9806172e-01 1.9382917e-03 4.5069222e-12]\n [4.7594633e-05 9.9566221e-01 4.2900806e-03]\n [9.9946874e-01 5.3111481e-04 5.8261650e-14]\n [7.6720468e-04 9.9874258e-01 4.9011409e-04]\n [4.5947208e-06 3.0641466e-01 6.9358081e-01]\n [9.9916506e-01 8.3498686e-04 3.6297191e-13]\n [6.1792984e-05 9.9003565e-01 9.9024186e-03]\n [9.9943411e-01 5.6587165e-04 6.9331653e-14]\n [1.2681982e-04 9.9869889e-01 1.1743259e-03]\n [1.3101969e-07 6.3623767e-03 9.9363756e-01]\n [1.1187630e-03 9.9319637e-01 5.6849075e-03]\n [9.9854863e-01 1.4513362e-03 3.3198364e-12]\n [9.9968213e-01 3.1789864e-04 6.2396470e-15]\n [2.2063234e-04 9.5556730e-01 4.4211969e-02]\n [2.8410594e-03 9.9692291e-01 2.3596971e-04]\n [2.3421185e-08 3.8140393e-03 9.9618596e-01]\n [2.8064492e-04 9.9876738e-01 9.5207797e-04]\n [2.1451027e-10 4.6397839e-04 9.9953604e-01]\n [3.4913389e-04 9.9937332e-01 2.7746896e-04]\n [9.9886537e-01 1.1346943e-03 1.2733420e-12]\n [3.7266931e-04 9.9817073e-01 1.4565402e-03]\n [1.1772375e-04 9.2994481e-01 6.9937520e-02]\n [9.9625766e-01 3.7422990e-03 1.7923980e-10]\n [2.0222605e-04 9.8535591e-01 1.4441739e-02]\n [1.8129933e-09 2.6365283e-03 9.9736351e-01]]\n```\n:::\n:::\n\n\nPara obtener la clase predicha, podemos usar la función `argmax` de `NumPy`.\n\n::: {#075b16a7 .cell execution_count=35}\n``` {.python .cell-code}\ny_pred = np.argmax(y_pred, axis=1)\n\nprint(y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 1 2 0 1 0 1 0 1 2 0 1 0 1 2 1 0 0 1 1 2 1 2 1 0 1 1 0 1 2]\n```\n:::\n:::\n\n\nPodemos visualizar la matriz de confusión.\n\n::: {#7b28e3c1 .cell execution_count=36}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, y_pred)\n\nsns.heatmap(cm, annot=True, fmt='d', \n            xticklabels=iris.target_names, \n            yticklabels=iris.target_names,\n            cmap='Blues')\n```\n\n::: {.cell-output .cell-output-display}\n![](ann_python_files/figure-html/cell-37-output-1.png){width=529 height=411}\n:::\n:::\n\n\n# Salvando y cargando modelos\n\nEn `PyTorch` se pueden guardar y cargar modelos utilizando el método `save` y `load`.\n\n::: {#fc5a5985 .cell execution_count=37}\n``` {.python .cell-code}\n# Guardar el modelo\n#torch.save(model.state_dict(), 'model.pth')\n\n# Cargar el modelo\n#model.load_state_dict(torch.load('model.pth'))\n```\n:::\n\n\nEn `TensorFlow` se pueden guardar y cargar modelos utilizando el método `save` y `load`.\n\n::: {#daebf239 .cell execution_count=38}\n``` {.python .cell-code}\n# Guardar el modelo\nmodel.save('model.keras')\n\n# Cargar el modelo\n#model = keras.models.load_model('path/to/location.keras')\n```\n:::\n\n\nEsto sirve para guardar el modelo y poder utilizarlo en otro momento sin tener que volver a entrenarlo.\n\n# Conclusiones\n\nEn este tutorial, hemos visto cómo implementar redes neuronales artificiales en Python utilizando las librerías `scikit-learn`, `PyTorch` y `TensorFlow`. Hemos visto cómo cargar una base de datos, dividirla en entrenamiento y prueba, definir la arquitectura de la red neuronal, entrenar la red neuronal, evaluar la red neuronal y visualizar la matriz de confusión. También hemos visto cómo normalizar los datos y ajustar los hiperparámetros de la red neuronal para mejorar el rendimiento del modelo.\n\n",
    "supporting": [
      "ann_python_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}