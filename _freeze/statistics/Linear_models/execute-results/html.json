{
  "hash": "351fdc30f4fd1a424b2330fffaf63ff6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Modelos Lineales\"\nauthor: \"Christian Badillo\"\nformat: html\ndate: today\norder: 3\nfilters:\n  - pyodide\n  - fontawesome\n  - iconify\n---\n\n# Modelos Lineales\n\n Los modelos lineales son una clase de modelos estadísticos que asumen que la relación entre las variables dependientes e independientes es lineal. En este tutorial, aprenderemos cómo ajustar un modelo lineal a un conjunto de datos y cómo interpretar los resultados.\n\n Entre los modelos lineales más comunes se encuentran la regresión lineal simple y la regresión lineal múltiple. Se pueden utilizar para predecir una variable dependiente a partir de una o más variables independientes.\n\n Otros modelos lineales incluyen algunas pruebas de hipótesis, como el análisis de varianza (ANOVA) y la comparación de medias (prueba t).\n\n Todos estos modelos caen en la categoría de modelos lineales generalizados (GLM), que son una extensión de los modelos lineales tradicionales que permiten una mayor flexibilidad en la especificación de la distribución de los errores y la función de enlace. \n\n# Regresión Lineal Simple\n\nEl modelo de regresión lineal simple es uno de los modelos lineales más simples y fáciles de entender. En este modelo, se asume que la relación entre la variable dependiente y la variable independiente es lineal. La ecuación de regresión lineal simple se puede expresar de la siguiente manera:\n\n$$Y = \\beta_0 + \\beta_1X + \\epsilon$$\n\nDonde: \n\n- $Y$ es la variable dependiente\n- $X$ es la variable independiente\n- $\\beta_0$ es la intersección\n- $\\beta_1$ es la pendiente\n- $\\epsilon$ es el error\n\nEl objetivo de la regresión lineal simple es encontrar los valores de $\\beta_0$ y $\\beta_1$ que minimizan la suma de los cuadrados de los errores (SSE), que se define como:\n\n$$SSE = \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})^2$$\n\nDonde:\n\n- $Y_i$ es el valor observado de la variable dependiente\n- $\\hat{Y_i}$ es el valor predicho de la variable dependiente\n- $n$ es el número de observaciones\n\nUna vez que se han estimado los valores de $\\beta_0$ y $\\beta_1$, se pueden utilizar para predecir los valores de la variable dependiente para nuevas observaciones.\n\n## Regresión Lineal Simple por Mínimos Cuadrados\n\nEl método más común para ajustar un modelo de regresión lineal simple es el método de mínimos cuadrados. Este método consiste en encontrar los valores de $\\beta_0$ y $\\beta_1$ que minimizan la suma de los cuadrados de los errores. La fórmula para calcular los valores de $\\beta_0$ y $\\beta_1$ es la siguiente:\n\n$$\\beta_1 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}$$\n\n$$\\beta_0 = \\bar{Y} - \\beta_1\\bar{X}$$\n\nDonde:\n\n- $\\bar{X}$ es la media de la variable independiente\n- $\\bar{Y}$ es la media de la variable dependiente\n- $n$ es el número de observaciones\n\nUsemos los datos de la altura y el peso de un grupo de personas para ajustar un modelo de regresión lineal simple y predecir el peso de una persona en función de su altura.\n\n::: {#a61bd115 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Datos\naltura = np.array([150, 160, 170, 180, 190])\npeso = np.array([50, 60, 70, 80, 90])\n\n# Calcular medias\naltura_media = np.mean(altura)\npeso_media = np.mean(peso)\n\n# Calcular beta_1\nbeta_1 = np.sum((altura - altura_media) * (peso - peso_media)) / np.sum((altura - altura_media) ** 2)\n\n# Calcular beta_0\nbeta_0 = peso_media - beta_1 * altura_media\n\n# Imprimir coeficientes\nprint(f'beta_0: {beta_0}')\nprint(f'beta_1: {beta_1}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbeta_0: -100.0\nbeta_1: 1.0\n```\n:::\n:::\n\n\nPodemos graficar los datos y la línea de regresión para visualizar la relación entre la altura y el peso.\n\n::: {#510884d2 .cell execution_count=3}\n``` {.python .cell-code}\n# Graficar datos\nplt.scatter(altura, peso, color='blue')\nplt.plot(altura, beta_0 + beta_1 * altura, color='red')\nplt.xlabel('Altura')\nplt.ylabel('Peso')\nplt.title('Regresión Lineal Simple')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Linear_models_files/figure-html/cell-3-output-1.png){width=585 height=450}\n:::\n:::\n\n\nAhora podemos crear datos datos mas realistas y ajustar un modelo de regresión lineal simple.\n\n::: {#4ddc4a60 .cell execution_count=4}\n``` {.python .cell-code}\n# Crear datos\nnp.random.seed(0)\n\naltura = np.random.normal(170, 10, 100)\npeso = 50 + 0.05 * altura + np.random.normal(0, 5, 100)\n\n# Calcular medias\naltura_media = np.mean(altura)\npeso_media = np.mean(peso)\n\n# Calcular beta_1\nbeta_1 = np.sum((altura - altura_media) * (peso - peso_media)) / np.sum((altura - altura_media) ** 2)\n\n# Calcular beta_0\nbeta_0 = peso_media - beta_1 * altura_media\n\n# Imprimir coeficientes\nprint(f'beta_0: {beta_0}')\nprint(f'beta_1: {beta_1}')\n\n# Graficar datos\nplt.figure(figsize=(10, 6))\nplt.scatter(altura, peso, color='blue')\nplt.plot(altura, beta_0 + beta_1 * altura, color='red')\nplt.xlabel('Altura')\nplt.ylabel('Peso')\nplt.title('Regresión Lineal Simple')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbeta_0: 40.626398573820204\nbeta_1: 0.10734921677319048\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Linear_models_files/figure-html/cell-4-output-2.png){width=808 height=524}\n:::\n:::\n\n\nUsando `seaborn` podemos ajustar un modelo de regresión lineal simple y visualizar los resultados.\n\n::: {#36f29593 .cell execution_count=5}\n``` {.python .cell-code}\nimport seaborn as sns\nimport pandas as pd\n\n# Crear DataFrame\ndf = pd.DataFrame({'altura': altura, 'peso': peso})\n\n# Ajustar modelo\nplt.figure(figsize=(10, 6))\nsns.lmplot(x='altura', y='peso', data=df)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 960x576 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Linear_models_files/figure-html/cell-5-output-2.png){width=470 height=470}\n:::\n:::\n\n\nNos muestra la gráfica de la regresión lineal simple y su intervalo de confianza.\n\nLa libreria `statsmodels` nos permite ajustar un modelo de regresión lineal simple y obtener un resumen de los resultados.\n\nPuedes instalar la libreria usando el siguiente comando:\n\n```bash\n!pip install statsmodels\n```\n\n::: {#54dcb84d .cell execution_count=6}\n``` {.python .cell-code}\nimport statsmodels.api as sm\n\n# Ajustar modelo\nX = sm.add_constant(altura)\nmodel = sm.OLS(peso, X).fit()\n\n# Imprimir resumen\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.042\nModel:                            OLS   Adj. R-squared:                  0.033\nMethod:                 Least Squares   F-statistic:                     4.341\nDate:                Sat, 22 Jun 2024   Prob (F-statistic):             0.0398\nTime:                        13:41:19   Log-Likelihood:                -305.62\nNo. Observations:                 100   AIC:                             615.2\nDf Residuals:                      98   BIC:                             620.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         40.6264      8.805      4.614      0.000      23.152      58.100\nx1             0.1073      0.052      2.083      0.040       0.005       0.210\n==============================================================================\nOmnibus:                        5.184   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.075   Jarque-Bera (JB):                3.000\nSkew:                           0.210   Prob(JB):                        0.223\nKurtosis:                       2.262   Cond. No.                     2.90e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.9e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\nPara ajustar el modelo primero agregamos una columna de unos a la matriz de características `X` y luego ajustamos el modelo usando la función `OLS` de `statsmodels`. Finalmente, imprimimos un resumen de los resultados del modelo.\n\nImprimamos el valor de X\n\n::: {#0d3f0267 .cell execution_count=7}\n``` {.python .cell-code}\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\narray([[  1.        , 187.64052346],\n       [  1.        , 174.00157208],\n       [  1.        , 179.78737984],\n       [  1.        , 192.40893199],\n       [  1.        , 188.6755799 ],\n       [  1.        , 160.2272212 ],\n       [  1.        , 179.50088418],\n       [  1.        , 168.48642792],\n       [  1.        , 168.96781148],\n       [  1.        , 174.10598502],\n       [  1.        , 171.44043571],\n       [  1.        , 184.54273507],\n       [  1.        , 177.61037725],\n       [  1.        , 171.21675016],\n       [  1.        , 174.43863233],\n       [  1.        , 173.33674327],\n       [  1.        , 184.94079073],\n       [  1.        , 167.94841736],\n       [  1.        , 173.13067702],\n       [  1.        , 161.45904261],\n       [  1.        , 144.47010184],\n       [  1.        , 176.53618595],\n       [  1.        , 178.64436199],\n       [  1.        , 162.5783498 ],\n       [  1.        , 192.69754624],\n       [  1.        , 155.45634325],\n       [  1.        , 170.45758517],\n       [  1.        , 168.1281615 ],\n       [  1.        , 185.32779214],\n       [  1.        , 184.6935877 ],\n       [  1.        , 171.54947426],\n       [  1.        , 173.7816252 ],\n       [  1.        , 161.12214252],\n       [  1.        , 150.19203532],\n       [  1.        , 166.52087851],\n       [  1.        , 171.56348969],\n       [  1.        , 182.30290681],\n       [  1.        , 182.02379849],\n       [  1.        , 166.12673183],\n       [  1.        , 166.97697249],\n       [  1.        , 159.51447035],\n       [  1.        , 155.79982063],\n       [  1.        , 152.93729809],\n       [  1.        , 189.50775395],\n       [  1.        , 164.90347818],\n       [  1.        , 165.61925698],\n       [  1.        , 157.4720464 ],\n       [  1.        , 177.77490356],\n       [  1.        , 153.86102152],\n       [  1.        , 167.8725972 ],\n       [  1.        , 161.04533439],\n       [  1.        , 173.86902498],\n       [  1.        , 164.89194862],\n       [  1.        , 158.19367816],\n       [  1.        , 169.71817772],\n       [  1.        , 174.28331871],\n       [  1.        , 170.66517222],\n       [  1.        , 173.02471898],\n       [  1.        , 163.65677906],\n       [  1.        , 166.37258834],\n       [  1.        , 163.27539552],\n       [  1.        , 166.40446838],\n       [  1.        , 161.86853718],\n       [  1.        , 152.73717398],\n       [  1.        , 171.77426142],\n       [  1.        , 165.98219064],\n       [  1.        , 153.69801653],\n       [  1.        , 174.62782256],\n       [  1.        , 160.92701636],\n       [  1.        , 170.51945396],\n       [  1.        , 177.29090562],\n       [  1.        , 171.28982911],\n       [  1.        , 181.39400685],\n       [  1.        , 157.6517418 ],\n       [  1.        , 174.02341641],\n       [  1.        , 163.15189909],\n       [  1.        , 161.29202851],\n       [  1.        , 164.21150335],\n       [  1.        , 166.88447468],\n       [  1.        , 170.56165342],\n       [  1.        , 158.34850159],\n       [  1.        , 179.00826487],\n       [  1.        , 174.6566244 ],\n       [  1.        , 154.63756314],\n       [  1.        , 184.88252194],\n       [  1.        , 188.95889176],\n       [  1.        , 181.78779571],\n       [  1.        , 168.20075164],\n       [  1.        , 159.29247378],\n       [  1.        , 180.54451727],\n       [  1.        , 165.96823053],\n       [  1.        , 182.2244507 ],\n       [  1.        , 172.08274978],\n       [  1.        , 179.76639036],\n       [  1.        , 173.56366397],\n       [  1.        , 177.06573168],\n       [  1.        , 170.10500021],\n       [  1.        , 187.85870494],\n       [  1.        , 171.26912093],\n       [  1.        , 174.01989363]])\n```\n:::\n:::\n\n\nQue corresponde a lo que se denomina como **matriz de diseño**. Que es una matriz que contiene las variables independientes y una columna de unos que representa la intersección. Podemos usar esta matriz para ajustar el modelo de regresión lineal simple por medio de la siguiente fórmula:\n\n$$Y = X\\beta + \\epsilon$$\n\nY usando el método de mínimos cuadrados para encontrar los valores de $\\beta$ que minimizan la suma de los cuadrados de los errores, nos da la siguiente fórmula:\n\n$$\\beta = (X^TX)^{-1}X^TY$$\n\nDonde:\n\n- $Y$ es el vector de la variable dependiente\n- $X$ es la matriz de diseño\n- $\\beta$ es el vector de coeficientes\n\nHagamos esto en Python.\n\n::: {#cf8e4233 .cell execution_count=8}\n``` {.python .cell-code}\n# Ajustar modelo\nX = np.column_stack((np.ones(len(altura)), altura))\n\nprint(X)\n\nbeta = np.linalg.inv(X.T @ X) @ X.T @ peso\n\n# Imprimir coeficientes\nprint(f'beta_0: {beta[0]}')\nprint(f'beta_1: {beta[1]}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[  1.         187.64052346]\n [  1.         174.00157208]\n [  1.         179.78737984]\n [  1.         192.40893199]\n [  1.         188.6755799 ]\n [  1.         160.2272212 ]\n [  1.         179.50088418]\n [  1.         168.48642792]\n [  1.         168.96781148]\n [  1.         174.10598502]\n [  1.         171.44043571]\n [  1.         184.54273507]\n [  1.         177.61037725]\n [  1.         171.21675016]\n [  1.         174.43863233]\n [  1.         173.33674327]\n [  1.         184.94079073]\n [  1.         167.94841736]\n [  1.         173.13067702]\n [  1.         161.45904261]\n [  1.         144.47010184]\n [  1.         176.53618595]\n [  1.         178.64436199]\n [  1.         162.5783498 ]\n [  1.         192.69754624]\n [  1.         155.45634325]\n [  1.         170.45758517]\n [  1.         168.1281615 ]\n [  1.         185.32779214]\n [  1.         184.6935877 ]\n [  1.         171.54947426]\n [  1.         173.7816252 ]\n [  1.         161.12214252]\n [  1.         150.19203532]\n [  1.         166.52087851]\n [  1.         171.56348969]\n [  1.         182.30290681]\n [  1.         182.02379849]\n [  1.         166.12673183]\n [  1.         166.97697249]\n [  1.         159.51447035]\n [  1.         155.79982063]\n [  1.         152.93729809]\n [  1.         189.50775395]\n [  1.         164.90347818]\n [  1.         165.61925698]\n [  1.         157.4720464 ]\n [  1.         177.77490356]\n [  1.         153.86102152]\n [  1.         167.8725972 ]\n [  1.         161.04533439]\n [  1.         173.86902498]\n [  1.         164.89194862]\n [  1.         158.19367816]\n [  1.         169.71817772]\n [  1.         174.28331871]\n [  1.         170.66517222]\n [  1.         173.02471898]\n [  1.         163.65677906]\n [  1.         166.37258834]\n [  1.         163.27539552]\n [  1.         166.40446838]\n [  1.         161.86853718]\n [  1.         152.73717398]\n [  1.         171.77426142]\n [  1.         165.98219064]\n [  1.         153.69801653]\n [  1.         174.62782256]\n [  1.         160.92701636]\n [  1.         170.51945396]\n [  1.         177.29090562]\n [  1.         171.28982911]\n [  1.         181.39400685]\n [  1.         157.6517418 ]\n [  1.         174.02341641]\n [  1.         163.15189909]\n [  1.         161.29202851]\n [  1.         164.21150335]\n [  1.         166.88447468]\n [  1.         170.56165342]\n [  1.         158.34850159]\n [  1.         179.00826487]\n [  1.         174.6566244 ]\n [  1.         154.63756314]\n [  1.         184.88252194]\n [  1.         188.95889176]\n [  1.         181.78779571]\n [  1.         168.20075164]\n [  1.         159.29247378]\n [  1.         180.54451727]\n [  1.         165.96823053]\n [  1.         182.2244507 ]\n [  1.         172.08274978]\n [  1.         179.76639036]\n [  1.         173.56366397]\n [  1.         177.06573168]\n [  1.         170.10500021]\n [  1.         187.85870494]\n [  1.         171.26912093]\n [  1.         174.01989363]]\nbeta_0: 40.62639857381858\nbeta_1: 0.10734921677318345\n```\n:::\n:::\n\n\n### Demostración de la Fórmula de Mínimos Cuadrados*\n\nTenemos el modelo de regresión lineal general:\n\n$$Y = \\beta_0 + \\beta_1X + ... + \\beta_nX_n + \\epsilon$$\n\nLa función de costo que queremos minimizar es el error cuadrático residual (RSS), que se define como:\n\n$$RSS = \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})^2$$\n\nEn forma matricial, la función de costo se puede expresar como:\n\n$$RSS = (Y - X\\beta)^T(Y - X\\beta)$$\n\nQue se conoce como la forma cuadrática de la función de costo. Para minimizar la función de costo, tomamos la derivada de la función de costo con respecto a $\\beta$:\n\n\\begin{align*}\n\n\\frac{\\partial RSS}{\\partial \\beta} & = \\frac{\\partial}{\\partial \\beta} (Y - X\\beta)^T(Y - X\\beta) \\\\\n& = \\frac{\\partial}{\\partial \\beta} (Y^TY - Y^TX\\beta - (X\\beta)^TY + (X\\beta)^TX\\beta) \\\\\n& = \\frac{\\partial}{\\partial \\beta} (Y^TY - Y^TX\\beta - \\beta^TX^TY + \\beta^TX^TX\\beta) \\\\\n& = \\frac{\\partial}{\\partial \\beta} Y^TY - \\frac{\\partial}{\\partial \\beta} Y^TX\\beta - \\frac{\\partial}{\\partial \\beta} \\beta^TX^TY + \\frac{\\partial}{\\partial \\beta} \\beta^TX^TX\\beta \\\\\n& = 0 - X^TY - X^TY + 2X^TX\\beta \\\\\n& = -2X^TY + 2X^TX\\beta \\\\\n& = 2X^T(X\\beta - Y) \\\\\n\\end{align*}\n\nIgualamos la derivada de la función de costo a cero para encontrar el mínimo:\n\n$$\\frac{\\partial RSS}{\\partial \\beta} = 0$$\n\n$$2X^T(X\\beta - Y) = 0$$\n\nResolviendo para $\\beta$:\n\n\\begin{align*}\n2X^T(X\\beta - Y) & = 0 \\\\\nX^T(X\\beta - Y) & = 0 \\\\\nX^TX\\beta - X^TY & = 0 \\\\\nX^TX\\beta & = X^TY \\\\\n\\beta & = (X^TX)^{-1}X^TY \\\\\n\\end{align*}\n\n## Regresión Lineal Simple por Descenso del Gradiente\n\nOtra forma de ajustar un modelo de regresión lineal simple es mediante el descenso del gradiente. Este método consiste en ajustar los valores de $\\beta_0$ y $\\beta_1$ iterativamente para minimizar una función de costo. La función de costo que se utiliza en la regresión lineal simple es el error cuadrático medio (MSE), que se define como:\n\n$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})^2$$\n\nDonde:\n\n- $Y_i$ es el valor observado de la variable dependiente\n- $\\hat{Y_i}$ es el valor predicho de la variable dependiente\n- $n$ es el número de observaciones\n\nEl descenso de gradiente utiliza la derivada de la función de costo con respecto a los parámetros $\\beta_0$ y $\\beta_1$ para ajustar los valores de los parámetros en la dirección que minimiza la función de costo. La regla de actualización de los parámetros es la siguiente:\n\n$$\\beta_0 = \\beta_0 - \\alpha \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})$$\n\n$$\\beta_1 = \\beta_1 - \\alpha \\frac{1}{n} \\sum_{i=1}^{n} (Y_i -\\hat{Y_i})X_i$$\n\nDonde:\n\n- $\\alpha$ es la tasa de aprendizaje\n- $\\hat{Y_i}$ es el valor predicho de la variable dependiente\n- $Y_i$ es el valor observado de la variable dependiente\n- $X_i$ es el valor de la variable independiente\n- $n$ es el número de observaciones\n- $\\alpha$ es la tasa de aprendizaje que controla el tamaño de los pasos de actualización de los parámetros\n\n### Derivación de las Ecuaciones de Descenso del Gradiente*\n\nAquí está la demostración de la fórmula de actualización de los parámetros en el descenso del gradiente para la regresión lineal simple.\n\nLa función de costo para la regresión lineal simple es el error cuadrático medio (MSE), que se define como:\n\n$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})^2$$\n\nReemplazando $\\hat{Y_i}$ con la ecuación de regresión lineal simple, obtenemos:\n\n$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1X_i)^2$$\n\nDerivamos respecto a $\\beta_0$ y $\\beta_1$ para obtener las derivadas parciales de la función de costo:\n\n$$\\frac{\\partial MSE}{\\partial \\beta_0} = \\frac{-2}{n} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1X_i)$$\n\n$$\\frac{\\partial MSE}{\\partial \\beta_1} = \\frac{-2}{n} \\sum_{i=1}^{n} X_i (Y_i - \\beta_0 - \\beta_1X_i)$$\n\nUsamos las derivadas parciales para actualizar los parámetros en la dirección que minimiza la función de costo:\n\n$$\\beta_0 = \\beta_0 - \\alpha \\frac{\\partial MSE}{\\partial \\beta_0}$$\n\n$$\\beta_1 = \\beta_1 - \\alpha \\frac{\\partial MSE}{\\partial \\beta_1}$$\n\nReemplazando las derivadas parciales, obtenemos la fórmula de actualización de los parámetros en el descenso del gradiente para la regresión lineal simple:\n\n$$\\beta_0 = \\beta_0 + \\alpha \\frac{2}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})$$\n\n$$\\beta_1 = \\beta_1 + \\alpha \\frac{2}{n} \\sum_{i=1}^{n} X_i (Y _i - \\hat{Y_i})$$\n\nEl vector de coeficientes $\\beta$ se puede actualizar iterativamente hasta que se alcance un criterio de convergencia, como un número máximo de iteraciones o una tolerancia en el cambio de los parámetros.\n\n### Implementación del Descenso del Gradiente\n\nVamos a implementar el descenso del gradiente para ajustar un modelo de regresión lineal simple.\n\n::: {#7878e174 .cell execution_count=9}\n``` {.python .cell-code}\n# Normalización de los datos\nX = (altura - np.mean(altura)) / np.std(altura)\nY = (peso - np.mean(peso)) / np.std(peso)\nn = len(X)\n\n# Valores iniciales de los parámetros\nb0 = 0\nb1 = 0\n\n# Hiperparámetros\nalpha = 1e-3\nepochs = 10000\n\n# Descenso del gradiente\nfor i in range(epochs):\n    y_pred = b0 + b1 * X # Predicción\n\n    db0 = (-2/n) * sum(Y - y_pred) # Derivada parcial de b0\n    db1 = (-2/n) * sum(X * (Y - y_pred)) # Derivada parcial de b1\n\n    b0 = b0 - alpha * db0 # Actualizar b0\n    b1 = b1 - alpha * db1 # Actualizar b1\n\n    if i % 500 == 0: # Imprimir resultados cada 500 iteraciones\n        print(f\"Epoch {i}: b0 = {b0 * np.std(peso) + np.mean(peso) - b1 * np.mean(altura)}, b1 = {b1 * (np.std(peso) / np.std(altura))}\")\n\n# Desnormalizar los parámetros\nb1 = b1 * (np.std(peso) / np.std(altura))\nb0 = b0 * np.std(peso) + np.mean(peso) - b1 * np.mean(altura)\n\n# Imprimir coeficientes\nprint(f\"Los valores óptimos son: b0 = {b0}, b1 = {b1}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 0: b0 = 58.86970065046024, b1 = 0.00021469843354638102\nEpoch 500: b0 = 36.69221818906457, b1 = 0.06797607549967058\nEpoch 1000: b0 = 28.541743780104426, b1 = 0.09287914421609095\nEpoch 1500: b0 = 25.546352702386315, b1 = 0.10203130224985668\nEpoch 2000: b0 = 24.44551276862618, b1 = 0.10539482333349699\nEpoch 2500: b0 = 24.040941703173104, b1 = 0.10663095518768421\nEpoch 3000: b0 = 23.89225728322957, b1 = 0.1070852475565854\nEpoch 3500: b0 = 23.83761408547845, b1 = 0.10725220511515218\nEpoch 4000: b0 = 23.817532095303484, b1 = 0.10731356389700908\nEpoch 4500: b0 = 23.81015173789428, b1 = 0.10733611393992575\nEpoch 5000: b0 = 23.8074393734815, b1 = 0.10734440133449522\nEpoch 5500: b0 = 23.80644254903254, b1 = 0.1073474470452729\nEpoch 6000: b0 = 23.806076204828443, b1 = 0.10734856637826272\nEpoch 6500: b0 = 23.80594156921029, b1 = 0.10734897774573438\nEpoch 7000: b0 = 23.80589208910532, b1 = 0.10734912892791008\nEpoch 7500: b0 = 23.805873904609804, b1 = 0.10734918448906133\nEpoch 8000: b0 = 23.805867221603073, b1 = 0.10734920490840964\nEpoch 8500: b0 = 23.805864765522912, b1 = 0.10734921241274988\nEpoch 9000: b0 = 23.80586386288578, b1 = 0.10734921517067947\nEpoch 9500: b0 = 23.805863531156447, b1 = 0.10734921618424971\nLos valores óptimos son: b0 = 40.62639861081885, b1 = 0.1073492165563143\n```\n:::\n:::\n\n\nLa normalización de los datos es importante para que el descenso del gradiente converja más rápido. En este caso, normalizamos las variables independientes y dependientes restando la media y dividiendo por la desviación estándar. Después de ajustar el modelo, desnormalizamos los parámetros para obtener los valores en la escala original.\n\nOtro factor importante es la tasa de aprendizaje $\\alpha$, que controla el tamaño de los pasos de actualización de los parámetros. Si la tasa de aprendizaje es demasiado pequeña, el algoritmo puede converger lentamente. Si es demasiado grande, el algoritmo puede divergir, encontrar la tasa de aprendizaje adecuada es crucial para el éxito del descenso del gradiente y muchos algoritmos adaptativos ajustan automáticamente la tasa de aprendizaje durante el entrenamiento para mejorar la convergencia.\n\nVeremos de nuevo el método de descenso del gradiente cuando hablemos de redes neuronales.\n\n# Regresión Lineal Múltiple\n\nLa regresión lineal múltiple es una extensión de la regresión lineal simple que permite modelar la relación entre una variable dependiente y dos o más variables independientes. La ecuación de regresión lineal múltiple se puede expresar de la siguiente manera:\n\n$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon$$\n\nDonde:\n\n- $Y$ es la variable dependiente\n- $X_1, X_2, ..., X_n$ son las variables independientes\n- $\\beta_0$ es la intersección\n- $\\epsilon$ es el error\n\nAl igual que la regresión lineal simple, el objetivo de la regresión lineal múltiple es encontrar los valores de $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n$ que minimizan la suma de los cuadrados de los errores (SSE).\n\n## Regresión Lineal Múltiple por Mínimos Cuadrados\n\nDebido a que ahora tratamos con múltiples variables independientes, la fórmula para calcular los valores de $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n$ esta en forma matricial:\n\n$$\\beta = (X^TX)^{-1}X^TY$$\n\nQué es idénctico a la fórmula de la regresión lineal simple, solo que ahora $X$ es una matriz que contiene las variables independientes y una columna de unos que representa la intersección.\n\nUsemos datos de ejemplo para ajustar un modelo de regresión lineal múltiple y predecir la variable dependiente. Generaremos datos de 5 variables independientes y una variable dependiente.\n\n::: {#d62dabc8 .cell execution_count=10}\n``` {.python .cell-code}\n# Crear datos\nnp.random.seed(1014)\n\n# Variables independientes\nX1 = np.random.normal(0, 1, 100)\nX2 = np.random.normal(10, 5, 100)\nX3 = np.random.normal(-5, 2, 100)\nX4 = np.random.normal(3, 1, 100)\nX5 = np.random.normal(2, 0.5, 100)\n\n# Efecto de cada variable independiente\nbetas = np.array([5, 10, 3, -2, -1, 4])\n\n# Error\nepsilon = np.random.normal(0, 2.5, 100)\n\n# Variable dependiente\nY = betas[0] + betas[1] * X1 + betas[2] * X2 + betas[3] * X3 + betas[4] * X4 + betas[5] * X5 + epsilon\n\ndf = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'X4': X4, 'X5': X5, 'Y': Y})\n\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.759433</td>\n      <td>10.731036</td>\n      <td>-6.699854</td>\n      <td>2.615701</td>\n      <td>1.670699</td>\n      <td>61.504776</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.007260</td>\n      <td>16.029547</td>\n      <td>-3.969604</td>\n      <td>1.957694</td>\n      <td>2.076248</td>\n      <td>55.376552</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.644990</td>\n      <td>10.329352</td>\n      <td>-7.744539</td>\n      <td>2.040599</td>\n      <td>2.300200</td>\n      <td>52.803668</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.266741</td>\n      <td>14.039842</td>\n      <td>-7.208872</td>\n      <td>3.020797</td>\n      <td>2.237179</td>\n      <td>62.968653</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.291256</td>\n      <td>13.169584</td>\n      <td>-5.827498</td>\n      <td>3.217659</td>\n      <td>1.341268</td>\n      <td>61.591722</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAhora ajustaremos un modelo de regresión lineal múltiple a los datos y obtendremos un resumen de los resultados.\n\n::: {#c2027081 .cell execution_count=11}\n``` {.python .cell-code}\n# Ajustar modelo\nX = sm.add_constant(df[['X1', 'X2', 'X3', 'X4', 'X5']])\nmodel = sm.OLS(df['Y'], X).fit()\n\n# Imprimir resumen\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.976\nModel:                            OLS   Adj. R-squared:                  0.975\nMethod:                 Least Squares   F-statistic:                     758.3\nDate:                Sat, 22 Jun 2024   Prob (F-statistic):           2.63e-74\nTime:                        13:41:20   Log-Likelihood:                -243.14\nNo. Observations:                 100   AIC:                             498.3\nDf Residuals:                      94   BIC:                             513.9\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.4953      1.737      3.165      0.002       2.047       8.943\nX1             9.9913      0.310     32.255      0.000       9.376      10.606\nX2             2.8923      0.058     49.999      0.000       2.777       3.007\nX3            -1.9544      0.142    -13.789      0.000      -2.236      -1.673\nX4            -0.7991      0.281     -2.843      0.005      -1.357      -0.241\nX5             4.0200      0.613      6.553      0.000       2.802       5.238\n==============================================================================\nOmnibus:                        2.713   Durbin-Watson:                   2.227\nProb(Omnibus):                  0.258   Jarque-Bera (JB):                1.890\nSkew:                           0.134   Prob(JB):                        0.389\nKurtosis:                       2.383   Cond. No.                         73.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nPodemos imprimir los coeficientes del modelo para ver cómo se relacionan con los valores reales.\n\n::: {#0776767f .cell execution_count=12}\n``` {.python .cell-code}\n# Imprimir coeficientes\nprint(f'Intercepto: {model.params[0]}')\nprint(f'Coeficientes: {model.params[1:]}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercepto: 5.495327623636475\nCoeficientes: X1    9.991312\nX2    2.892250\nX3   -1.954408\nX4   -0.799148\nX5    4.020035\ndtype: float64\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_11165/3979803380.py:2: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n```\n:::\n:::\n\n\nEl resumen del modelo nos proporciona información sobre la calidad del ajuste, los coeficientes de las variables independientes, los errores estándar de los coeficientes, los valores p, el coeficiente de determinación $R^2$, entre otros.\n\nGráfiquemos los parámetros estimados y los valores reales.\n\n::: {#844f6551 .cell execution_count=13}\n``` {.python .cell-code}\n# Graficar parámetros estimados y valores reales\nplt.figure(figsize=(10, 6))\nplt.plot(betas, label='Real', marker='o', markersize=10)\nplt.plot(model.params, label='Estimado', marker='x', markersize=10)\nplt.xlabel('Variables', fontsize=14)\nplt.ylabel('Coeficientes', fontsize=14)\nplt.title('Coeficientes Estimados vs. Coeficientes Reales', fontsize=16, fontweight='bold')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Linear_models_files/figure-html/cell-13-output-1.png){width=816 height=533}\n:::\n:::\n\n\nLa paquetería `statsmodels` es muy completa y nos permite ajustar modelos de regresión lineal múltiple con facilidad. También podemos usar la función `ols` de `statsmodels.formula.api` para ajustar modelos de regresión lineal múltiple con fórmulas de estilo R.\n\n::: {#74774251 .cell execution_count=14}\n``` {.python .cell-code}\nimport statsmodels.formula.api as smf\n\n# Ajustar modelo\nmodel = smf.ols('Y ~ X1 + X2 + X3 + X4 + X5', data=df).fit()\n\n# Imprimir resumen\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.976\nModel:                            OLS   Adj. R-squared:                  0.975\nMethod:                 Least Squares   F-statistic:                     758.3\nDate:                Sat, 22 Jun 2024   Prob (F-statistic):           2.63e-74\nTime:                        13:41:20   Log-Likelihood:                -243.14\nNo. Observations:                 100   AIC:                             498.3\nDf Residuals:                      94   BIC:                             513.9\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      5.4953      1.737      3.165      0.002       2.047       8.943\nX1             9.9913      0.310     32.255      0.000       9.376      10.606\nX2             2.8923      0.058     49.999      0.000       2.777       3.007\nX3            -1.9544      0.142    -13.789      0.000      -2.236      -1.673\nX4            -0.7991      0.281     -2.843      0.005      -1.357      -0.241\nX5             4.0200      0.613      6.553      0.000       2.802       5.238\n==============================================================================\nOmnibus:                        2.713   Durbin-Watson:                   2.227\nProb(Omnibus):                  0.258   Jarque-Bera (JB):                1.890\nSkew:                           0.134   Prob(JB):                        0.389\nKurtosis:                       2.383   Cond. No.                         73.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nDos resultados importantes del resumen es la prueba Durbin-Watson y la prueba de Jarque-Bera. La prueba Durbin-Watson se utiliza para detectar la presencia de autocorrelación en los residuos del modelo. Un valor de Durbin-Watson cercano a 2 indica que no hay autocorrelación. La prueba de Jarque-Bera se utiliza para detectar la normalidad de los residuos del modelo. Un valor de Jarque-Bera cercano a 0 indica que los residuos son normales.\n\nDetectar la presencia de autocorrelación y no normalidad en los residuos es importante para evaluar la calidad del ajuste del modelo y tomar decisiones informadas sobre su uso. Si se obtienen resultados significativos en estas pruebas, es posible que sea necesario realizar ajustes adicionales al modelo para mejorar su rendimiento.\n\n# Regresión Lineal con Scikit-Learn\n\nLa librería `scikit-learn` es una de las librerías más populares para el aprendizaje automático en Python. Proporciona una amplia gama de algoritmos de aprendizaje automático, incluidos los modelos de regresión lineal.\n\nVamos a ajustar un modelo de regresión lineal simple y un modelo de regresión lineal múltiple con `scikit-learn`.\n\n::: {#04280b41 .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\n\n# Regresión Lineal Simple\nX = altura.reshape(-1, 1)\nY = peso\n\nmodel = LinearRegression()\nmodel.fit(X, Y)\n\nprint(f'Intercepto: {model.intercept_}')\nprint(f'Coeficiente: {model.coef_[0]}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercepto: 40.6263985738202\nCoeficiente: 0.1073492167731905\n```\n:::\n:::\n\n\nLa función `reshape(-1, 1)` se utiliza para convertir el vector de altura en una matriz de una sola columna. Esto es necesario porque `scikit-learn` espera que las variables independientes sean matrices bidimensionales. Imprimamos esta matriz.\n\n::: {#ae68c637 .cell execution_count=16}\n``` {.python .cell-code}\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\narray([[187.64052346],\n       [174.00157208],\n       [179.78737984],\n       [192.40893199],\n       [188.6755799 ],\n       [160.2272212 ],\n       [179.50088418],\n       [168.48642792],\n       [168.96781148],\n       [174.10598502],\n       [171.44043571],\n       [184.54273507],\n       [177.61037725],\n       [171.21675016],\n       [174.43863233],\n       [173.33674327],\n       [184.94079073],\n       [167.94841736],\n       [173.13067702],\n       [161.45904261],\n       [144.47010184],\n       [176.53618595],\n       [178.64436199],\n       [162.5783498 ],\n       [192.69754624],\n       [155.45634325],\n       [170.45758517],\n       [168.1281615 ],\n       [185.32779214],\n       [184.6935877 ],\n       [171.54947426],\n       [173.7816252 ],\n       [161.12214252],\n       [150.19203532],\n       [166.52087851],\n       [171.56348969],\n       [182.30290681],\n       [182.02379849],\n       [166.12673183],\n       [166.97697249],\n       [159.51447035],\n       [155.79982063],\n       [152.93729809],\n       [189.50775395],\n       [164.90347818],\n       [165.61925698],\n       [157.4720464 ],\n       [177.77490356],\n       [153.86102152],\n       [167.8725972 ],\n       [161.04533439],\n       [173.86902498],\n       [164.89194862],\n       [158.19367816],\n       [169.71817772],\n       [174.28331871],\n       [170.66517222],\n       [173.02471898],\n       [163.65677906],\n       [166.37258834],\n       [163.27539552],\n       [166.40446838],\n       [161.86853718],\n       [152.73717398],\n       [171.77426142],\n       [165.98219064],\n       [153.69801653],\n       [174.62782256],\n       [160.92701636],\n       [170.51945396],\n       [177.29090562],\n       [171.28982911],\n       [181.39400685],\n       [157.6517418 ],\n       [174.02341641],\n       [163.15189909],\n       [161.29202851],\n       [164.21150335],\n       [166.88447468],\n       [170.56165342],\n       [158.34850159],\n       [179.00826487],\n       [174.6566244 ],\n       [154.63756314],\n       [184.88252194],\n       [188.95889176],\n       [181.78779571],\n       [168.20075164],\n       [159.29247378],\n       [180.54451727],\n       [165.96823053],\n       [182.2244507 ],\n       [172.08274978],\n       [179.76639036],\n       [173.56366397],\n       [177.06573168],\n       [170.10500021],\n       [187.85870494],\n       [171.26912093],\n       [174.01989363]])\n```\n:::\n:::\n\n\nPodemos utilizar el método `predict` para hacer predicciones con el modelo.\n\n::: {#32b765cc .cell execution_count=17}\n``` {.python .cell-code}\n# Predicción\naltura_nueva = np.array([[160], [170], [180]])\npeso_pred = model.predict(altura_nueva)\n\nprint(peso_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[57.80227326 58.87576543 59.94925759]\n```\n:::\n:::\n\n\nPodemos usar otras funciones de `scikit-learn` para realizar una regresion Rodge o Lasso.\n\n::: {#573eb11f .cell execution_count=18}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Ridge, Lasso\n\n# Regresión Ridge\nmodel = Ridge(alpha=1.0)\nmodel.fit(X, Y)\n\nprint(f'Intercepto: {model.intercept_}')\nprint(f'Coeficiente: {model.coef_[0]}')\n\n# Regresión Lasso\nmodel = Lasso(alpha=1.0)\nmodel.fit(X, Y)\n\nprint(f'Intercepto: {model.intercept_}')\nprint(f'Coeficiente: {model.coef_[0]}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercepto: 40.62820122077731\nCoeficiente: 0.10733865014222098\nIntercepto: 42.30580012649057\nCoeficiente: 0.09750501717175815\n```\n:::\n:::\n\n\n# Pruebas de Normalidad\n\nLas pruebas de normalidad son pruebas estadísticas que se utilizan para determinar si una muestra de datos proviene de una distribución normal. Existen varias pruebas de normalidad, como la prueba de Shapiro-Wilk, la prueba de Kolmogorov-Smirnov y la prueba de Anderson-Darling. O métodos gráficos como los histogramas y el gráfico Q-Q.\n\n## Gráfico Q-Q\n\nEl gráfico Q-Q (quantile-quantile) es una forma visual de evaluar si una muestra de datos proviene de una distribución normal. En un gráfico Q-Q, los cuantiles de la muestra se comparan con los cuantiles de una distribución normal teórica. Si los puntos en el gráfico Q-Q siguen una línea recta, entonces los datos se ajustan a una distribución normal. Evidentemente, si los puntos se alejan de la línea recta, entonces los datos no se ajustan a una distribución normal.\n\nVamos a generar datos de una distribución normal y una distribución no normal para comparar los gráficos Q-Q.\n\n::: {#25cde0c3 .cell execution_count=19}\n``` {.python .cell-code}\n# Datos de una distribución normal\nnp.random.seed(0)\nnormal_data = np.random.normal(0, 1, 1000)\n\n# Datos de una distribución no normal\nnon_normal_data = np.random.exponential(1, 1000)\n\n# Gráfico Q-Q\nfig, ax = plt.subplots(1, 2, figsize=(10, 6))\n\n# Distribución normal\nax[0].set_title('Distribución Normal')\nsm.qqplot(normal_data, line='s', ax=ax[0])\nax[0].set_aspect('equal')\nax[0].set_xlabel('Cuantiles Teóricos')\nax[0].set_ylabel('Cuantiles de los Datos')\n\n# Distribución no normal\nax[1].set_title('Distribución No Normal')\nsm.qqplot(non_normal_data, line='s', ax=ax[1])\nax[1].set_aspect('equal')\nax[1].set_xlabel('Cuantiles Teóricos')\nax[1].set_ylabel('Cuantiles de los Datos')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Linear_models_files/figure-html/cell-19-output-1.png){width=792 height=523}\n:::\n:::\n\n\nEn el gráfico Q-Q de la distribución normal, los puntos siguen una línea recta, lo que indica que los datos se ajustan a una distribución normal. En el gráfico Q-Q de la distribución no normal, los puntos se alejan de la línea recta, lo que indica que los datos no se ajustan a una distribución normal.\n\n## Prueba de Shapiro-Wilk\n\nLa prueba de Shapiro-Wilk es una prueba estadística que se utiliza para determinar si una muestra de datos proviene de una distribución normal. \n\nHipótesis:\n\n- $H_0$: Los datos **provienen** de una distribución normal\n- $H_1$: Los datos *no provienen* de una distribución normal\n\nVamos a aplicar la prueba de Shapiro-Wilk a los datos de las distribuciones normal y no normal.\n\n::: {#19fdec52 .cell execution_count=20}\n``` {.python .cell-code}\nfrom scipy.stats import shapiro\n\n# Prueba de Shapiro-Wilk\nstat, p = shapiro(normal_data)\nprint(f'Distribución Normal: Estadístico = {stat}, p-valor = {p}')\n\nstat, p = shapiro(non_normal_data)\nprint(f'Distribución No Normal: Estadístico = {stat}, p-valor = {p}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDistribución Normal: Estadístico = 0.9985554728235057, p-valor = 0.5912267898687746\nDistribución No Normal: Estadístico = 0.8336406754912049, p-valor = 5.034540538267324e-31\n```\n:::\n:::\n\n\nEl valor p de los datos normales fue de $0.5912267898687746$ y el valor p de los datos no normales fue de $5.034540538267324e-31$. Por lo que podemos rechazar la hipótesis nula en el segundo caso.\n\n# Ejercicios\n\n1. Carga los datos desde https://github.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/blob/main/data/cars.csv.\n\n*Columnas:*\n\n* **Car_ID**: A unique identifier for each car listing.\n* **Brand**: The brand or manufacturer of the car (e.g., Toyota, Honda, Ford, etc.).\n* **Model**: The model of the car (e.g., Camry, Civic, Mustang, etc.).\n* **Year**: The manufacturing year of the car.\n* **Kilometers_Driven**: The total kilometers driven by the car.\n* **Fuel_Type**: The type of fuel used by the car (e.g., Petrol, Diesel, Electric, etc.).\n* **Transmission**: The transmission type of the car (e.g., Manual, Automatic).\n* **Owner_Type**: The number of previous owners of the car (e.g., First, Second, Third).\n* **Mileage**: The fuel efficiency of the car in kilometers per liter.\n* **Engine**: The engine capacity of the car in CC (Cubic Centimeters).\n* **Power**: The maximum power output of the car in bhp (Brake Horsepower).\n* **Seats**: The number of seats available in the car.\n* **Price**: The selling price of the car in INR (Indian Rupees), which is the target variable to predict.\n\n2. Revisa si existen valores nulos en el conjunto de datos y elimina las filas que los contengan.\n\n3. Usando la función de pandas [`get_dummies`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) convierte las variables categóricas en variables dummy. O en su defecto, usa la función [`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) de `sklearn`.\n\n4. Realiza un análisis exploratorio de los datos y describe las características de las variables.\n\n5. Visualiza los datos utilizando gráficos de dispersión, histogramas y diagramas de caja para identificar posibles relaciones entre las variables.\n\n6. Realiza un análisis de correlación entre las variables y visualiza los resultados.\n\n7. Comprueba la normalidad de la variable dependiente (precio de los autos) y de las variables independientes (kilómetros recorridos, eficiencia de combustible, capacidad del motor, potencia máxima, número de asientos) utilizando gráficos Q-Q y la prueba de Shapiro-Wilk. ¿Qué puedes concluir?\n\n7. Ajusta un modelo de regresión lineal múltiple para predecir el precio de los autos en función de las variables independientes (usando `statsmodels` y `scikit-learn`). ¿Cuál libreria prefieres y por qué? ¿Qué variables tienen un mayor impacto en el precio de los autos? ¿Qué otras variables podrían ser importantes para predecir el precio de los autos?\n\n8. Evalúa la calidad del ajuste del modelo y realiza predicciones para un conjunto de datos de prueba de tu elección.\n\n# Modelos Lineales Generalizados.\n\nRecordemos que el modelo de regresión lineal se puede representar de la siguiente forma:\n\n$$ y \\sim N(\\mu, \\sigma) $$\n\n$$ \\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p $$\n\nDonde $y$ es la variable dependiente, $\\mu$ es la media de la distribución normal, $\\beta_0$ es el intercepto, $\\beta_1, \\beta_2, \\ldots, \\beta_p$ son los coeficientes de las variables independientes $x_1, x_2, \\ldots, x_p$ respectivamente y $\\sigma$ es la desviación estándar de la distribución normal.\n\nEn el caso de los modelos lineales generalizados, la distribución de la variable dependiente $y$ no necesariamente tiene que ser normal. En general, la distribución de $y$ puede ser cualquier distribución de la familia exponencial. La forma general de un modelo lineal generalizado es la siguiente:\n\n$$ y \\sim f(g(\\mu), \\phi) $$\n\n$$ g(\\mu) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p $$\n\nDonde $y$ es la variable dependiente, $f$ es la función de densidad de probabilidad de la distribución de $y$, $g$ es la función de enlace, $\\mu$ es el valor esperado de la distribución de $y$, $\\beta_0$ es el intercepto, $\\beta_1, \\beta_2, \\ldots, \\beta_p$ son los coeficientes de las variables independientes $x_1, x_2, \\ldots, x_p$ respectivamente y $\\phi$ es un parámetro de dispersión.\n\nDentro de las distribuciones de la familia exponencial nos encontramos con:\n\n- Distribución Normal\n- Distribución Binomial\n- Distribución Poisson\n- Distribución Gamma\n\nEn el caso de la distribución normal, la función de enlace es la identidad, es decir, $g(\\mu) = \\mu$. En el caso de la distribución binomial, la función de enlace es la función logit, es decir, $g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right)$. En el caso de la distribución Poisson, la función de enlace es la función log, es decir, $g(\\mu) = \\log(\\mu)$. En el caso de la distribución Gamma, la función de enlace es la inversa, es decir, $g(\\mu) = \\frac{1}{\\mu}$.\n\nLas funciones de enlace tienen como objetivo enlazar la linealidad de los coeficientes $\\beta_0, \\beta_1, \\ldots, \\beta_p$ y las variables independientes con el valor esperado de la distribución de $y$ $\\mu$ de forma no lineal.\n\n## Regresión Logística\n\nPodemos derivar el modelo de regresión logística a partir del modelo lineal que ya vimos. En el caso de la regresión logística, la variable dependiente $y$ es binaria, es decir, $y \\in \\{0, 1\\}$. Por tanto podemos pensar en modelar la probabilidad de que $y = 1$ en función de las variables independientes $x_1, x_2, \\ldots, x_p$. El complemento de la probabilidad de que $y = 1$ es la probabilidad de que $y = 0$.\n\nAhora calculemos los \"odds\" de que $y = 1$:\n\n$$ \\text{odds} = \\frac{P(y = 1)}{P(y = 0)} $$\n\nLos \"odds\" son la razón de la probabilidad de que $y = 1$ entre la probabilidad de que $y = 0$. Si los \"odds\" son mayores a 1, entonces la probabilidad de que $y = 1$ es mayor que la probabilidad de que $y = 0. Si los \"odds\" son menores a 1, entonces la probabilidad de que $y = 0$ es mayor que la probabilidad de que $y = 1$.\n\nAhora vamos a suponer que los \"odds\" de que $y = 1$ son una función lineal de las variables independientes $x_1, x_2, \\ldots, x_p$:\n\n$$\\text{odds} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p$$\n\nUn problema que tienen los \"odds\" es que están desbalanceados. Por ejemplo, si los \"odds\" son 10, entonces la probabilidad de que $y = 1$ es 10 veces mayor que la probabilidad de que $y = 0$. Pero si los \"odds\" son 0.1, entonces la probabilidad de que $y = 1$ es 10 veces menor que la probabilidad de que $y = 0$. Para resolver este problema, vamos a tomar el logaritmo de los \"odds\" y vamos a modelar el logaritmo de los \"odds\" en función de las variables independientes $x_1, x_2, \\ldots, x_p$:\n\n$$\\log\\left(\\frac{P(y = 1)}{P(y = 0)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p$$\n\n$$\\log\\left(\\frac{P(y = 1)}{1 - P(y = 1)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p$$\n\nAhora solo tenemos que despejar $P(y = 1)$ paso a paso:\n\n$$\\log\\left(\\frac{P(y = 1)}{1 - P(y = 1)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p$$\n\n$$\\frac{P(y = 1)}{1 - P(y = 1)} = e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}$$\n\n$$P(y = 1) = (1 - P(y = 1))e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}$$\n\n$$P(y = 1) = e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p} - P(y = 1)e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}$$\n\n$$P(y = 1) + P(y = 1)e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p} = e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}$$\n\n$$P(y = 1)(1 + e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}) = e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}$$\n\n$$P(y = 1) = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}}$$\n\n$$P(y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p)}}$$\n\nEsta última ecuación es la función logística. La función logística es una función sigmoide que toma valores entre 0 y 1. Gráficamente, la función logística se ve de la siguiente forma:\n\n::: {#f053ebf9 .cell execution_count=21}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 1000)\ny = 1 / (1 + np.exp(-x))\n\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Función Logística')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Linear_models_files/figure-html/cell-21-output-1.png){width=589 height=450}\n:::\n:::\n\n\nAhora veamos como se ve la función logística dandole valor a dos coeficientes $\\beta_0 = 0$ y $\\beta_1 = 1$:\n\n::: {#25a8d52b .cell execution_count=22}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 1000)\nb0 = 5\nb1 = 10\n\ny = 1 / (1 + np.exp(-(b0 + b1 * x)))\n\nplt.plot(x, y, label=r'$y = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}$')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Función Logística')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Linear_models_files/figure-html/cell-22-output-1.png){width=589 height=450}\n:::\n:::\n\n\nAlgunas propiedades de la función logística son:\n\n- La función logística es simétrica respecto al punto $(0, 0.5)$.\n- La función logística es monótona creciente.\n- La función logística es acotada entre 0 y 1.\n\n### Estimar los coeficientes de la regresión logística\n\nPara estimar los coeficientes de la regresión logística, podemos utilizar el método de máxima verosimilitud. La función de verosimilitud es la probabilidad de observar los datos dados los coeficientes del modelo. La función de verosimilitud de la regresión logística es la siguiente:\n\n$$L(\\beta_0, \\beta_1, \\ldots, \\beta_p) = \\prod_{i=1}^{n} P(y_i = 1)^{y_i} (1 - P(y_i = 1))^{1 - y_i}$$\n\nDonde $n$ es el número de observaciones, $y_i$ es la variable dependiente de la observación $i$ y $P(y_i = 1)$ es la probabilidad de que la observación $i$ tome el valor 1. La función de verosimilitud es el producto de las probabilidades de que cada observación tome el valor 1 o 0.\n\nDado que los datos son independientes e idénticamente distribuidos, podemos tomar el logaritmo de la función de verosimilitud para simplificar los cálculos:\n\n$$\\log L(\\beta_0, \\beta_1, \\ldots, \\beta_p) = \\sum_{i=1}^{n} y_i \\log P(y_i = 1) + (1 - y_i) \\log (1 - P(y_i = 1))$$\n\nAhora solo tenemos que maximizar la función de verosimilitud logarítmica para encontrar los coeficientes del modelo. Para maximizar la función de verosimilitud logarítmica, podemos utilizar el método de Newton-Raphson o con descenso de gradiente. En la práctica, la mayoría de los paquetes de software utilizan el método de Newton-Raphson.\n\nDado que la explicación de la estimación de los coeficientes de la regresión logística es un poco extensa, vamos a ver un ejemplo en Python utilizando el paquete `statsmodels`.\n\n::: {#215773dc .cell execution_count=23}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# Generar datos\nnp.random.seed(0)\nn = 1000\nx1 = np.random.normal(0, 1, n)\nx2 = np.random.normal(0, 1, n)\n\n# Definir coeficientes\nb0 = 5\nb1 = 3\nb2 = 2\n\n# Generar variable dependiente\np = 1 / (1 + np.exp(-(b0 + b1 * x1 + b2 * x2)))\n\ny = np.random.binomial(1, p)\n\n# Crear DataFrame\ndf = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2})\n\n# Muestra de los datos\ndf.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y</th>\n      <th>x1</th>\n      <th>x2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.764052</td>\n      <td>0.555963</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.400157</td>\n      <td>0.892474</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0.978738</td>\n      <td>-0.422315</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2.240893</td>\n      <td>0.104714</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1.867558</td>\n      <td>0.228053</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>-0.977278</td>\n      <td>0.201480</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0.950088</td>\n      <td>0.540774</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>-0.151357</td>\n      <td>-1.818078</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>-0.103219</td>\n      <td>-0.049324</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>0.410599</td>\n      <td>0.239034</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAhora estimemos los coeficientes del modelo de regresión logística:\n\n::: {#aca36be3 .cell execution_count=24}\n``` {.python .cell-code}\n# Estimar modelo\ndata = sm.add_constant(df[['x1', 'x2']])\nmodel = sm.GLM(df['y'], data, family=sm.families.Binomial())\nresult = model.fit()\n\n# Mostrar resultados\nprint(result.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1000\nModel:                            GLM   Df Residuals:                      997\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -143.21\nDate:                Sat, 22 Jun 2024   Deviance:                       286.43\nTime:                        13:41:21   Pearson chi2:                 1.41e+03\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.2988\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.4077      0.442     12.228      0.000       4.541       6.274\nx1             3.2868      0.317     10.383      0.000       2.666       3.907\nx2             2.0070      0.248      8.093      0.000       1.521       2.493\n==============================================================================\n```\n:::\n:::\n\n\nComo antes tenemos que añaadir una columna de unos al DataFrame para poder estimar el intercepto del modelo. Después utilizamos la función `GLM` de `statsmodels` que estima distribuciones de la familia exponencial. En este caso, estamos utilizando la distribución binomial, con el parámetro `family=sm.families.Binomial()`. Finalmente, utilizamos el método `fit` para estimar los coeficientes del modelo.\n\nPodemos estimar la regresión logística con `scikit-learn`:\n\n::: {#0a90542c .cell execution_count=25}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\n\n# Estimar modelo\nmodel = LogisticRegression()\nmodel.fit(df[['x1', 'x2']], df['y'])\n\n# Imprimir coeficientes\nprint(f'Intercepto: {model.intercept_}')\nprint(f'Coeficientes: {model.coef_}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercepto: [4.94144067]\nCoeficientes: [[2.94011   1.7773892]]\n```\n:::\n:::\n\n\nPodemos probar que tan bueno es el modelo al verificar que predice con los datos que ya tenemos y compararlo con los datos reales:\n\n::: {#2a965aaa .cell execution_count=26}\n``` {.python .cell-code}\n# Predicción\ny_pred = model.predict(df[['x1', 'x2']])\n\n# Comparar predicciones con datos reales\ndf['y_pred'] = y_pred\ndf.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=54}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y</th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>y_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.764052</td>\n      <td>0.555963</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.400157</td>\n      <td>0.892474</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0.978738</td>\n      <td>-0.422315</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2.240893</td>\n      <td>0.104714</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1.867558</td>\n      <td>0.228053</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>-0.977278</td>\n      <td>0.201480</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0.950088</td>\n      <td>0.540774</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>-0.151357</td>\n      <td>-1.818078</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>-0.103219</td>\n      <td>-0.049324</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>0.410599</td>\n      <td>0.239034</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nPara comparar modelos de regresión logística, podemos utilizar métricas como la precisión, la sensibilidad, la especificidad, el valor predictivo positivo y el valor predictivo negativo. Estas métricas nos permiten evaluar la capacidad del modelo para predecir correctamente los valores positivos y negativos y se basan en la **matriz de confusión**.\n\n::: {#503b7d92 .cell execution_count=27}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\n\n# Matriz de confusión\ncm = confusion_matrix(df['y'], df['y_pred'])\nprint(cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 52  46]\n [ 11 891]]\n```\n:::\n:::\n\n\nLa matriz de confusión es una tabla que muestra el número de verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos del modelo. A partir de la matriz de confusión, podemos calcular métricas como la precisión, la sensibilidad, la especificidad, el valor predictivo positivo y el valor predictivo negativo.\n\nLa diagonal de la matriz de confusión contiene los valores correctos, es decir, los verdaderos positivos y los verdaderos negativos. Los valores fuera de la diagonal contienen los valores incorrectos, es decir, los falsos positivos y los falsos negativos.\n\n \\begin{array}{|c|c|}\n\\hline\n\\text{Verdaderos Positivos (TP)} & \\text{Falsos Positivos (FP)} \\\\\n\\hline\n\\text{Falsos Negativos (FN)} & \\text{Verdaderos Negativos (TN)} \\\\\n\\hline\n\\end{array}\n\nLa **precisión** es la proporción de predicciones correctas entre todas las predicciones realizadas por el modelo:\n\n$$\\text{Precisión} = \\frac{TP + TN}{TP + FP + FN + TN}$$\n\nLa **sensibilidad** es la proporción de verdaderos positivos entre todos los valores positivos reales:\n\n$$\\text{Sensibilidad} = \\frac{TP}{TP + FN}$$\n\nLa **especificidad** es la proporción de verdaderos negativos entre todos los valores negativos reales:\n\n$$\\text{Especificidad} = \\frac{TN}{TN + FP}$$\n\nEl **valor predictivo positivo** es la proporción de verdaderos positivos entre todas las predicciones positivas realizadas por el modelo:\n\n$$\\text{Valor Predictivo Positivo} = \\frac{TP}{TP + FP}$$\n\nEl **valor predictivo negativo** es la proporción de verdaderos negativos entre todas las predicciones negativas realizadas por el modelo:\n\n$$\\text{Valor Predictivo Negativo} = \\frac{TN}{TN + FN}$$\n\nEn nuestro modelo de regresión logística tenemos los siguientes valores:\n\n::: {#58d647f3 .cell execution_count=28}\n``` {.python .cell-code}\n# Precisión\nprecision = (cm[0, 0] + cm[1, 1]) / np.sum(cm)\nprint(f'Precisión: {precision}')\n\n# Especificidad\nspecificity = cm[1, 1] / (cm[1, 1] + cm[0, 1])\nprint(f'Especificidad: {specificity}')\n\n# Sensibilidad\nsensitivity = cm[0, 0] / (cm[0, 0] + cm[1, 0])\nprint(f'Sensibilidad: {sensitivity}')\n\n# Valor Predictivo Positivo\nppv = cm[0, 0] / (cm[0, 0] + cm[0, 1])\nprint(f'Valor Predictivo Positivo: {ppv}')\n\n# Valor Predictivo Negativo\nnpv = cm[1, 1] / (cm[1, 1] + cm[1, 0])\nprint(f'Valor Predictivo Negativo: {npv}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecisión: 0.943\nEspecificidad: 0.9509071504802561\nSensibilidad: 0.8253968253968254\nValor Predictivo Positivo: 0.5306122448979592\nValor Predictivo Negativo: 0.9878048780487805\n```\n:::\n:::\n\n\nEstas métricas son usadas también en la evaluación de modelos de clasificación como lo son las redes neuronales.\n\nOtras métricas comunes para evaluar modelos de clasificación son el área bajo la curva ROC (AUC-ROC) y el área bajo la curva PR (AUC-PR). El AUC-ROC mide la capacidad del modelo para distinguir entre las clases positiva y negativa, mientras que el AUC-PR mide la precisión del modelo en la clase positiva.\n\n::: {#d05246a2 .cell execution_count=29}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n\n# Área bajo la curva ROC\nroc_auc = roc_auc_score(df['y'], df['y_pred'])\n\n# Área bajo la curva PR\npr_auc = average_precision_score(df['y'], df['y_pred'])\n\nprint(f'Área bajo la curva ROC: {roc_auc}')\nprint(f'Área bajo la curva PR: {pr_auc}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nÁrea bajo la curva ROC: 0.7592085614733698\nÁrea bajo la curva PR: 0.9503107218158627\n```\n:::\n:::\n\n\nEl AUC-ROC y el AUC-PR son valores entre 0 y 1, donde un valor de 1 indica un modelo perfecto y un valor de 0.5 indica un modelo aleatorio.\n\nPodemos graficar la curva ROC y la curva PR para visualizar la calidad del modelo de regresión logística.\n\n::: {#de710c88 .cell execution_count=30}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_curve, precision_recall_curve\n\n# Curva ROC\nfpr, tpr, _ = roc_curve(df['y'], df['y_pred'])\n\n# Curva PR\nprecision, recall, _ = precision_recall_curve(df['y'], df['y_pred'])\n\n# Graficar curva ROC\nfig, ax = plt.subplots(1, 2, figsize=(10, 6))\n\nax[0].plot(fpr, tpr)\nax[0].plot([0, 1], [0, 1], linestyle='--', color='gray')\nax[0].set_xlabel('Tasa de Falsos Positivos')\nax[0].set_ylabel('Tasa de Verdaderos Positivos')\nax[0].set_title('Curva ROC')\n\n# Graficar curva PR\nax[1].plot(recall, precision)\nax[1].set_xlabel('Recall')\nax[1].set_ylabel('Precision')\nax[1].set_title('Curva PR')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Linear_models_files/figure-html/cell-30-output-1.png){width=812 height=523}\n:::\n:::\n\n\nLa forma de la curva ROC y la curva PR nos permite evaluar la calidad del modelo de regresión logística. Una curva ROC que se acerca al punto (0, 1) y una curva PR que se acerca al punto (1, 1) indican un modelo de alta calidad. Cada punto en la curva ROC representa una combinación de la tasa de verdaderos positivos y la tasa de falsos positivos del modelo, mientras que cada punto en la curva PR representa una combinación de la precisión y el recall del modelo. El **recall** es la sensibilidad del modelo, es decir, la proporción de verdaderos positivos entre todos los valores positivos reales.\n\n",
    "supporting": [
      "Linear_models_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}