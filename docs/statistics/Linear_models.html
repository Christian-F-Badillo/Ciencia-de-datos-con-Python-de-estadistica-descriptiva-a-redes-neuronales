<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Badillo">

<title>Ciencia de Datos con Python - Modelos Lineales</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../statistics/hipothesis_testing.html" rel="next">
<link href="../statistics/data-visualization.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Ciencia de Datos con Python - Modelos Lineales">
<meta property="og:description" content="">
<meta property="og:site_name" content="Ciencia de Datos con Python">
<meta property="og:locale" content="es_MX">
<meta name="twitter:title" content="Ciencia de Datos con Python - Modelos Lineales">
<meta name="twitter:description" content="">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../statistics/data_manipulation.html">Manejo de Datos y Estadística con Python</a></li><li class="breadcrumb-item"><a href="../statistics/Linear_models.html">Modelos Lineales</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../img/logo-members/Lab25_logo_2015.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Ciencia de Datos con Python</a> 
        <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://github.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales">
            Source Code
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Alternar modo oscuro"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inicio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acerca</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Fundamentos de Python</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_python/Cloud.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Instalación de Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_python/Variables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Declaración de Variables y Operaciones</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_python/functions_and_loops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Funciones y Bucles de Control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_python/data_structures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Estructuras de Datos</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Manejo de Datos y Estadística con Python</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../statistics/data_manipulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Manipulación de datos con Pandas</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../statistics/data-visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualización de datos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../statistics/Linear_models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Modelos Lineales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../statistics/hipothesis_testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prueba de Hipótesis</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Redes Neuronales</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../neural_neworks/classes_python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Programación Orientada a Objetos en Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../neural_neworks/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introducción a Redes Neuronales en Python</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de Contenido</h2>
   
  <ul>
  <li><a href="#modelos-lineales" id="toc-modelos-lineales" class="nav-link active" data-scroll-target="#modelos-lineales">Modelos Lineales</a></li>
  <li><a href="#regresión-lineal-simple" id="toc-regresión-lineal-simple" class="nav-link" data-scroll-target="#regresión-lineal-simple">Regresión Lineal Simple</a>
  <ul class="collapse">
  <li><a href="#regresión-lineal-simple-por-mínimos-cuadrados" id="toc-regresión-lineal-simple-por-mínimos-cuadrados" class="nav-link" data-scroll-target="#regresión-lineal-simple-por-mínimos-cuadrados">Regresión Lineal Simple por Mínimos Cuadrados</a>
  <ul class="collapse">
  <li><a href="#demostración-de-la-fórmula-de-mínimos-cuadrados" id="toc-demostración-de-la-fórmula-de-mínimos-cuadrados" class="nav-link" data-scroll-target="#demostración-de-la-fórmula-de-mínimos-cuadrados">Demostración de la Fórmula de Mínimos Cuadrados*</a></li>
  </ul></li>
  <li><a href="#regresión-lineal-simple-por-descenso-del-gradiente" id="toc-regresión-lineal-simple-por-descenso-del-gradiente" class="nav-link" data-scroll-target="#regresión-lineal-simple-por-descenso-del-gradiente">Regresión Lineal Simple por Descenso del Gradiente</a>
  <ul class="collapse">
  <li><a href="#derivación-de-las-ecuaciones-de-descenso-del-gradiente" id="toc-derivación-de-las-ecuaciones-de-descenso-del-gradiente" class="nav-link" data-scroll-target="#derivación-de-las-ecuaciones-de-descenso-del-gradiente">Derivación de las Ecuaciones de Descenso del Gradiente*</a></li>
  <li><a href="#implementación-del-descenso-del-gradiente" id="toc-implementación-del-descenso-del-gradiente" class="nav-link" data-scroll-target="#implementación-del-descenso-del-gradiente">Implementación del Descenso del Gradiente</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#regresión-lineal-múltiple" id="toc-regresión-lineal-múltiple" class="nav-link" data-scroll-target="#regresión-lineal-múltiple">Regresión Lineal Múltiple</a>
  <ul class="collapse">
  <li><a href="#regresión-lineal-múltiple-por-mínimos-cuadrados" id="toc-regresión-lineal-múltiple-por-mínimos-cuadrados" class="nav-link" data-scroll-target="#regresión-lineal-múltiple-por-mínimos-cuadrados">Regresión Lineal Múltiple por Mínimos Cuadrados</a></li>
  </ul></li>
  <li><a href="#regresión-lineal-con-scikit-learn" id="toc-regresión-lineal-con-scikit-learn" class="nav-link" data-scroll-target="#regresión-lineal-con-scikit-learn">Regresión Lineal con Scikit-Learn</a></li>
  <li><a href="#pruebas-de-normalidad" id="toc-pruebas-de-normalidad" class="nav-link" data-scroll-target="#pruebas-de-normalidad">Pruebas de Normalidad</a>
  <ul class="collapse">
  <li><a href="#gráfico-q-q" id="toc-gráfico-q-q" class="nav-link" data-scroll-target="#gráfico-q-q">Gráfico Q-Q</a></li>
  <li><a href="#prueba-de-shapiro-wilk" id="toc-prueba-de-shapiro-wilk" class="nav-link" data-scroll-target="#prueba-de-shapiro-wilk">Prueba de Shapiro-Wilk</a></li>
  </ul></li>
  <li><a href="#ejercicios" id="toc-ejercicios" class="nav-link" data-scroll-target="#ejercicios">Ejercicios</a></li>
  <li><a href="#modelos-lineales-generalizados." id="toc-modelos-lineales-generalizados." class="nav-link" data-scroll-target="#modelos-lineales-generalizados.">Modelos Lineales Generalizados.</a>
  <ul class="collapse">
  <li><a href="#regresión-logística" id="toc-regresión-logística" class="nav-link" data-scroll-target="#regresión-logística">Regresión Logística</a>
  <ul class="collapse">
  <li><a href="#estimar-los-coeficientes-de-la-regresión-logística" id="toc-estimar-los-coeficientes-de-la-regresión-logística" class="nav-link" data-scroll-target="#estimar-los-coeficientes-de-la-regresión-logística">Estimar los coeficientes de la regresión logística</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/edit/main/statistics/Linear_models.qmd" class="toc-action"><i class="bi bi-github"></i>Editar esta página</a></li><li><a href="https://github.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/issues/new" class="toc-action"><i class="bi empty"></i>Informar de un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../statistics/data_manipulation.html">Manejo de Datos y Estadística con Python</a></li><li class="breadcrumb-item"><a href="../statistics/Linear_models.html">Modelos Lineales</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Modelos Lineales</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Ver código</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Mostrar todo el código</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Ocultar todo el código</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button" data-quarto-source-url="https://github.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/blob/main/statistics/Linear_models.qmd">Ver el código fuente</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Autor/a</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Badillo </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Fecha de publicación</div>
    <div class="quarto-title-meta-contents">
      <p class="date">23 de junio de 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="modelos-lineales" class="level1">
<h1>Modelos Lineales</h1>
<p>Los modelos lineales son una clase de modelos estadísticos que asumen que la relación entre las variables dependientes e independientes es lineal. En este tutorial, aprenderemos cómo ajustar un modelo lineal a un conjunto de datos y cómo interpretar los resultados.</p>
<p>Entre los modelos lineales más comunes se encuentran la regresión lineal simple y la regresión lineal múltiple. Se pueden utilizar para predecir una variable dependiente a partir de una o más variables independientes.</p>
<p>Otros modelos lineales incluyen algunas pruebas de hipótesis, como el análisis de varianza (ANOVA) y la comparación de medias (prueba t).</p>
<p>Todos estos modelos caen en la categoría de modelos lineales generalizados (GLM), que son una extensión de los modelos lineales tradicionales que permiten una mayor flexibilidad en la especificación de la distribución de los errores y la función de enlace.</p>
</section>
<section id="regresión-lineal-simple" class="level1">
<h1>Regresión Lineal Simple</h1>
<p>El modelo de regresión lineal simple es uno de los modelos lineales más simples y fáciles de entender. En este modelo, se asume que la relación entre la variable dependiente y la variable independiente es lineal. La ecuación de regresión lineal simple se puede expresar de la siguiente manera:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1X + \epsilon\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(Y\)</span> es la variable dependiente</li>
<li><span class="math inline">\(X\)</span> es la variable independiente</li>
<li><span class="math inline">\(\beta_0\)</span> es la intersección</li>
<li><span class="math inline">\(\beta_1\)</span> es la pendiente</li>
<li><span class="math inline">\(\epsilon\)</span> es el error</li>
</ul>
<p>El objetivo de la regresión lineal simple es encontrar los valores de <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> que minimizan la suma de los cuadrados de los errores (SSE), que se define como:</p>
<p><span class="math display">\[SSE = \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(Y_i\)</span> es el valor observado de la variable dependiente</li>
<li><span class="math inline">\(\hat{Y_i}\)</span> es el valor predicho de la variable dependiente</li>
<li><span class="math inline">\(n\)</span> es el número de observaciones</li>
</ul>
<p>Una vez que se han estimado los valores de <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span>, se pueden utilizar para predecir los valores de la variable dependiente para nuevas observaciones.</p>
<section id="regresión-lineal-simple-por-mínimos-cuadrados" class="level2">
<h2 class="anchored" data-anchor-id="regresión-lineal-simple-por-mínimos-cuadrados">Regresión Lineal Simple por Mínimos Cuadrados</h2>
<p>El método más común para ajustar un modelo de regresión lineal simple es el método de mínimos cuadrados. Este método consiste en encontrar los valores de <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> que minimizan la suma de los cuadrados de los errores. La fórmula para calcular los valores de <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> es la siguiente:</p>
<p><span class="math display">\[\beta_1 = \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n} (X_i - \bar{X})^2}\]</span></p>
<p><span class="math display">\[\beta_0 = \bar{Y} - \beta_1\bar{X}\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(\bar{X}\)</span> es la media de la variable independiente</li>
<li><span class="math inline">\(\bar{Y}\)</span> es la media de la variable dependiente</li>
<li><span class="math inline">\(n\)</span> es el número de observaciones</li>
</ul>
<p>Usemos los datos de la altura y el peso de un grupo de personas para ajustar un modelo de regresión lineal simple y predecir el peso de una persona en función de su altura.</p>
<div id="a61bd115" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co"># Datos</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>altura <span class="op">=</span> np.array([<span class="dv">150</span>, <span class="dv">160</span>, <span class="dv">170</span>, <span class="dv">180</span>, <span class="dv">190</span>])</span>
<span id="cb1-6"><a href="#cb1-6"></a>peso <span class="op">=</span> np.array([<span class="dv">50</span>, <span class="dv">60</span>, <span class="dv">70</span>, <span class="dv">80</span>, <span class="dv">90</span>])</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co"># Calcular medias</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>altura_media <span class="op">=</span> np.mean(altura)</span>
<span id="cb1-10"><a href="#cb1-10"></a>peso_media <span class="op">=</span> np.mean(peso)</span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co"># Calcular beta_1</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>beta_1 <span class="op">=</span> np.<span class="bu">sum</span>((altura <span class="op">-</span> altura_media) <span class="op">*</span> (peso <span class="op">-</span> peso_media)) <span class="op">/</span> np.<span class="bu">sum</span>((altura <span class="op">-</span> altura_media) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb1-14"><a href="#cb1-14"></a></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co"># Calcular beta_0</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>beta_0 <span class="op">=</span> peso_media <span class="op">-</span> beta_1 <span class="op">*</span> altura_media</span>
<span id="cb1-17"><a href="#cb1-17"></a></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="co"># Imprimir coeficientes</span></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="bu">print</span>(<span class="ss">f'beta_0: </span><span class="sc">{</span>beta_0<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="bu">print</span>(<span class="ss">f'beta_1: </span><span class="sc">{</span>beta_1<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>beta_0: -100.0
beta_1: 1.0</code></pre>
</div>
</div>
<p>Podemos graficar los datos y la línea de regresión para visualizar la relación entre la altura y el peso.</p>
<div id="510884d2" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Graficar datos</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>plt.scatter(altura, peso, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb3-3"><a href="#cb3-3"></a>plt.plot(altura, beta_0 <span class="op">+</span> beta_1 <span class="op">*</span> altura, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb3-4"><a href="#cb3-4"></a>plt.xlabel(<span class="st">'Altura'</span>)</span>
<span id="cb3-5"><a href="#cb3-5"></a>plt.ylabel(<span class="st">'Peso'</span>)</span>
<span id="cb3-6"><a href="#cb3-6"></a>plt.title(<span class="st">'Regresión Lineal Simple'</span>)</span>
<span id="cb3-7"><a href="#cb3-7"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Linear_models_files/figure-html/cell-3-output-1.png" width="585" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Ahora podemos crear datos datos mas realistas y ajustar un modelo de regresión lineal simple.</p>
<div id="4ddc4a60" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Crear datos</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a>altura <span class="op">=</span> np.random.normal(<span class="dv">170</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb4-5"><a href="#cb4-5"></a>peso <span class="op">=</span> <span class="dv">50</span> <span class="op">+</span> <span class="fl">0.05</span> <span class="op">*</span> altura <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="co"># Calcular medias</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>altura_media <span class="op">=</span> np.mean(altura)</span>
<span id="cb4-9"><a href="#cb4-9"></a>peso_media <span class="op">=</span> np.mean(peso)</span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co"># Calcular beta_1</span></span>
<span id="cb4-12"><a href="#cb4-12"></a>beta_1 <span class="op">=</span> np.<span class="bu">sum</span>((altura <span class="op">-</span> altura_media) <span class="op">*</span> (peso <span class="op">-</span> peso_media)) <span class="op">/</span> np.<span class="bu">sum</span>((altura <span class="op">-</span> altura_media) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb4-13"><a href="#cb4-13"></a></span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="co"># Calcular beta_0</span></span>
<span id="cb4-15"><a href="#cb4-15"></a>beta_0 <span class="op">=</span> peso_media <span class="op">-</span> beta_1 <span class="op">*</span> altura_media</span>
<span id="cb4-16"><a href="#cb4-16"></a></span>
<span id="cb4-17"><a href="#cb4-17"></a><span class="co"># Imprimir coeficientes</span></span>
<span id="cb4-18"><a href="#cb4-18"></a><span class="bu">print</span>(<span class="ss">f'beta_0: </span><span class="sc">{</span>beta_0<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-19"><a href="#cb4-19"></a><span class="bu">print</span>(<span class="ss">f'beta_1: </span><span class="sc">{</span>beta_1<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-20"><a href="#cb4-20"></a></span>
<span id="cb4-21"><a href="#cb4-21"></a><span class="co"># Graficar datos</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb4-23"><a href="#cb4-23"></a>plt.scatter(altura, peso, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb4-24"><a href="#cb4-24"></a>plt.plot(altura, beta_0 <span class="op">+</span> beta_1 <span class="op">*</span> altura, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb4-25"><a href="#cb4-25"></a>plt.xlabel(<span class="st">'Altura'</span>)</span>
<span id="cb4-26"><a href="#cb4-26"></a>plt.ylabel(<span class="st">'Peso'</span>)</span>
<span id="cb4-27"><a href="#cb4-27"></a>plt.title(<span class="st">'Regresión Lineal Simple'</span>)</span>
<span id="cb4-28"><a href="#cb4-28"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>beta_0: 40.626398573820204
beta_1: 0.10734921677319048</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Linear_models_files/figure-html/cell-4-output-2.png" width="808" height="524" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Usando <code>seaborn</code> podemos ajustar un modelo de regresión lineal simple y visualizar los resultados.</p>
<div id="36f29593" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># Crear DataFrame</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'altura'</span>: altura, <span class="st">'peso'</span>: peso})</span>
<span id="cb6-6"><a href="#cb6-6"></a></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb6-8"><a href="#cb6-8"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb6-9"><a href="#cb6-9"></a>sns.lmplot(x<span class="op">=</span><span class="st">'altura'</span>, y<span class="op">=</span><span class="st">'peso'</span>, data<span class="op">=</span>df)</span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>&lt;Figure size 960x576 with 0 Axes&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Linear_models_files/figure-html/cell-5-output-2.png" width="470" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nos muestra la gráfica de la regresión lineal simple y su intervalo de confianza.</p>
<p>La libreria <code>statsmodels</code> nos permite ajustar un modelo de regresión lineal simple y obtener un resumen de los resultados.</p>
<p>Puedes instalar la libreria usando el siguiente comando:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1"></a><span class="ex">!pip</span> install statsmodels</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="54dcb84d" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>X <span class="op">=</span> sm.add_constant(altura)</span>
<span id="cb9-5"><a href="#cb9-5"></a>model <span class="op">=</span> sm.OLS(peso, X).fit()</span>
<span id="cb9-6"><a href="#cb9-6"></a></span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="co"># Imprimir resumen</span></span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="bu">print</span>(model.summary())</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.042
Model:                            OLS   Adj. R-squared:                  0.033
Method:                 Least Squares   F-statistic:                     4.341
Date:                Sat, 22 Jun 2024   Prob (F-statistic):             0.0398
Time:                        13:41:19   Log-Likelihood:                -305.62
No. Observations:                 100   AIC:                             615.2
Df Residuals:                      98   BIC:                             620.4
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         40.6264      8.805      4.614      0.000      23.152      58.100
x1             0.1073      0.052      2.083      0.040       0.005       0.210
==============================================================================
Omnibus:                        5.184   Durbin-Watson:                   1.995
Prob(Omnibus):                  0.075   Jarque-Bera (JB):                3.000
Skew:                           0.210   Prob(JB):                        0.223
Kurtosis:                       2.262   Cond. No.                     2.90e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 2.9e+03. This might indicate that there are
strong multicollinearity or other numerical problems.</code></pre>
</div>
</div>
<p>Para ajustar el modelo primero agregamos una columna de unos a la matriz de características <code>X</code> y luego ajustamos el modelo usando la función <code>OLS</code> de <code>statsmodels</code>. Finalmente, imprimimos un resumen de los resultados del modelo.</p>
<p>Imprimamos el valor de X</p>
<div id="0d3f0267" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>X</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>array([[  1.        , 187.64052346],
       [  1.        , 174.00157208],
       [  1.        , 179.78737984],
       [  1.        , 192.40893199],
       [  1.        , 188.6755799 ],
       [  1.        , 160.2272212 ],
       [  1.        , 179.50088418],
       [  1.        , 168.48642792],
       [  1.        , 168.96781148],
       [  1.        , 174.10598502],
       [  1.        , 171.44043571],
       [  1.        , 184.54273507],
       [  1.        , 177.61037725],
       [  1.        , 171.21675016],
       [  1.        , 174.43863233],
       [  1.        , 173.33674327],
       [  1.        , 184.94079073],
       [  1.        , 167.94841736],
       [  1.        , 173.13067702],
       [  1.        , 161.45904261],
       [  1.        , 144.47010184],
       [  1.        , 176.53618595],
       [  1.        , 178.64436199],
       [  1.        , 162.5783498 ],
       [  1.        , 192.69754624],
       [  1.        , 155.45634325],
       [  1.        , 170.45758517],
       [  1.        , 168.1281615 ],
       [  1.        , 185.32779214],
       [  1.        , 184.6935877 ],
       [  1.        , 171.54947426],
       [  1.        , 173.7816252 ],
       [  1.        , 161.12214252],
       [  1.        , 150.19203532],
       [  1.        , 166.52087851],
       [  1.        , 171.56348969],
       [  1.        , 182.30290681],
       [  1.        , 182.02379849],
       [  1.        , 166.12673183],
       [  1.        , 166.97697249],
       [  1.        , 159.51447035],
       [  1.        , 155.79982063],
       [  1.        , 152.93729809],
       [  1.        , 189.50775395],
       [  1.        , 164.90347818],
       [  1.        , 165.61925698],
       [  1.        , 157.4720464 ],
       [  1.        , 177.77490356],
       [  1.        , 153.86102152],
       [  1.        , 167.8725972 ],
       [  1.        , 161.04533439],
       [  1.        , 173.86902498],
       [  1.        , 164.89194862],
       [  1.        , 158.19367816],
       [  1.        , 169.71817772],
       [  1.        , 174.28331871],
       [  1.        , 170.66517222],
       [  1.        , 173.02471898],
       [  1.        , 163.65677906],
       [  1.        , 166.37258834],
       [  1.        , 163.27539552],
       [  1.        , 166.40446838],
       [  1.        , 161.86853718],
       [  1.        , 152.73717398],
       [  1.        , 171.77426142],
       [  1.        , 165.98219064],
       [  1.        , 153.69801653],
       [  1.        , 174.62782256],
       [  1.        , 160.92701636],
       [  1.        , 170.51945396],
       [  1.        , 177.29090562],
       [  1.        , 171.28982911],
       [  1.        , 181.39400685],
       [  1.        , 157.6517418 ],
       [  1.        , 174.02341641],
       [  1.        , 163.15189909],
       [  1.        , 161.29202851],
       [  1.        , 164.21150335],
       [  1.        , 166.88447468],
       [  1.        , 170.56165342],
       [  1.        , 158.34850159],
       [  1.        , 179.00826487],
       [  1.        , 174.6566244 ],
       [  1.        , 154.63756314],
       [  1.        , 184.88252194],
       [  1.        , 188.95889176],
       [  1.        , 181.78779571],
       [  1.        , 168.20075164],
       [  1.        , 159.29247378],
       [  1.        , 180.54451727],
       [  1.        , 165.96823053],
       [  1.        , 182.2244507 ],
       [  1.        , 172.08274978],
       [  1.        , 179.76639036],
       [  1.        , 173.56366397],
       [  1.        , 177.06573168],
       [  1.        , 170.10500021],
       [  1.        , 187.85870494],
       [  1.        , 171.26912093],
       [  1.        , 174.01989363]])</code></pre>
</div>
</div>
<p>Que corresponde a lo que se denomina como <strong>matriz de diseño</strong>. Que es una matriz que contiene las variables independientes y una columna de unos que representa la intersección. Podemos usar esta matriz para ajustar el modelo de regresión lineal simple por medio de la siguiente fórmula:</p>
<p><span class="math display">\[Y = X\beta + \epsilon\]</span></p>
<p>Y usando el método de mínimos cuadrados para encontrar los valores de <span class="math inline">\(\beta\)</span> que minimizan la suma de los cuadrados de los errores, nos da la siguiente fórmula:</p>
<p><span class="math display">\[\beta = (X^TX)^{-1}X^TY\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(Y\)</span> es el vector de la variable dependiente</li>
<li><span class="math inline">\(X\)</span> es la matriz de diseño</li>
<li><span class="math inline">\(\beta\)</span> es el vector de coeficientes</li>
</ul>
<p>Hagamos esto en Python.</p>
<div id="cf8e4233" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>X <span class="op">=</span> np.column_stack((np.ones(<span class="bu">len</span>(altura)), altura))</span>
<span id="cb13-3"><a href="#cb13-3"></a></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="bu">print</span>(X)</span>
<span id="cb13-5"><a href="#cb13-5"></a></span>
<span id="cb13-6"><a href="#cb13-6"></a>beta <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">@</span> X.T <span class="op">@</span> peso</span>
<span id="cb13-7"><a href="#cb13-7"></a></span>
<span id="cb13-8"><a href="#cb13-8"></a><span class="co"># Imprimir coeficientes</span></span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="bu">print</span>(<span class="ss">f'beta_0: </span><span class="sc">{</span>beta[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="bu">print</span>(<span class="ss">f'beta_1: </span><span class="sc">{</span>beta[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[[  1.         187.64052346]
 [  1.         174.00157208]
 [  1.         179.78737984]
 [  1.         192.40893199]
 [  1.         188.6755799 ]
 [  1.         160.2272212 ]
 [  1.         179.50088418]
 [  1.         168.48642792]
 [  1.         168.96781148]
 [  1.         174.10598502]
 [  1.         171.44043571]
 [  1.         184.54273507]
 [  1.         177.61037725]
 [  1.         171.21675016]
 [  1.         174.43863233]
 [  1.         173.33674327]
 [  1.         184.94079073]
 [  1.         167.94841736]
 [  1.         173.13067702]
 [  1.         161.45904261]
 [  1.         144.47010184]
 [  1.         176.53618595]
 [  1.         178.64436199]
 [  1.         162.5783498 ]
 [  1.         192.69754624]
 [  1.         155.45634325]
 [  1.         170.45758517]
 [  1.         168.1281615 ]
 [  1.         185.32779214]
 [  1.         184.6935877 ]
 [  1.         171.54947426]
 [  1.         173.7816252 ]
 [  1.         161.12214252]
 [  1.         150.19203532]
 [  1.         166.52087851]
 [  1.         171.56348969]
 [  1.         182.30290681]
 [  1.         182.02379849]
 [  1.         166.12673183]
 [  1.         166.97697249]
 [  1.         159.51447035]
 [  1.         155.79982063]
 [  1.         152.93729809]
 [  1.         189.50775395]
 [  1.         164.90347818]
 [  1.         165.61925698]
 [  1.         157.4720464 ]
 [  1.         177.77490356]
 [  1.         153.86102152]
 [  1.         167.8725972 ]
 [  1.         161.04533439]
 [  1.         173.86902498]
 [  1.         164.89194862]
 [  1.         158.19367816]
 [  1.         169.71817772]
 [  1.         174.28331871]
 [  1.         170.66517222]
 [  1.         173.02471898]
 [  1.         163.65677906]
 [  1.         166.37258834]
 [  1.         163.27539552]
 [  1.         166.40446838]
 [  1.         161.86853718]
 [  1.         152.73717398]
 [  1.         171.77426142]
 [  1.         165.98219064]
 [  1.         153.69801653]
 [  1.         174.62782256]
 [  1.         160.92701636]
 [  1.         170.51945396]
 [  1.         177.29090562]
 [  1.         171.28982911]
 [  1.         181.39400685]
 [  1.         157.6517418 ]
 [  1.         174.02341641]
 [  1.         163.15189909]
 [  1.         161.29202851]
 [  1.         164.21150335]
 [  1.         166.88447468]
 [  1.         170.56165342]
 [  1.         158.34850159]
 [  1.         179.00826487]
 [  1.         174.6566244 ]
 [  1.         154.63756314]
 [  1.         184.88252194]
 [  1.         188.95889176]
 [  1.         181.78779571]
 [  1.         168.20075164]
 [  1.         159.29247378]
 [  1.         180.54451727]
 [  1.         165.96823053]
 [  1.         182.2244507 ]
 [  1.         172.08274978]
 [  1.         179.76639036]
 [  1.         173.56366397]
 [  1.         177.06573168]
 [  1.         170.10500021]
 [  1.         187.85870494]
 [  1.         171.26912093]
 [  1.         174.01989363]]
beta_0: 40.62639857381858
beta_1: 0.10734921677318345</code></pre>
</div>
</div>
<section id="demostración-de-la-fórmula-de-mínimos-cuadrados" class="level3">
<h3 class="anchored" data-anchor-id="demostración-de-la-fórmula-de-mínimos-cuadrados">Demostración de la Fórmula de Mínimos Cuadrados*</h3>
<p>Tenemos el modelo de regresión lineal general:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1X + ... + \beta_nX_n + \epsilon\]</span></p>
<p>La función de costo que queremos minimizar es el error cuadrático residual (RSS), que se define como:</p>
<p><span class="math display">\[RSS = \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2\]</span></p>
<p>En forma matricial, la función de costo se puede expresar como:</p>
<p><span class="math display">\[RSS = (Y - X\beta)^T(Y - X\beta)\]</span></p>
<p>Que se conoce como la forma cuadrática de la función de costo. Para minimizar la función de costo, tomamos la derivada de la función de costo con respecto a <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[\begin{align*}

\frac{\partial RSS}{\partial \beta} &amp; = \frac{\partial}{\partial \beta} (Y - X\beta)^T(Y - X\beta) \\
&amp; = \frac{\partial}{\partial \beta} (Y^TY - Y^TX\beta - (X\beta)^TY + (X\beta)^TX\beta) \\
&amp; = \frac{\partial}{\partial \beta} (Y^TY - Y^TX\beta - \beta^TX^TY + \beta^TX^TX\beta) \\
&amp; = \frac{\partial}{\partial \beta} Y^TY - \frac{\partial}{\partial \beta} Y^TX\beta - \frac{\partial}{\partial \beta} \beta^TX^TY + \frac{\partial}{\partial \beta} \beta^TX^TX\beta \\
&amp; = 0 - X^TY - X^TY + 2X^TX\beta \\
&amp; = -2X^TY + 2X^TX\beta \\
&amp; = 2X^T(X\beta - Y) \\
\end{align*}\]</span></p>
<p>Igualamos la derivada de la función de costo a cero para encontrar el mínimo:</p>
<p><span class="math display">\[\frac{\partial RSS}{\partial \beta} = 0\]</span></p>
<p><span class="math display">\[2X^T(X\beta - Y) = 0\]</span></p>
<p>Resolviendo para <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[\begin{align*}
2X^T(X\beta - Y) &amp; = 0 \\
X^T(X\beta - Y) &amp; = 0 \\
X^TX\beta - X^TY &amp; = 0 \\
X^TX\beta &amp; = X^TY \\
\beta &amp; = (X^TX)^{-1}X^TY \\
\end{align*}\]</span></p>
</section>
</section>
<section id="regresión-lineal-simple-por-descenso-del-gradiente" class="level2">
<h2 class="anchored" data-anchor-id="regresión-lineal-simple-por-descenso-del-gradiente">Regresión Lineal Simple por Descenso del Gradiente</h2>
<p>Otra forma de ajustar un modelo de regresión lineal simple es mediante el descenso del gradiente. Este método consiste en ajustar los valores de <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> iterativamente para minimizar una función de costo. La función de costo que se utiliza en la regresión lineal simple es el error cuadrático medio (MSE), que se define como:</p>
<p><span class="math display">\[MSE = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(Y_i\)</span> es el valor observado de la variable dependiente</li>
<li><span class="math inline">\(\hat{Y_i}\)</span> es el valor predicho de la variable dependiente</li>
<li><span class="math inline">\(n\)</span> es el número de observaciones</li>
</ul>
<p>El descenso de gradiente utiliza la derivada de la función de costo con respecto a los parámetros <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> para ajustar los valores de los parámetros en la dirección que minimiza la función de costo. La regla de actualización de los parámetros es la siguiente:</p>
<p><span class="math display">\[\beta_0 = \beta_0 - \alpha \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y_i})\]</span></p>
<p><span class="math display">\[\beta_1 = \beta_1 - \alpha \frac{1}{n} \sum_{i=1}^{n} (Y_i -\hat{Y_i})X_i\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> es la tasa de aprendizaje</li>
<li><span class="math inline">\(\hat{Y_i}\)</span> es el valor predicho de la variable dependiente</li>
<li><span class="math inline">\(Y_i\)</span> es el valor observado de la variable dependiente</li>
<li><span class="math inline">\(X_i\)</span> es el valor de la variable independiente</li>
<li><span class="math inline">\(n\)</span> es el número de observaciones</li>
<li><span class="math inline">\(\alpha\)</span> es la tasa de aprendizaje que controla el tamaño de los pasos de actualización de los parámetros</li>
</ul>
<section id="derivación-de-las-ecuaciones-de-descenso-del-gradiente" class="level3">
<h3 class="anchored" data-anchor-id="derivación-de-las-ecuaciones-de-descenso-del-gradiente">Derivación de las Ecuaciones de Descenso del Gradiente*</h3>
<p>Aquí está la demostración de la fórmula de actualización de los parámetros en el descenso del gradiente para la regresión lineal simple.</p>
<p>La función de costo para la regresión lineal simple es el error cuadrático medio (MSE), que se define como:</p>
<p><span class="math display">\[MSE = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2\]</span></p>
<p>Reemplazando <span class="math inline">\(\hat{Y_i}\)</span> con la ecuación de regresión lineal simple, obtenemos:</p>
<p><span class="math display">\[MSE = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \beta_0 - \beta_1X_i)^2\]</span></p>
<p>Derivamos respecto a <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> para obtener las derivadas parciales de la función de costo:</p>
<p><span class="math display">\[\frac{\partial MSE}{\partial \beta_0} = \frac{-2}{n} \sum_{i=1}^{n} (Y_i - \beta_0 - \beta_1X_i)\]</span></p>
<p><span class="math display">\[\frac{\partial MSE}{\partial \beta_1} = \frac{-2}{n} \sum_{i=1}^{n} X_i (Y_i - \beta_0 - \beta_1X_i)\]</span></p>
<p>Usamos las derivadas parciales para actualizar los parámetros en la dirección que minimiza la función de costo:</p>
<p><span class="math display">\[\beta_0 = \beta_0 - \alpha \frac{\partial MSE}{\partial \beta_0}\]</span></p>
<p><span class="math display">\[\beta_1 = \beta_1 - \alpha \frac{\partial MSE}{\partial \beta_1}\]</span></p>
<p>Reemplazando las derivadas parciales, obtenemos la fórmula de actualización de los parámetros en el descenso del gradiente para la regresión lineal simple:</p>
<p><span class="math display">\[\beta_0 = \beta_0 + \alpha \frac{2}{n} \sum_{i=1}^{n} (Y_i - \hat{Y_i})\]</span></p>
<p><span class="math display">\[\beta_1 = \beta_1 + \alpha \frac{2}{n} \sum_{i=1}^{n} X_i (Y _i - \hat{Y_i})\]</span></p>
<p>El vector de coeficientes <span class="math inline">\(\beta\)</span> se puede actualizar iterativamente hasta que se alcance un criterio de convergencia, como un número máximo de iteraciones o una tolerancia en el cambio de los parámetros.</p>
</section>
<section id="implementación-del-descenso-del-gradiente" class="level3">
<h3 class="anchored" data-anchor-id="implementación-del-descenso-del-gradiente">Implementación del Descenso del Gradiente</h3>
<p>Vamos a implementar el descenso del gradiente para ajustar un modelo de regresión lineal simple.</p>
<div id="7878e174" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># Normalización de los datos</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>X <span class="op">=</span> (altura <span class="op">-</span> np.mean(altura)) <span class="op">/</span> np.std(altura)</span>
<span id="cb15-3"><a href="#cb15-3"></a>Y <span class="op">=</span> (peso <span class="op">-</span> np.mean(peso)) <span class="op">/</span> np.std(peso)</span>
<span id="cb15-4"><a href="#cb15-4"></a>n <span class="op">=</span> <span class="bu">len</span>(X)</span>
<span id="cb15-5"><a href="#cb15-5"></a></span>
<span id="cb15-6"><a href="#cb15-6"></a><span class="co"># Valores iniciales de los parámetros</span></span>
<span id="cb15-7"><a href="#cb15-7"></a>b0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-8"><a href="#cb15-8"></a>b1 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-9"><a href="#cb15-9"></a></span>
<span id="cb15-10"><a href="#cb15-10"></a><span class="co"># Hiperparámetros</span></span>
<span id="cb15-11"><a href="#cb15-11"></a>alpha <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb15-12"><a href="#cb15-12"></a>epochs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb15-13"><a href="#cb15-13"></a></span>
<span id="cb15-14"><a href="#cb15-14"></a><span class="co"># Descenso del gradiente</span></span>
<span id="cb15-15"><a href="#cb15-15"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb15-16"><a href="#cb15-16"></a>    y_pred <span class="op">=</span> b0 <span class="op">+</span> b1 <span class="op">*</span> X <span class="co"># Predicción</span></span>
<span id="cb15-17"><a href="#cb15-17"></a></span>
<span id="cb15-18"><a href="#cb15-18"></a>    db0 <span class="op">=</span> (<span class="op">-</span><span class="dv">2</span><span class="op">/</span>n) <span class="op">*</span> <span class="bu">sum</span>(Y <span class="op">-</span> y_pred) <span class="co"># Derivada parcial de b0</span></span>
<span id="cb15-19"><a href="#cb15-19"></a>    db1 <span class="op">=</span> (<span class="op">-</span><span class="dv">2</span><span class="op">/</span>n) <span class="op">*</span> <span class="bu">sum</span>(X <span class="op">*</span> (Y <span class="op">-</span> y_pred)) <span class="co"># Derivada parcial de b1</span></span>
<span id="cb15-20"><a href="#cb15-20"></a></span>
<span id="cb15-21"><a href="#cb15-21"></a>    b0 <span class="op">=</span> b0 <span class="op">-</span> alpha <span class="op">*</span> db0 <span class="co"># Actualizar b0</span></span>
<span id="cb15-22"><a href="#cb15-22"></a>    b1 <span class="op">=</span> b1 <span class="op">-</span> alpha <span class="op">*</span> db1 <span class="co"># Actualizar b1</span></span>
<span id="cb15-23"><a href="#cb15-23"></a></span>
<span id="cb15-24"><a href="#cb15-24"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">500</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># Imprimir resultados cada 500 iteraciones</span></span>
<span id="cb15-25"><a href="#cb15-25"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: b0 = </span><span class="sc">{</span>b0 <span class="op">*</span> np<span class="sc">.</span>std(peso) <span class="op">+</span> np<span class="sc">.</span>mean(peso) <span class="op">-</span> b1 <span class="op">*</span> np<span class="sc">.</span>mean(altura)<span class="sc">}</span><span class="ss">, b1 = </span><span class="sc">{</span>b1 <span class="op">*</span> (np.std(peso) <span class="op">/</span> np.std(altura))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-26"><a href="#cb15-26"></a></span>
<span id="cb15-27"><a href="#cb15-27"></a><span class="co"># Desnormalizar los parámetros</span></span>
<span id="cb15-28"><a href="#cb15-28"></a>b1 <span class="op">=</span> b1 <span class="op">*</span> (np.std(peso) <span class="op">/</span> np.std(altura))</span>
<span id="cb15-29"><a href="#cb15-29"></a>b0 <span class="op">=</span> b0 <span class="op">*</span> np.std(peso) <span class="op">+</span> np.mean(peso) <span class="op">-</span> b1 <span class="op">*</span> np.mean(altura)</span>
<span id="cb15-30"><a href="#cb15-30"></a></span>
<span id="cb15-31"><a href="#cb15-31"></a><span class="co"># Imprimir coeficientes</span></span>
<span id="cb15-32"><a href="#cb15-32"></a><span class="bu">print</span>(<span class="ss">f"Los valores óptimos son: b0 = </span><span class="sc">{</span>b0<span class="sc">}</span><span class="ss">, b1 = </span><span class="sc">{</span>b1<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0: b0 = 58.86970065046024, b1 = 0.00021469843354638102
Epoch 500: b0 = 36.69221818906457, b1 = 0.06797607549967058
Epoch 1000: b0 = 28.541743780104426, b1 = 0.09287914421609095
Epoch 1500: b0 = 25.546352702386315, b1 = 0.10203130224985668
Epoch 2000: b0 = 24.44551276862618, b1 = 0.10539482333349699
Epoch 2500: b0 = 24.040941703173104, b1 = 0.10663095518768421
Epoch 3000: b0 = 23.89225728322957, b1 = 0.1070852475565854
Epoch 3500: b0 = 23.83761408547845, b1 = 0.10725220511515218
Epoch 4000: b0 = 23.817532095303484, b1 = 0.10731356389700908
Epoch 4500: b0 = 23.81015173789428, b1 = 0.10733611393992575
Epoch 5000: b0 = 23.8074393734815, b1 = 0.10734440133449522
Epoch 5500: b0 = 23.80644254903254, b1 = 0.1073474470452729
Epoch 6000: b0 = 23.806076204828443, b1 = 0.10734856637826272
Epoch 6500: b0 = 23.80594156921029, b1 = 0.10734897774573438
Epoch 7000: b0 = 23.80589208910532, b1 = 0.10734912892791008
Epoch 7500: b0 = 23.805873904609804, b1 = 0.10734918448906133
Epoch 8000: b0 = 23.805867221603073, b1 = 0.10734920490840964
Epoch 8500: b0 = 23.805864765522912, b1 = 0.10734921241274988
Epoch 9000: b0 = 23.80586386288578, b1 = 0.10734921517067947
Epoch 9500: b0 = 23.805863531156447, b1 = 0.10734921618424971
Los valores óptimos son: b0 = 40.62639861081885, b1 = 0.1073492165563143</code></pre>
</div>
</div>
<p>La normalización de los datos es importante para que el descenso del gradiente converja más rápido. En este caso, normalizamos las variables independientes y dependientes restando la media y dividiendo por la desviación estándar. Después de ajustar el modelo, desnormalizamos los parámetros para obtener los valores en la escala original.</p>
<p>Otro factor importante es la tasa de aprendizaje <span class="math inline">\(\alpha\)</span>, que controla el tamaño de los pasos de actualización de los parámetros. Si la tasa de aprendizaje es demasiado pequeña, el algoritmo puede converger lentamente. Si es demasiado grande, el algoritmo puede divergir, encontrar la tasa de aprendizaje adecuada es crucial para el éxito del descenso del gradiente y muchos algoritmos adaptativos ajustan automáticamente la tasa de aprendizaje durante el entrenamiento para mejorar la convergencia.</p>
<p>Veremos de nuevo el método de descenso del gradiente cuando hablemos de redes neuronales.</p>
</section>
</section>
</section>
<section id="regresión-lineal-múltiple" class="level1">
<h1>Regresión Lineal Múltiple</h1>
<p>La regresión lineal múltiple es una extensión de la regresión lineal simple que permite modelar la relación entre una variable dependiente y dos o más variables independientes. La ecuación de regresión lineal múltiple se puede expresar de la siguiente manera:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(Y\)</span> es la variable dependiente</li>
<li><span class="math inline">\(X_1, X_2, ..., X_n\)</span> son las variables independientes</li>
<li><span class="math inline">\(\beta_0\)</span> es la intersección</li>
<li><span class="math inline">\(\epsilon\)</span> es el error</li>
</ul>
<p>Al igual que la regresión lineal simple, el objetivo de la regresión lineal múltiple es encontrar los valores de <span class="math inline">\(\beta_0, \beta_1, \beta_2, ..., \beta_n\)</span> que minimizan la suma de los cuadrados de los errores (SSE).</p>
<section id="regresión-lineal-múltiple-por-mínimos-cuadrados" class="level2">
<h2 class="anchored" data-anchor-id="regresión-lineal-múltiple-por-mínimos-cuadrados">Regresión Lineal Múltiple por Mínimos Cuadrados</h2>
<p>Debido a que ahora tratamos con múltiples variables independientes, la fórmula para calcular los valores de <span class="math inline">\(\beta_0, \beta_1, \beta_2, ..., \beta_n\)</span> esta en forma matricial:</p>
<p><span class="math display">\[\beta = (X^TX)^{-1}X^TY\]</span></p>
<p>Qué es idénctico a la fórmula de la regresión lineal simple, solo que ahora <span class="math inline">\(X\)</span> es una matriz que contiene las variables independientes y una columna de unos que representa la intersección.</p>
<p>Usemos datos de ejemplo para ajustar un modelo de regresión lineal múltiple y predecir la variable dependiente. Generaremos datos de 5 variables independientes y una variable dependiente.</p>
<div id="d62dabc8" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># Crear datos</span></span>
<span id="cb17-2"><a href="#cb17-2"></a>np.random.seed(<span class="dv">1014</span>)</span>
<span id="cb17-3"><a href="#cb17-3"></a></span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="co"># Variables independientes</span></span>
<span id="cb17-5"><a href="#cb17-5"></a>X1 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb17-6"><a href="#cb17-6"></a>X2 <span class="op">=</span> np.random.normal(<span class="dv">10</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb17-7"><a href="#cb17-7"></a>X3 <span class="op">=</span> np.random.normal(<span class="op">-</span><span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb17-8"><a href="#cb17-8"></a>X4 <span class="op">=</span> np.random.normal(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb17-9"><a href="#cb17-9"></a>X5 <span class="op">=</span> np.random.normal(<span class="dv">2</span>, <span class="fl">0.5</span>, <span class="dv">100</span>)</span>
<span id="cb17-10"><a href="#cb17-10"></a></span>
<span id="cb17-11"><a href="#cb17-11"></a><span class="co"># Efecto de cada variable independiente</span></span>
<span id="cb17-12"><a href="#cb17-12"></a>betas <span class="op">=</span> np.array([<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">3</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">4</span>])</span>
<span id="cb17-13"><a href="#cb17-13"></a></span>
<span id="cb17-14"><a href="#cb17-14"></a><span class="co"># Error</span></span>
<span id="cb17-15"><a href="#cb17-15"></a>epsilon <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">2.5</span>, <span class="dv">100</span>)</span>
<span id="cb17-16"><a href="#cb17-16"></a></span>
<span id="cb17-17"><a href="#cb17-17"></a><span class="co"># Variable dependiente</span></span>
<span id="cb17-18"><a href="#cb17-18"></a>Y <span class="op">=</span> betas[<span class="dv">0</span>] <span class="op">+</span> betas[<span class="dv">1</span>] <span class="op">*</span> X1 <span class="op">+</span> betas[<span class="dv">2</span>] <span class="op">*</span> X2 <span class="op">+</span> betas[<span class="dv">3</span>] <span class="op">*</span> X3 <span class="op">+</span> betas[<span class="dv">4</span>] <span class="op">*</span> X4 <span class="op">+</span> betas[<span class="dv">5</span>] <span class="op">*</span> X5 <span class="op">+</span> epsilon</span>
<span id="cb17-19"><a href="#cb17-19"></a></span>
<span id="cb17-20"><a href="#cb17-20"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'X1'</span>: X1, <span class="st">'X2'</span>: X2, <span class="st">'X3'</span>: X3, <span class="st">'X4'</span>: X4, <span class="st">'X5'</span>: X5, <span class="st">'Y'</span>: Y})</span>
<span id="cb17-21"><a href="#cb17-21"></a></span>
<span id="cb17-22"><a href="#cb17-22"></a>df.head()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="38">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">X1</th>
<th data-quarto-table-cell-role="th">X2</th>
<th data-quarto-table-cell-role="th">X3</th>
<th data-quarto-table-cell-role="th">X4</th>
<th data-quarto-table-cell-role="th">X5</th>
<th data-quarto-table-cell-role="th">Y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.759433</td>
<td>10.731036</td>
<td>-6.699854</td>
<td>2.615701</td>
<td>1.670699</td>
<td>61.504776</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>-1.007260</td>
<td>16.029547</td>
<td>-3.969604</td>
<td>1.957694</td>
<td>2.076248</td>
<td>55.376552</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>-0.644990</td>
<td>10.329352</td>
<td>-7.744539</td>
<td>2.040599</td>
<td>2.300200</td>
<td>52.803668</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>-0.266741</td>
<td>14.039842</td>
<td>-7.208872</td>
<td>3.020797</td>
<td>2.237179</td>
<td>62.968653</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>0.291256</td>
<td>13.169584</td>
<td>-5.827498</td>
<td>3.217659</td>
<td>1.341268</td>
<td>61.591722</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>Ahora ajustaremos un modelo de regresión lineal múltiple a los datos y obtendremos un resumen de los resultados.</p>
<div id="c2027081" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>X <span class="op">=</span> sm.add_constant(df[[<span class="st">'X1'</span>, <span class="st">'X2'</span>, <span class="st">'X3'</span>, <span class="st">'X4'</span>, <span class="st">'X5'</span>]])</span>
<span id="cb18-3"><a href="#cb18-3"></a>model <span class="op">=</span> sm.OLS(df[<span class="st">'Y'</span>], X).fit()</span>
<span id="cb18-4"><a href="#cb18-4"></a></span>
<span id="cb18-5"><a href="#cb18-5"></a><span class="co"># Imprimir resumen</span></span>
<span id="cb18-6"><a href="#cb18-6"></a><span class="bu">print</span>(model.summary())</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.976
Model:                            OLS   Adj. R-squared:                  0.975
Method:                 Least Squares   F-statistic:                     758.3
Date:                Sat, 22 Jun 2024   Prob (F-statistic):           2.63e-74
Time:                        13:41:20   Log-Likelihood:                -243.14
No. Observations:                 100   AIC:                             498.3
Df Residuals:                      94   BIC:                             513.9
Df Model:                           5                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          5.4953      1.737      3.165      0.002       2.047       8.943
X1             9.9913      0.310     32.255      0.000       9.376      10.606
X2             2.8923      0.058     49.999      0.000       2.777       3.007
X3            -1.9544      0.142    -13.789      0.000      -2.236      -1.673
X4            -0.7991      0.281     -2.843      0.005      -1.357      -0.241
X5             4.0200      0.613      6.553      0.000       2.802       5.238
==============================================================================
Omnibus:                        2.713   Durbin-Watson:                   2.227
Prob(Omnibus):                  0.258   Jarque-Bera (JB):                1.890
Skew:                           0.134   Prob(JB):                        0.389
Kurtosis:                       2.383   Cond. No.                         73.8
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
<p>Podemos imprimir los coeficientes del modelo para ver cómo se relacionan con los valores reales.</p>
<div id="0776767f" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># Imprimir coeficientes</span></span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="bu">print</span>(<span class="ss">f'Intercepto: </span><span class="sc">{</span>model<span class="sc">.</span>params[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="bu">print</span>(<span class="ss">f'Coeficientes: </span><span class="sc">{</span>model<span class="sc">.</span>params[<span class="dv">1</span>:]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Intercepto: 5.495327623636475
Coeficientes: X1    9.991312
X2    2.892250
X3   -1.954408
X4   -0.799148
X5    4.020035
dtype: float64</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_11165/3979803380.py:2: FutureWarning:

Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
</code></pre>
</div>
</div>
<p>El resumen del modelo nos proporciona información sobre la calidad del ajuste, los coeficientes de las variables independientes, los errores estándar de los coeficientes, los valores p, el coeficiente de determinación <span class="math inline">\(R^2\)</span>, entre otros.</p>
<p>Gráfiquemos los parámetros estimados y los valores reales.</p>
<div id="844f6551" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="co"># Graficar parámetros estimados y valores reales</span></span>
<span id="cb23-2"><a href="#cb23-2"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb23-3"><a href="#cb23-3"></a>plt.plot(betas, label<span class="op">=</span><span class="st">'Real'</span>, marker<span class="op">=</span><span class="st">'o'</span>, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb23-4"><a href="#cb23-4"></a>plt.plot(model.params, label<span class="op">=</span><span class="st">'Estimado'</span>, marker<span class="op">=</span><span class="st">'x'</span>, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb23-5"><a href="#cb23-5"></a>plt.xlabel(<span class="st">'Variables'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb23-6"><a href="#cb23-6"></a>plt.ylabel(<span class="st">'Coeficientes'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb23-7"><a href="#cb23-7"></a>plt.title(<span class="st">'Coeficientes Estimados vs. Coeficientes Reales'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb23-8"><a href="#cb23-8"></a>plt.legend()</span>
<span id="cb23-9"><a href="#cb23-9"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Linear_models_files/figure-html/cell-13-output-1.png" width="816" height="533" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>La paquetería <code>statsmodels</code> es muy completa y nos permite ajustar modelos de regresión lineal múltiple con facilidad. También podemos usar la función <code>ols</code> de <code>statsmodels.formula.api</code> para ajustar modelos de regresión lineal múltiple con fórmulas de estilo R.</p>
<div id="74774251" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb24-2"><a href="#cb24-2"></a></span>
<span id="cb24-3"><a href="#cb24-3"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb24-4"><a href="#cb24-4"></a>model <span class="op">=</span> smf.ols(<span class="st">'Y ~ X1 + X2 + X3 + X4 + X5'</span>, data<span class="op">=</span>df).fit()</span>
<span id="cb24-5"><a href="#cb24-5"></a></span>
<span id="cb24-6"><a href="#cb24-6"></a><span class="co"># Imprimir resumen</span></span>
<span id="cb24-7"><a href="#cb24-7"></a><span class="bu">print</span>(model.summary())</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.976
Model:                            OLS   Adj. R-squared:                  0.975
Method:                 Least Squares   F-statistic:                     758.3
Date:                Sat, 22 Jun 2024   Prob (F-statistic):           2.63e-74
Time:                        13:41:20   Log-Likelihood:                -243.14
No. Observations:                 100   AIC:                             498.3
Df Residuals:                      94   BIC:                             513.9
Df Model:                           5                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      5.4953      1.737      3.165      0.002       2.047       8.943
X1             9.9913      0.310     32.255      0.000       9.376      10.606
X2             2.8923      0.058     49.999      0.000       2.777       3.007
X3            -1.9544      0.142    -13.789      0.000      -2.236      -1.673
X4            -0.7991      0.281     -2.843      0.005      -1.357      -0.241
X5             4.0200      0.613      6.553      0.000       2.802       5.238
==============================================================================
Omnibus:                        2.713   Durbin-Watson:                   2.227
Prob(Omnibus):                  0.258   Jarque-Bera (JB):                1.890
Skew:                           0.134   Prob(JB):                        0.389
Kurtosis:                       2.383   Cond. No.                         73.8
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
<p>Dos resultados importantes del resumen es la prueba Durbin-Watson y la prueba de Jarque-Bera. La prueba Durbin-Watson se utiliza para detectar la presencia de autocorrelación en los residuos del modelo. Un valor de Durbin-Watson cercano a 2 indica que no hay autocorrelación. La prueba de Jarque-Bera se utiliza para detectar la normalidad de los residuos del modelo. Un valor de Jarque-Bera cercano a 0 indica que los residuos son normales.</p>
<p>Detectar la presencia de autocorrelación y no normalidad en los residuos es importante para evaluar la calidad del ajuste del modelo y tomar decisiones informadas sobre su uso. Si se obtienen resultados significativos en estas pruebas, es posible que sea necesario realizar ajustes adicionales al modelo para mejorar su rendimiento.</p>
</section>
</section>
<section id="regresión-lineal-con-scikit-learn" class="level1">
<h1>Regresión Lineal con Scikit-Learn</h1>
<p>La librería <code>scikit-learn</code> es una de las librerías más populares para el aprendizaje automático en Python. Proporciona una amplia gama de algoritmos de aprendizaje automático, incluidos los modelos de regresión lineal.</p>
<p>Vamos a ajustar un modelo de regresión lineal simple y un modelo de regresión lineal múltiple con <code>scikit-learn</code>.</p>
<div id="04280b41" class="cell" data-execution_count="15">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb26-2"><a href="#cb26-2"></a></span>
<span id="cb26-3"><a href="#cb26-3"></a><span class="co"># Regresión Lineal Simple</span></span>
<span id="cb26-4"><a href="#cb26-4"></a>X <span class="op">=</span> altura.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb26-5"><a href="#cb26-5"></a>Y <span class="op">=</span> peso</span>
<span id="cb26-6"><a href="#cb26-6"></a></span>
<span id="cb26-7"><a href="#cb26-7"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb26-8"><a href="#cb26-8"></a>model.fit(X, Y)</span>
<span id="cb26-9"><a href="#cb26-9"></a></span>
<span id="cb26-10"><a href="#cb26-10"></a><span class="bu">print</span>(<span class="ss">f'Intercepto: </span><span class="sc">{</span>model<span class="sc">.</span>intercept_<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb26-11"><a href="#cb26-11"></a><span class="bu">print</span>(<span class="ss">f'Coeficiente: </span><span class="sc">{</span>model<span class="sc">.</span>coef_[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Intercepto: 40.6263985738202
Coeficiente: 0.1073492167731905</code></pre>
</div>
</div>
<p>La función <code>reshape(-1, 1)</code> se utiliza para convertir el vector de altura en una matriz de una sola columna. Esto es necesario porque <code>scikit-learn</code> espera que las variables independientes sean matrices bidimensionales. Imprimamos esta matriz.</p>
<div id="ae68c637" class="cell" data-execution_count="16">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>X</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>array([[187.64052346],
       [174.00157208],
       [179.78737984],
       [192.40893199],
       [188.6755799 ],
       [160.2272212 ],
       [179.50088418],
       [168.48642792],
       [168.96781148],
       [174.10598502],
       [171.44043571],
       [184.54273507],
       [177.61037725],
       [171.21675016],
       [174.43863233],
       [173.33674327],
       [184.94079073],
       [167.94841736],
       [173.13067702],
       [161.45904261],
       [144.47010184],
       [176.53618595],
       [178.64436199],
       [162.5783498 ],
       [192.69754624],
       [155.45634325],
       [170.45758517],
       [168.1281615 ],
       [185.32779214],
       [184.6935877 ],
       [171.54947426],
       [173.7816252 ],
       [161.12214252],
       [150.19203532],
       [166.52087851],
       [171.56348969],
       [182.30290681],
       [182.02379849],
       [166.12673183],
       [166.97697249],
       [159.51447035],
       [155.79982063],
       [152.93729809],
       [189.50775395],
       [164.90347818],
       [165.61925698],
       [157.4720464 ],
       [177.77490356],
       [153.86102152],
       [167.8725972 ],
       [161.04533439],
       [173.86902498],
       [164.89194862],
       [158.19367816],
       [169.71817772],
       [174.28331871],
       [170.66517222],
       [173.02471898],
       [163.65677906],
       [166.37258834],
       [163.27539552],
       [166.40446838],
       [161.86853718],
       [152.73717398],
       [171.77426142],
       [165.98219064],
       [153.69801653],
       [174.62782256],
       [160.92701636],
       [170.51945396],
       [177.29090562],
       [171.28982911],
       [181.39400685],
       [157.6517418 ],
       [174.02341641],
       [163.15189909],
       [161.29202851],
       [164.21150335],
       [166.88447468],
       [170.56165342],
       [158.34850159],
       [179.00826487],
       [174.6566244 ],
       [154.63756314],
       [184.88252194],
       [188.95889176],
       [181.78779571],
       [168.20075164],
       [159.29247378],
       [180.54451727],
       [165.96823053],
       [182.2244507 ],
       [172.08274978],
       [179.76639036],
       [173.56366397],
       [177.06573168],
       [170.10500021],
       [187.85870494],
       [171.26912093],
       [174.01989363]])</code></pre>
</div>
</div>
<p>Podemos utilizar el método <code>predict</code> para hacer predicciones con el modelo.</p>
<div id="32b765cc" class="cell" data-execution_count="17">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># Predicción</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>altura_nueva <span class="op">=</span> np.array([[<span class="dv">160</span>], [<span class="dv">170</span>], [<span class="dv">180</span>]])</span>
<span id="cb30-3"><a href="#cb30-3"></a>peso_pred <span class="op">=</span> model.predict(altura_nueva)</span>
<span id="cb30-4"><a href="#cb30-4"></a></span>
<span id="cb30-5"><a href="#cb30-5"></a><span class="bu">print</span>(peso_pred)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[57.80227326 58.87576543 59.94925759]</code></pre>
</div>
</div>
<p>Podemos usar otras funciones de <code>scikit-learn</code> para realizar una regresion Rodge o Lasso.</p>
<div id="573eb11f" class="cell" data-execution_count="18">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge, Lasso</span>
<span id="cb32-2"><a href="#cb32-2"></a></span>
<span id="cb32-3"><a href="#cb32-3"></a><span class="co"># Regresión Ridge</span></span>
<span id="cb32-4"><a href="#cb32-4"></a>model <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb32-5"><a href="#cb32-5"></a>model.fit(X, Y)</span>
<span id="cb32-6"><a href="#cb32-6"></a></span>
<span id="cb32-7"><a href="#cb32-7"></a><span class="bu">print</span>(<span class="ss">f'Intercepto: </span><span class="sc">{</span>model<span class="sc">.</span>intercept_<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb32-8"><a href="#cb32-8"></a><span class="bu">print</span>(<span class="ss">f'Coeficiente: </span><span class="sc">{</span>model<span class="sc">.</span>coef_[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb32-9"><a href="#cb32-9"></a></span>
<span id="cb32-10"><a href="#cb32-10"></a><span class="co"># Regresión Lasso</span></span>
<span id="cb32-11"><a href="#cb32-11"></a>model <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb32-12"><a href="#cb32-12"></a>model.fit(X, Y)</span>
<span id="cb32-13"><a href="#cb32-13"></a></span>
<span id="cb32-14"><a href="#cb32-14"></a><span class="bu">print</span>(<span class="ss">f'Intercepto: </span><span class="sc">{</span>model<span class="sc">.</span>intercept_<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb32-15"><a href="#cb32-15"></a><span class="bu">print</span>(<span class="ss">f'Coeficiente: </span><span class="sc">{</span>model<span class="sc">.</span>coef_[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Intercepto: 40.62820122077731
Coeficiente: 0.10733865014222098
Intercepto: 42.30580012649057
Coeficiente: 0.09750501717175815</code></pre>
</div>
</div>
</section>
<section id="pruebas-de-normalidad" class="level1">
<h1>Pruebas de Normalidad</h1>
<p>Las pruebas de normalidad son pruebas estadísticas que se utilizan para determinar si una muestra de datos proviene de una distribución normal. Existen varias pruebas de normalidad, como la prueba de Shapiro-Wilk, la prueba de Kolmogorov-Smirnov y la prueba de Anderson-Darling. O métodos gráficos como los histogramas y el gráfico Q-Q.</p>
<section id="gráfico-q-q" class="level2">
<h2 class="anchored" data-anchor-id="gráfico-q-q">Gráfico Q-Q</h2>
<p>El gráfico Q-Q (quantile-quantile) es una forma visual de evaluar si una muestra de datos proviene de una distribución normal. En un gráfico Q-Q, los cuantiles de la muestra se comparan con los cuantiles de una distribución normal teórica. Si los puntos en el gráfico Q-Q siguen una línea recta, entonces los datos se ajustan a una distribución normal. Evidentemente, si los puntos se alejan de la línea recta, entonces los datos no se ajustan a una distribución normal.</p>
<p>Vamos a generar datos de una distribución normal y una distribución no normal para comparar los gráficos Q-Q.</p>
<div id="25cde0c3" class="cell" data-execution_count="19">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="co"># Datos de una distribución normal</span></span>
<span id="cb34-2"><a href="#cb34-2"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb34-3"><a href="#cb34-3"></a>normal_data <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb34-4"><a href="#cb34-4"></a></span>
<span id="cb34-5"><a href="#cb34-5"></a><span class="co"># Datos de una distribución no normal</span></span>
<span id="cb34-6"><a href="#cb34-6"></a>non_normal_data <span class="op">=</span> np.random.exponential(<span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb34-7"><a href="#cb34-7"></a></span>
<span id="cb34-8"><a href="#cb34-8"></a><span class="co"># Gráfico Q-Q</span></span>
<span id="cb34-9"><a href="#cb34-9"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb34-10"><a href="#cb34-10"></a></span>
<span id="cb34-11"><a href="#cb34-11"></a><span class="co"># Distribución normal</span></span>
<span id="cb34-12"><a href="#cb34-12"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Distribución Normal'</span>)</span>
<span id="cb34-13"><a href="#cb34-13"></a>sm.qqplot(normal_data, line<span class="op">=</span><span class="st">'s'</span>, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb34-14"><a href="#cb34-14"></a>ax[<span class="dv">0</span>].set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb34-15"><a href="#cb34-15"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">'Cuantiles Teóricos'</span>)</span>
<span id="cb34-16"><a href="#cb34-16"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">'Cuantiles de los Datos'</span>)</span>
<span id="cb34-17"><a href="#cb34-17"></a></span>
<span id="cb34-18"><a href="#cb34-18"></a><span class="co"># Distribución no normal</span></span>
<span id="cb34-19"><a href="#cb34-19"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Distribución No Normal'</span>)</span>
<span id="cb34-20"><a href="#cb34-20"></a>sm.qqplot(non_normal_data, line<span class="op">=</span><span class="st">'s'</span>, ax<span class="op">=</span>ax[<span class="dv">1</span>])</span>
<span id="cb34-21"><a href="#cb34-21"></a>ax[<span class="dv">1</span>].set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb34-22"><a href="#cb34-22"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'Cuantiles Teóricos'</span>)</span>
<span id="cb34-23"><a href="#cb34-23"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">'Cuantiles de los Datos'</span>)</span>
<span id="cb34-24"><a href="#cb34-24"></a></span>
<span id="cb34-25"><a href="#cb34-25"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Linear_models_files/figure-html/cell-19-output-1.png" width="792" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>En el gráfico Q-Q de la distribución normal, los puntos siguen una línea recta, lo que indica que los datos se ajustan a una distribución normal. En el gráfico Q-Q de la distribución no normal, los puntos se alejan de la línea recta, lo que indica que los datos no se ajustan a una distribución normal.</p>
</section>
<section id="prueba-de-shapiro-wilk" class="level2">
<h2 class="anchored" data-anchor-id="prueba-de-shapiro-wilk">Prueba de Shapiro-Wilk</h2>
<p>La prueba de Shapiro-Wilk es una prueba estadística que se utiliza para determinar si una muestra de datos proviene de una distribución normal.</p>
<p>Hipótesis:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: Los datos <strong>provienen</strong> de una distribución normal</li>
<li><span class="math inline">\(H_1\)</span>: Los datos <em>no provienen</em> de una distribución normal</li>
</ul>
<p>Vamos a aplicar la prueba de Shapiro-Wilk a los datos de las distribuciones normal y no normal.</p>
<div id="19fdec52" class="cell" data-execution_count="20">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> shapiro</span>
<span id="cb35-2"><a href="#cb35-2"></a></span>
<span id="cb35-3"><a href="#cb35-3"></a><span class="co"># Prueba de Shapiro-Wilk</span></span>
<span id="cb35-4"><a href="#cb35-4"></a>stat, p <span class="op">=</span> shapiro(normal_data)</span>
<span id="cb35-5"><a href="#cb35-5"></a><span class="bu">print</span>(<span class="ss">f'Distribución Normal: Estadístico = </span><span class="sc">{</span>stat<span class="sc">}</span><span class="ss">, p-valor = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb35-6"><a href="#cb35-6"></a></span>
<span id="cb35-7"><a href="#cb35-7"></a>stat, p <span class="op">=</span> shapiro(non_normal_data)</span>
<span id="cb35-8"><a href="#cb35-8"></a><span class="bu">print</span>(<span class="ss">f'Distribución No Normal: Estadístico = </span><span class="sc">{</span>stat<span class="sc">}</span><span class="ss">, p-valor = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Distribución Normal: Estadístico = 0.9985554728235057, p-valor = 0.5912267898687746
Distribución No Normal: Estadístico = 0.8336406754912049, p-valor = 5.034540538267324e-31</code></pre>
</div>
</div>
<p>El valor p de los datos normales fue de <span class="math inline">\(0.5912267898687746\)</span> y el valor p de los datos no normales fue de <span class="math inline">\(5.034540538267324e-31\)</span>. Por lo que podemos rechazar la hipótesis nula en el segundo caso.</p>
</section>
</section>
<section id="ejercicios" class="level1">
<h1>Ejercicios</h1>
<ol type="1">
<li>Carga los datos desde https://github.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/blob/main/data/cars.csv.</li>
</ol>
<p><em>Columnas:</em></p>
<ul>
<li><strong>Car_ID</strong>: A unique identifier for each car listing.</li>
<li><strong>Brand</strong>: The brand or manufacturer of the car (e.g., Toyota, Honda, Ford, etc.).</li>
<li><strong>Model</strong>: The model of the car (e.g., Camry, Civic, Mustang, etc.).</li>
<li><strong>Year</strong>: The manufacturing year of the car.</li>
<li><strong>Kilometers_Driven</strong>: The total kilometers driven by the car.</li>
<li><strong>Fuel_Type</strong>: The type of fuel used by the car (e.g., Petrol, Diesel, Electric, etc.).</li>
<li><strong>Transmission</strong>: The transmission type of the car (e.g., Manual, Automatic).</li>
<li><strong>Owner_Type</strong>: The number of previous owners of the car (e.g., First, Second, Third).</li>
<li><strong>Mileage</strong>: The fuel efficiency of the car in kilometers per liter.</li>
<li><strong>Engine</strong>: The engine capacity of the car in CC (Cubic Centimeters).</li>
<li><strong>Power</strong>: The maximum power output of the car in bhp (Brake Horsepower).</li>
<li><strong>Seats</strong>: The number of seats available in the car.</li>
<li><strong>Price</strong>: The selling price of the car in INR (Indian Rupees), which is the target variable to predict.</li>
</ul>
<ol start="2" type="1">
<li><p>Revisa si existen valores nulos en el conjunto de datos y elimina las filas que los contengan.</p></li>
<li><p>Usando la función de pandas <a href="https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html"><code>get_dummies</code></a> convierte las variables categóricas en variables dummy. O en su defecto, usa la función <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"><code>LabelEncoder</code></a> de <code>sklearn</code>.</p></li>
<li><p>Realiza un análisis exploratorio de los datos y describe las características de las variables.</p></li>
<li><p>Visualiza los datos utilizando gráficos de dispersión, histogramas y diagramas de caja para identificar posibles relaciones entre las variables.</p></li>
<li><p>Realiza un análisis de correlación entre las variables y visualiza los resultados.</p></li>
<li><p>Comprueba la normalidad de la variable dependiente (precio de los autos) y de las variables independientes (kilómetros recorridos, eficiencia de combustible, capacidad del motor, potencia máxima, número de asientos) utilizando gráficos Q-Q y la prueba de Shapiro-Wilk. ¿Qué puedes concluir?</p></li>
<li><p>Ajusta un modelo de regresión lineal múltiple para predecir el precio de los autos en función de las variables independientes (usando <code>statsmodels</code> y <code>scikit-learn</code>). ¿Cuál libreria prefieres y por qué? ¿Qué variables tienen un mayor impacto en el precio de los autos? ¿Qué otras variables podrían ser importantes para predecir el precio de los autos?</p></li>
<li><p>Evalúa la calidad del ajuste del modelo y realiza predicciones para un conjunto de datos de prueba de tu elección.</p></li>
</ol>
</section>
<section id="modelos-lineales-generalizados." class="level1">
<h1>Modelos Lineales Generalizados.</h1>
<p>Recordemos que el modelo de regresión lineal se puede representar de la siguiente forma:</p>
<p><span class="math display">\[ y \sim N(\mu, \sigma) \]</span></p>
<p><span class="math display">\[ \mu = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p \]</span></p>
<p>Donde <span class="math inline">\(y\)</span> es la variable dependiente, <span class="math inline">\(\mu\)</span> es la media de la distribución normal, <span class="math inline">\(\beta_0\)</span> es el intercepto, <span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_p\)</span> son los coeficientes de las variables independientes <span class="math inline">\(x_1, x_2, \ldots, x_p\)</span> respectivamente y <span class="math inline">\(\sigma\)</span> es la desviación estándar de la distribución normal.</p>
<p>En el caso de los modelos lineales generalizados, la distribución de la variable dependiente <span class="math inline">\(y\)</span> no necesariamente tiene que ser normal. En general, la distribución de <span class="math inline">\(y\)</span> puede ser cualquier distribución de la familia exponencial. La forma general de un modelo lineal generalizado es la siguiente:</p>
<p><span class="math display">\[ y \sim f(g(\mu), \phi) \]</span></p>
<p><span class="math display">\[ g(\mu) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p \]</span></p>
<p>Donde <span class="math inline">\(y\)</span> es la variable dependiente, <span class="math inline">\(f\)</span> es la función de densidad de probabilidad de la distribución de <span class="math inline">\(y\)</span>, <span class="math inline">\(g\)</span> es la función de enlace, <span class="math inline">\(\mu\)</span> es el valor esperado de la distribución de <span class="math inline">\(y\)</span>, <span class="math inline">\(\beta_0\)</span> es el intercepto, <span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_p\)</span> son los coeficientes de las variables independientes <span class="math inline">\(x_1, x_2, \ldots, x_p\)</span> respectivamente y <span class="math inline">\(\phi\)</span> es un parámetro de dispersión.</p>
<p>Dentro de las distribuciones de la familia exponencial nos encontramos con:</p>
<ul>
<li>Distribución Normal</li>
<li>Distribución Binomial</li>
<li>Distribución Poisson</li>
<li>Distribución Gamma</li>
</ul>
<p>En el caso de la distribución normal, la función de enlace es la identidad, es decir, <span class="math inline">\(g(\mu) = \mu\)</span>. En el caso de la distribución binomial, la función de enlace es la función logit, es decir, <span class="math inline">\(g(\mu) = \log\left(\frac{\mu}{1-\mu}\right)\)</span>. En el caso de la distribución Poisson, la función de enlace es la función log, es decir, <span class="math inline">\(g(\mu) = \log(\mu)\)</span>. En el caso de la distribución Gamma, la función de enlace es la inversa, es decir, <span class="math inline">\(g(\mu) = \frac{1}{\mu}\)</span>.</p>
<p>Las funciones de enlace tienen como objetivo enlazar la linealidad de los coeficientes <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span> y las variables independientes con el valor esperado de la distribución de <span class="math inline">\(y\)</span> <span class="math inline">\(\mu\)</span> de forma no lineal.</p>
<section id="regresión-logística" class="level2">
<h2 class="anchored" data-anchor-id="regresión-logística">Regresión Logística</h2>
<p>Podemos derivar el modelo de regresión logística a partir del modelo lineal que ya vimos. En el caso de la regresión logística, la variable dependiente <span class="math inline">\(y\)</span> es binaria, es decir, <span class="math inline">\(y \in \{0, 1\}\)</span>. Por tanto podemos pensar en modelar la probabilidad de que <span class="math inline">\(y = 1\)</span> en función de las variables independientes <span class="math inline">\(x_1, x_2, \ldots, x_p\)</span>. El complemento de la probabilidad de que <span class="math inline">\(y = 1\)</span> es la probabilidad de que <span class="math inline">\(y = 0\)</span>.</p>
<p>Ahora calculemos los “odds” de que <span class="math inline">\(y = 1\)</span>:</p>
<p><span class="math display">\[ \text{odds} = \frac{P(y = 1)}{P(y = 0)} \]</span></p>
<p>Los “odds” son la razón de la probabilidad de que <span class="math inline">\(y = 1\)</span> entre la probabilidad de que <span class="math inline">\(y = 0\)</span>. Si los “odds” son mayores a 1, entonces la probabilidad de que <span class="math inline">\(y = 1\)</span> es mayor que la probabilidad de que $y = 0. Si los “odds” son menores a 1, entonces la probabilidad de que <span class="math inline">\(y = 0\)</span> es mayor que la probabilidad de que <span class="math inline">\(y = 1\)</span>.</p>
<p>Ahora vamos a suponer que los “odds” de que <span class="math inline">\(y = 1\)</span> son una función lineal de las variables independientes <span class="math inline">\(x_1, x_2, \ldots, x_p\)</span>:</p>
<p><span class="math display">\[\text{odds} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p\]</span></p>
<p>Un problema que tienen los “odds” es que están desbalanceados. Por ejemplo, si los “odds” son 10, entonces la probabilidad de que <span class="math inline">\(y = 1\)</span> es 10 veces mayor que la probabilidad de que <span class="math inline">\(y = 0\)</span>. Pero si los “odds” son 0.1, entonces la probabilidad de que <span class="math inline">\(y = 1\)</span> es 10 veces menor que la probabilidad de que <span class="math inline">\(y = 0\)</span>. Para resolver este problema, vamos a tomar el logaritmo de los “odds” y vamos a modelar el logaritmo de los “odds” en función de las variables independientes <span class="math inline">\(x_1, x_2, \ldots, x_p\)</span>:</p>
<p><span class="math display">\[\log\left(\frac{P(y = 1)}{P(y = 0)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p\]</span></p>
<p><span class="math display">\[\log\left(\frac{P(y = 1)}{1 - P(y = 1)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p\]</span></p>
<p>Ahora solo tenemos que despejar <span class="math inline">\(P(y = 1)\)</span> paso a paso:</p>
<p><span class="math display">\[\log\left(\frac{P(y = 1)}{1 - P(y = 1)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p\]</span></p>
<p><span class="math display">\[\frac{P(y = 1)}{1 - P(y = 1)} = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p}\]</span></p>
<p><span class="math display">\[P(y = 1) = (1 - P(y = 1))e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p}\]</span></p>
<p><span class="math display">\[P(y = 1) = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p} - P(y = 1)e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p}\]</span></p>
<p><span class="math display">\[P(y = 1) + P(y = 1)e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p} = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p}\]</span></p>
<p><span class="math display">\[P(y = 1)(1 + e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p}) = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p}\]</span></p>
<p><span class="math display">\[P(y = 1) = \frac{e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p}}\]</span></p>
<p><span class="math display">\[P(y = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p)}}\]</span></p>
<p>Esta última ecuación es la función logística. La función logística es una función sigmoide que toma valores entre 0 y 1. Gráficamente, la función logística se ve de la siguiente forma:</p>
<div id="f053ebf9" class="cell" data-execution_count="21">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-2"><a href="#cb37-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb37-3"><a href="#cb37-3"></a></span>
<span id="cb37-4"><a href="#cb37-4"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb37-5"><a href="#cb37-5"></a>y <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb37-6"><a href="#cb37-6"></a></span>
<span id="cb37-7"><a href="#cb37-7"></a>plt.plot(x, y)</span>
<span id="cb37-8"><a href="#cb37-8"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb37-9"><a href="#cb37-9"></a>plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb37-10"><a href="#cb37-10"></a>plt.title(<span class="st">'Función Logística'</span>)</span>
<span id="cb37-11"><a href="#cb37-11"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Linear_models_files/figure-html/cell-21-output-1.png" width="589" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Ahora veamos como se ve la función logística dandole valor a dos coeficientes <span class="math inline">\(\beta_0 = 0\)</span> y <span class="math inline">\(\beta_1 = 1\)</span>:</p>
<div id="25a8d52b" class="cell" data-execution_count="22">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-2"><a href="#cb38-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb38-3"><a href="#cb38-3"></a></span>
<span id="cb38-4"><a href="#cb38-4"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb38-5"><a href="#cb38-5"></a>b0 <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb38-6"><a href="#cb38-6"></a>b1 <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb38-7"><a href="#cb38-7"></a></span>
<span id="cb38-8"><a href="#cb38-8"></a>y <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(b0 <span class="op">+</span> b1 <span class="op">*</span> x)))</span>
<span id="cb38-9"><a href="#cb38-9"></a></span>
<span id="cb38-10"><a href="#cb38-10"></a>plt.plot(x, y, label<span class="op">=</span><span class="vs">r'$y = \frac</span><span class="sc">{1}</span><span class="vs">{1 + e^{-(\beta_0 + \beta_1 x)</span><span class="sc">}}</span><span class="vs">$'</span>)</span>
<span id="cb38-11"><a href="#cb38-11"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb38-12"><a href="#cb38-12"></a>plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb38-13"><a href="#cb38-13"></a>plt.title(<span class="st">'Función Logística'</span>)</span>
<span id="cb38-14"><a href="#cb38-14"></a>plt.legend()</span>
<span id="cb38-15"><a href="#cb38-15"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Linear_models_files/figure-html/cell-22-output-1.png" width="589" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Algunas propiedades de la función logística son:</p>
<ul>
<li>La función logística es simétrica respecto al punto <span class="math inline">\((0, 0.5)\)</span>.</li>
<li>La función logística es monótona creciente.</li>
<li>La función logística es acotada entre 0 y 1.</li>
</ul>
<section id="estimar-los-coeficientes-de-la-regresión-logística" class="level3">
<h3 class="anchored" data-anchor-id="estimar-los-coeficientes-de-la-regresión-logística">Estimar los coeficientes de la regresión logística</h3>
<p>Para estimar los coeficientes de la regresión logística, podemos utilizar el método de máxima verosimilitud. La función de verosimilitud es la probabilidad de observar los datos dados los coeficientes del modelo. La función de verosimilitud de la regresión logística es la siguiente:</p>
<p><span class="math display">\[L(\beta_0, \beta_1, \ldots, \beta_p) = \prod_{i=1}^{n} P(y_i = 1)^{y_i} (1 - P(y_i = 1))^{1 - y_i}\]</span></p>
<p>Donde <span class="math inline">\(n\)</span> es el número de observaciones, <span class="math inline">\(y_i\)</span> es la variable dependiente de la observación <span class="math inline">\(i\)</span> y <span class="math inline">\(P(y_i = 1)\)</span> es la probabilidad de que la observación <span class="math inline">\(i\)</span> tome el valor 1. La función de verosimilitud es el producto de las probabilidades de que cada observación tome el valor 1 o 0.</p>
<p>Dado que los datos son independientes e idénticamente distribuidos, podemos tomar el logaritmo de la función de verosimilitud para simplificar los cálculos:</p>
<p><span class="math display">\[\log L(\beta_0, \beta_1, \ldots, \beta_p) = \sum_{i=1}^{n} y_i \log P(y_i = 1) + (1 - y_i) \log (1 - P(y_i = 1))\]</span></p>
<p>Ahora solo tenemos que maximizar la función de verosimilitud logarítmica para encontrar los coeficientes del modelo. Para maximizar la función de verosimilitud logarítmica, podemos utilizar el método de Newton-Raphson o con descenso de gradiente. En la práctica, la mayoría de los paquetes de software utilizan el método de Newton-Raphson.</p>
<p>Dado que la explicación de la estimación de los coeficientes de la regresión logística es un poco extensa, vamos a ver un ejemplo en Python utilizando el paquete <code>statsmodels</code>.</p>
<div id="215773dc" class="cell" data-execution_count="23">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-3"><a href="#cb39-3"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb39-4"><a href="#cb39-4"></a></span>
<span id="cb39-5"><a href="#cb39-5"></a><span class="co"># Generar datos</span></span>
<span id="cb39-6"><a href="#cb39-6"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb39-7"><a href="#cb39-7"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb39-8"><a href="#cb39-8"></a>x1 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb39-9"><a href="#cb39-9"></a>x2 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb39-10"><a href="#cb39-10"></a></span>
<span id="cb39-11"><a href="#cb39-11"></a><span class="co"># Definir coeficientes</span></span>
<span id="cb39-12"><a href="#cb39-12"></a>b0 <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb39-13"><a href="#cb39-13"></a>b1 <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb39-14"><a href="#cb39-14"></a>b2 <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb39-15"><a href="#cb39-15"></a></span>
<span id="cb39-16"><a href="#cb39-16"></a><span class="co"># Generar variable dependiente</span></span>
<span id="cb39-17"><a href="#cb39-17"></a>p <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(b0 <span class="op">+</span> b1 <span class="op">*</span> x1 <span class="op">+</span> b2 <span class="op">*</span> x2)))</span>
<span id="cb39-18"><a href="#cb39-18"></a></span>
<span id="cb39-19"><a href="#cb39-19"></a>y <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, p)</span>
<span id="cb39-20"><a href="#cb39-20"></a></span>
<span id="cb39-21"><a href="#cb39-21"></a><span class="co"># Crear DataFrame</span></span>
<span id="cb39-22"><a href="#cb39-22"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'y'</span>: y, <span class="st">'x1'</span>: x1, <span class="st">'x2'</span>: x2})</span>
<span id="cb39-23"><a href="#cb39-23"></a></span>
<span id="cb39-24"><a href="#cb39-24"></a><span class="co"># Muestra de los datos</span></span>
<span id="cb39-25"><a href="#cb39-25"></a>df.head(<span class="dv">10</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="51">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">y</th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1</td>
<td>1.764052</td>
<td>0.555963</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1</td>
<td>0.400157</td>
<td>0.892474</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1</td>
<td>0.978738</td>
<td>-0.422315</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1</td>
<td>2.240893</td>
<td>0.104714</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1</td>
<td>1.867558</td>
<td>0.228053</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>1</td>
<td>-0.977278</td>
<td>0.201480</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>1</td>
<td>0.950088</td>
<td>0.540774</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>1</td>
<td>-0.151357</td>
<td>-1.818078</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>1</td>
<td>-0.103219</td>
<td>-0.049324</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>1</td>
<td>0.410599</td>
<td>0.239034</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>Ahora estimemos los coeficientes del modelo de regresión logística:</p>
<div id="aca36be3" class="cell" data-execution_count="24">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a><span class="co"># Estimar modelo</span></span>
<span id="cb40-2"><a href="#cb40-2"></a>data <span class="op">=</span> sm.add_constant(df[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]])</span>
<span id="cb40-3"><a href="#cb40-3"></a>model <span class="op">=</span> sm.GLM(df[<span class="st">'y'</span>], data, family<span class="op">=</span>sm.families.Binomial())</span>
<span id="cb40-4"><a href="#cb40-4"></a>result <span class="op">=</span> model.fit()</span>
<span id="cb40-5"><a href="#cb40-5"></a></span>
<span id="cb40-6"><a href="#cb40-6"></a><span class="co"># Mostrar resultados</span></span>
<span id="cb40-7"><a href="#cb40-7"></a><span class="bu">print</span>(result.summary())</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                      y   No. Observations:                 1000
Model:                            GLM   Df Residuals:                      997
Model Family:                Binomial   Df Model:                            2
Link Function:                  Logit   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -143.21
Date:                Sat, 22 Jun 2024   Deviance:                       286.43
Time:                        13:41:21   Pearson chi2:                 1.41e+03
No. Iterations:                     8   Pseudo R-squ. (CS):             0.2988
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const          5.4077      0.442     12.228      0.000       4.541       6.274
x1             3.2868      0.317     10.383      0.000       2.666       3.907
x2             2.0070      0.248      8.093      0.000       1.521       2.493
==============================================================================</code></pre>
</div>
</div>
<p>Como antes tenemos que añaadir una columna de unos al DataFrame para poder estimar el intercepto del modelo. Después utilizamos la función <code>GLM</code> de <code>statsmodels</code> que estima distribuciones de la familia exponencial. En este caso, estamos utilizando la distribución binomial, con el parámetro <code>family=sm.families.Binomial()</code>. Finalmente, utilizamos el método <code>fit</code> para estimar los coeficientes del modelo.</p>
<p>Podemos estimar la regresión logística con <code>scikit-learn</code>:</p>
<div id="0a90542c" class="cell" data-execution_count="25">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb42-2"><a href="#cb42-2"></a></span>
<span id="cb42-3"><a href="#cb42-3"></a><span class="co"># Estimar modelo</span></span>
<span id="cb42-4"><a href="#cb42-4"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb42-5"><a href="#cb42-5"></a>model.fit(df[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]], df[<span class="st">'y'</span>])</span>
<span id="cb42-6"><a href="#cb42-6"></a></span>
<span id="cb42-7"><a href="#cb42-7"></a><span class="co"># Imprimir coeficientes</span></span>
<span id="cb42-8"><a href="#cb42-8"></a><span class="bu">print</span>(<span class="ss">f'Intercepto: </span><span class="sc">{</span>model<span class="sc">.</span>intercept_<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb42-9"><a href="#cb42-9"></a><span class="bu">print</span>(<span class="ss">f'Coeficientes: </span><span class="sc">{</span>model<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Intercepto: [4.94144067]
Coeficientes: [[2.94011   1.7773892]]</code></pre>
</div>
</div>
<p>Podemos probar que tan bueno es el modelo al verificar que predice con los datos que ya tenemos y compararlo con los datos reales:</p>
<div id="2a965aaa" class="cell" data-execution_count="26">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a><span class="co"># Predicción</span></span>
<span id="cb44-2"><a href="#cb44-2"></a>y_pred <span class="op">=</span> model.predict(df[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]])</span>
<span id="cb44-3"><a href="#cb44-3"></a></span>
<span id="cb44-4"><a href="#cb44-4"></a><span class="co"># Comparar predicciones con datos reales</span></span>
<span id="cb44-5"><a href="#cb44-5"></a>df[<span class="st">'y_pred'</span>] <span class="op">=</span> y_pred</span>
<span id="cb44-6"><a href="#cb44-6"></a>df.head(<span class="dv">10</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="54">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">y</th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
<th data-quarto-table-cell-role="th">y_pred</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1</td>
<td>1.764052</td>
<td>0.555963</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1</td>
<td>0.400157</td>
<td>0.892474</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1</td>
<td>0.978738</td>
<td>-0.422315</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1</td>
<td>2.240893</td>
<td>0.104714</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1</td>
<td>1.867558</td>
<td>0.228053</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>1</td>
<td>-0.977278</td>
<td>0.201480</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>1</td>
<td>0.950088</td>
<td>0.540774</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>1</td>
<td>-0.151357</td>
<td>-1.818078</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>1</td>
<td>-0.103219</td>
<td>-0.049324</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>1</td>
<td>0.410599</td>
<td>0.239034</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>Para comparar modelos de regresión logística, podemos utilizar métricas como la precisión, la sensibilidad, la especificidad, el valor predictivo positivo y el valor predictivo negativo. Estas métricas nos permiten evaluar la capacidad del modelo para predecir correctamente los valores positivos y negativos y se basan en la <strong>matriz de confusión</strong>.</p>
<div id="503b7d92" class="cell" data-execution_count="27">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb45-2"><a href="#cb45-2"></a></span>
<span id="cb45-3"><a href="#cb45-3"></a><span class="co"># Matriz de confusión</span></span>
<span id="cb45-4"><a href="#cb45-4"></a>cm <span class="op">=</span> confusion_matrix(df[<span class="st">'y'</span>], df[<span class="st">'y_pred'</span>])</span>
<span id="cb45-5"><a href="#cb45-5"></a><span class="bu">print</span>(cm)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[[ 52  46]
 [ 11 891]]</code></pre>
</div>
</div>
<p>La matriz de confusión es una tabla que muestra el número de verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos del modelo. A partir de la matriz de confusión, podemos calcular métricas como la precisión, la sensibilidad, la especificidad, el valor predictivo positivo y el valor predictivo negativo.</p>
<p>La diagonal de la matriz de confusión contiene los valores correctos, es decir, los verdaderos positivos y los verdaderos negativos. Los valores fuera de la diagonal contienen los valores incorrectos, es decir, los falsos positivos y los falsos negativos.</p>
<span class="math display">\[\begin{array}{|c|c|}
\hline
\text{Verdaderos Positivos (TP)} &amp; \text{Falsos Positivos (FP)} \\
\hline
\text{Falsos Negativos (FN)} &amp; \text{Verdaderos Negativos (TN)} \\
\hline
\end{array}\]</span>
<p>La <strong>precisión</strong> es la proporción de predicciones correctas entre todas las predicciones realizadas por el modelo:</p>
<p><span class="math display">\[\text{Precisión} = \frac{TP + TN}{TP + FP + FN + TN}\]</span></p>
<p>La <strong>sensibilidad</strong> es la proporción de verdaderos positivos entre todos los valores positivos reales:</p>
<p><span class="math display">\[\text{Sensibilidad} = \frac{TP}{TP + FN}\]</span></p>
<p>La <strong>especificidad</strong> es la proporción de verdaderos negativos entre todos los valores negativos reales:</p>
<p><span class="math display">\[\text{Especificidad} = \frac{TN}{TN + FP}\]</span></p>
<p>El <strong>valor predictivo positivo</strong> es la proporción de verdaderos positivos entre todas las predicciones positivas realizadas por el modelo:</p>
<p><span class="math display">\[\text{Valor Predictivo Positivo} = \frac{TP}{TP + FP}\]</span></p>
<p>El <strong>valor predictivo negativo</strong> es la proporción de verdaderos negativos entre todas las predicciones negativas realizadas por el modelo:</p>
<p><span class="math display">\[\text{Valor Predictivo Negativo} = \frac{TN}{TN + FN}\]</span></p>
<p>En nuestro modelo de regresión logística tenemos los siguientes valores:</p>
<div id="58d647f3" class="cell" data-execution_count="28">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a><span class="co"># Precisión</span></span>
<span id="cb47-2"><a href="#cb47-2"></a>precision <span class="op">=</span> (cm[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">+</span> cm[<span class="dv">1</span>, <span class="dv">1</span>]) <span class="op">/</span> np.<span class="bu">sum</span>(cm)</span>
<span id="cb47-3"><a href="#cb47-3"></a><span class="bu">print</span>(<span class="ss">f'Precisión: </span><span class="sc">{</span>precision<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb47-4"><a href="#cb47-4"></a></span>
<span id="cb47-5"><a href="#cb47-5"></a><span class="co"># Especificidad</span></span>
<span id="cb47-6"><a href="#cb47-6"></a>specificity <span class="op">=</span> cm[<span class="dv">1</span>, <span class="dv">1</span>] <span class="op">/</span> (cm[<span class="dv">1</span>, <span class="dv">1</span>] <span class="op">+</span> cm[<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb47-7"><a href="#cb47-7"></a><span class="bu">print</span>(<span class="ss">f'Especificidad: </span><span class="sc">{</span>specificity<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb47-8"><a href="#cb47-8"></a></span>
<span id="cb47-9"><a href="#cb47-9"></a><span class="co"># Sensibilidad</span></span>
<span id="cb47-10"><a href="#cb47-10"></a>sensitivity <span class="op">=</span> cm[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">/</span> (cm[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">+</span> cm[<span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb47-11"><a href="#cb47-11"></a><span class="bu">print</span>(<span class="ss">f'Sensibilidad: </span><span class="sc">{</span>sensitivity<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb47-12"><a href="#cb47-12"></a></span>
<span id="cb47-13"><a href="#cb47-13"></a><span class="co"># Valor Predictivo Positivo</span></span>
<span id="cb47-14"><a href="#cb47-14"></a>ppv <span class="op">=</span> cm[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">/</span> (cm[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">+</span> cm[<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb47-15"><a href="#cb47-15"></a><span class="bu">print</span>(<span class="ss">f'Valor Predictivo Positivo: </span><span class="sc">{</span>ppv<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb47-16"><a href="#cb47-16"></a></span>
<span id="cb47-17"><a href="#cb47-17"></a><span class="co"># Valor Predictivo Negativo</span></span>
<span id="cb47-18"><a href="#cb47-18"></a>npv <span class="op">=</span> cm[<span class="dv">1</span>, <span class="dv">1</span>] <span class="op">/</span> (cm[<span class="dv">1</span>, <span class="dv">1</span>] <span class="op">+</span> cm[<span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb47-19"><a href="#cb47-19"></a><span class="bu">print</span>(<span class="ss">f'Valor Predictivo Negativo: </span><span class="sc">{</span>npv<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Precisión: 0.943
Especificidad: 0.9509071504802561
Sensibilidad: 0.8253968253968254
Valor Predictivo Positivo: 0.5306122448979592
Valor Predictivo Negativo: 0.9878048780487805</code></pre>
</div>
</div>
<p>Estas métricas son usadas también en la evaluación de modelos de clasificación como lo son las redes neuronales.</p>
<p>Otras métricas comunes para evaluar modelos de clasificación son el área bajo la curva ROC (AUC-ROC) y el área bajo la curva PR (AUC-PR). El AUC-ROC mide la capacidad del modelo para distinguir entre las clases positiva y negativa, mientras que el AUC-PR mide la precisión del modelo en la clase positiva.</p>
<div id="d05246a2" class="cell" data-execution_count="29">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score, average_precision_score</span>
<span id="cb49-2"><a href="#cb49-2"></a></span>
<span id="cb49-3"><a href="#cb49-3"></a><span class="co"># Área bajo la curva ROC</span></span>
<span id="cb49-4"><a href="#cb49-4"></a>roc_auc <span class="op">=</span> roc_auc_score(df[<span class="st">'y'</span>], df[<span class="st">'y_pred'</span>])</span>
<span id="cb49-5"><a href="#cb49-5"></a></span>
<span id="cb49-6"><a href="#cb49-6"></a><span class="co"># Área bajo la curva PR</span></span>
<span id="cb49-7"><a href="#cb49-7"></a>pr_auc <span class="op">=</span> average_precision_score(df[<span class="st">'y'</span>], df[<span class="st">'y_pred'</span>])</span>
<span id="cb49-8"><a href="#cb49-8"></a></span>
<span id="cb49-9"><a href="#cb49-9"></a><span class="bu">print</span>(<span class="ss">f'Área bajo la curva ROC: </span><span class="sc">{</span>roc_auc<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb49-10"><a href="#cb49-10"></a><span class="bu">print</span>(<span class="ss">f'Área bajo la curva PR: </span><span class="sc">{</span>pr_auc<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Área bajo la curva ROC: 0.7592085614733698
Área bajo la curva PR: 0.9503107218158627</code></pre>
</div>
</div>
<p>El AUC-ROC y el AUC-PR son valores entre 0 y 1, donde un valor de 1 indica un modelo perfecto y un valor de 0.5 indica un modelo aleatorio.</p>
<p>Podemos graficar la curva ROC y la curva PR para visualizar la calidad del modelo de regresión logística.</p>
<div id="de710c88" class="cell" data-execution_count="30">
<details open="" class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve, precision_recall_curve</span>
<span id="cb51-2"><a href="#cb51-2"></a></span>
<span id="cb51-3"><a href="#cb51-3"></a><span class="co"># Curva ROC</span></span>
<span id="cb51-4"><a href="#cb51-4"></a>fpr, tpr, _ <span class="op">=</span> roc_curve(df[<span class="st">'y'</span>], df[<span class="st">'y_pred'</span>])</span>
<span id="cb51-5"><a href="#cb51-5"></a></span>
<span id="cb51-6"><a href="#cb51-6"></a><span class="co"># Curva PR</span></span>
<span id="cb51-7"><a href="#cb51-7"></a>precision, recall, _ <span class="op">=</span> precision_recall_curve(df[<span class="st">'y'</span>], df[<span class="st">'y_pred'</span>])</span>
<span id="cb51-8"><a href="#cb51-8"></a></span>
<span id="cb51-9"><a href="#cb51-9"></a><span class="co"># Graficar curva ROC</span></span>
<span id="cb51-10"><a href="#cb51-10"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb51-11"><a href="#cb51-11"></a></span>
<span id="cb51-12"><a href="#cb51-12"></a>ax[<span class="dv">0</span>].plot(fpr, tpr)</span>
<span id="cb51-13"><a href="#cb51-13"></a>ax[<span class="dv">0</span>].plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], linestyle<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb51-14"><a href="#cb51-14"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">'Tasa de Falsos Positivos'</span>)</span>
<span id="cb51-15"><a href="#cb51-15"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">'Tasa de Verdaderos Positivos'</span>)</span>
<span id="cb51-16"><a href="#cb51-16"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Curva ROC'</span>)</span>
<span id="cb51-17"><a href="#cb51-17"></a></span>
<span id="cb51-18"><a href="#cb51-18"></a><span class="co"># Graficar curva PR</span></span>
<span id="cb51-19"><a href="#cb51-19"></a>ax[<span class="dv">1</span>].plot(recall, precision)</span>
<span id="cb51-20"><a href="#cb51-20"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'Recall'</span>)</span>
<span id="cb51-21"><a href="#cb51-21"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">'Precision'</span>)</span>
<span id="cb51-22"><a href="#cb51-22"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Curva PR'</span>)</span>
<span id="cb51-23"><a href="#cb51-23"></a></span>
<span id="cb51-24"><a href="#cb51-24"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Linear_models_files/figure-html/cell-30-output-1.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>La forma de la curva ROC y la curva PR nos permite evaluar la calidad del modelo de regresión logística. Una curva ROC que se acerca al punto (0, 1) y una curva PR que se acerca al punto (1, 1) indican un modelo de alta calidad. Cada punto en la curva ROC representa una combinación de la tasa de verdaderos positivos y la tasa de falsos positivos del modelo, mientras que cada punto en la curva PR representa una combinación de la precisión y el recall del modelo. El <strong>recall</strong> es la sensibilidad del modelo, es decir, la proporción de verdaderos positivos entre todos los valores positivos reales.</p>


</section>
</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Volver arriba</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/christian-f-badillo\.github\.io\/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../statistics/data-visualization.html" class="pagination-link" aria-label="Visualización de datos">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Visualización de datos</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../statistics/hipothesis_testing.html" class="pagination-link" aria-label="Prueba de Hipótesis">
        <span class="nav-page-text">Prueba de Hipótesis</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright <i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i> 2024 por Christian Francisco Badillo Hernández <br> Contenido con licencia <i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i> <i class="fa-brands fa-creative-commons-by" aria-label="creative-commons-by"></i> <i class="fa-brands fa-creative-commons-nc" aria-label="creative-commons-nc"></i> <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International license (CC BY-NC 4.0)</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/edit/main/statistics/Linear_models.qmd" class="toc-action"><i class="bi bi-github"></i>Editar esta página</a></li><li><a href="https://github.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/issues/new" class="toc-action"><i class="bi empty"></i>Informar de un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Hecho con <a href="https://www.python.org/"><i class="fa-brands fa-python" aria-label="python"></i></a> y <a href="https://quarto.org">Quarto</a> <br> Código fuente disponible en <a href="https://github.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales">GitHub</a></p>
</div>
  </div>
</footer>




</body></html>