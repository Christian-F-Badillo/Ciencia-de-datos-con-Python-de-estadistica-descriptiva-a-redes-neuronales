[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Christian Francisco Badillo Hernández",
    "section": "",
    "text": "twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n\n  \n  \nChristian Badiillo es egresado de la Facultad de Psicología, UNAM con apasionado interés por el estudio del comportamiento humano su modelamiento computacional y el análisis estadístico bayesiano de datos. Actualmente es miembro del Lab25 “Aprendizaje y Comportamiento Adaptable” de la Facultad de Psicología, UNAM donde se dedica a la investigación en el campo del modelamiento computacional del comportamiento y su intersección con la inteligencia artificial.\n\n\nFacultad de Psicología UNAM | Ciudad de México, México Licenciatura en Psicología | Ago 2019 - Ago 2023\n\n\n\nIIMAS | Servicio Social - Desarrollo de sistemas inteligentes usando deep learning | Feb 2024 - presente\nFacultad de Psicología UNAM | Impartición de Curso Intersemestral - Temas Selectos en Estadística | Periodo 2024-1\nUNAM | Curso de Verano - Macroentrenamiento en Inteligencia Artificial (MeIA) 2023. | Jun 2023\nNeuromatch Academy | Curso de Verano - NMA Computational Neuroscience | Jul 2022",
    "crumbs": [
      "Acerca"
    ]
  },
  {
    "objectID": "about.html#educación",
    "href": "about.html#educación",
    "title": "Christian Francisco Badillo Hernández",
    "section": "",
    "text": "Facultad de Psicología UNAM | Ciudad de México, México Licenciatura en Psicología | Ago 2019 - Ago 2023",
    "crumbs": [
      "Acerca"
    ]
  },
  {
    "objectID": "about.html#experiencia",
    "href": "about.html#experiencia",
    "title": "Christian Francisco Badillo Hernández",
    "section": "",
    "text": "IIMAS | Servicio Social - Desarrollo de sistemas inteligentes usando deep learning | Feb 2024 - presente\nFacultad de Psicología UNAM | Impartición de Curso Intersemestral - Temas Selectos en Estadística | Periodo 2024-1\nUNAM | Curso de Verano - Macroentrenamiento en Inteligencia Artificial (MeIA) 2023. | Jun 2023\nNeuromatch Academy | Curso de Verano - NMA Computational Neuroscience | Jul 2022",
    "crumbs": [
      "Acerca"
    ]
  },
  {
    "objectID": "statistics/hipothesis_testing.html",
    "href": "statistics/hipothesis_testing.html",
    "title": "Prueba de Hipótesis",
    "section": "",
    "text": "Dentro del quehacer de la ciencia, una de las tareas más comunes es la de probar hipótesis. En términos generales, una hipótesis es una afirmación que se hace sobre el valor de un parámetro poblacional. Por ejemplo, una hipótesis podría ser que la media de una población es igual a un valor específico. Para probar esta hipótesis, se recolecta una muestra de la población y se calcula la media muestral. Si la media muestral es muy diferente de la media hipotética, entonces se puede concluir que la hipótesis es falsa. En este caso, se dice que la hipótesis es rechazada. Por otro lado, si la media muestral es muy similar a la media hipotética, entonces se puede concluir que la hipótesis es verdadera. En este caso, se dice que la hipótesis es aceptada.\nNo debemos tomar tan a la ligera la decisión de rechazar o aceptar una hipótesis. Siempre existe la posibilidad de que la muestra que se recolectó no sea representativa de la población. Por lo tanto, es importante tener en cuenta el error que se comete al rechazar una hipótesis verdadera. Este error se conoce como error de tipo I. Por otro lado, también es importante tener en cuenta el error que se comete al aceptar una hipótesis falsa. Este error se conoce como error de tipo II. En general, se busca minimizar ambos errores. Sin embargo, es imposible minimizar ambos errores al mismo tiempo. Por lo tanto, se debe tomar una decisión sobre cuál de los dos errores es más grave. Esta decisión depende del contexto en el que se esté trabajando. Pero en general, se prefiere minimizar el error de tipo I.\nPara probar una hipótesis, se sigue un procedimiento estándar.\nEl valor p depende directamente de los datos y del procedimiento de su recolección. Por si mismo no es suficiente para tomar una decisión. Por lo tanto, es importante tener en cuenta el contexto en el que se está trabajando. Una medida auxiliar es el poder de la prueba. El poder de una prueba es la probabilidad de rechazar la hipótesis nula dado que la hipótesis nula es falsa. En general, se busca que el poder de la prueba sea lo más alto posible. Sin embargo, el poder de la prueba depende de varios factores, como el tamaño de la muestra, el nivel de significancia y la magnitud del efecto.\nDe mayor importancia que obtener valores p bajos es tener un buen diseño experimental. Un buen diseño experimental es aquel que permite obtener resultados confiables y válidos. Para ello, es importante tener en cuenta varios factores, como el tamaño de la muestra, la selección de la muestra, la recolección de los datos y el análisis de los datos. En general, se busca que el diseño experimental sea lo más simple posible. Sin embargo, es importante tener en cuenta que un diseño experimental simple no siempre es el mejor.\nUna medida que se sugiera reportar el tamaño del efecto de la prueba. El tamaño del efecto es una medida de la magnitud del efecto que se está estudiando, es de gran importancia en la interpretación de los resultados. Puede ser absoluta o relativo.\nExisten distintas medidas del tamaño del efecto, como la diferencia de medias, la razón de medias, la diferencia de proporciones, la razón de proporciones, la correlación, la regresión, entre otras. La elección de la medida del tamaño del efecto depende del contexto en el que se esté trabajando. En general, se busca que el tamaño del efecto sea lo más grande posible. Sin embargo, es importante tener en cuenta que un tamaño del efecto grande no siempre es mejor. Por ejemplo, un tamaño del efecto grande puede ser el resultado de un error en la recolección de los datos.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Prueba de Hipótesis"
    ]
  },
  {
    "objectID": "statistics/hipothesis_testing.html#prueba-de-hipótesis-para-la-media-de-una-población.",
    "href": "statistics/hipothesis_testing.html#prueba-de-hipótesis-para-la-media-de-una-población.",
    "title": "Prueba de Hipótesis",
    "section": "Prueba de Hipótesis para la Media de una Población.",
    "text": "Prueba de Hipótesis para la Media de una Población.\nSupongamos que tenemos los datos de la estatura de 1000 personas y queremos saber si la media es distinta de 1.70 m. Para ello, planteamos las siguientes hipótesis:\n\\[H_0: \\mu = 1.70\\]\n\\[H_1: \\mu \\neq 1.70\\]\nDonde \\(\\mu\\) es la media de la población. Supongamos que la media muestral es de 1.75 m y la desviación estándar es de 0.10 m. Para probar estas hipótesis, calculamos el estadístico de prueba:\n\\[t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\\]\nDonde \\(\\bar{x}\\) es la media muestral, \\(s\\) es la desviación estándar muestral y \\(n\\) es el tamaño de la muestra. En este caso, el valor del estadístico de prueba es:\n\\[t = \\frac{1.75 - 1.70}{0.10/\\sqrt{1000}} = 15.81\\]\nPara calcular el valor p, necesitamos la distribución del estadístico de prueba. En este caso, la distribución del estadístico de prueba es una distribución t de Student con 999 grados de libertad. El valor p es la probabilidad de obtener un estadístico de prueba tan extremo como el que se obtuvo dado que la hipótesis nula es verdadera. En este caso, el valor p es:\n\n\nCódigo\nimport scipy.stats as stats\n\nt = 15.81\np = 2 * (1 - stats.t.cdf(t, 999))\n\nprint(f'El valor p es {p}')\n\n\nEl valor p es 0.0\n\n\nVisualizemos la distribución del estadístico de prueba:\n\n\nCódigo\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(-20, 20, 1000)\ny = stats.t.pdf(x, 999)\n\nplt.plot(x, y)\nplt.axvline(t, color='red', linestyle='--')\nplt.xlabel('Estadístico de Prueba')\nplt.ylabel('Densidad')\nplt.title('Distribución del Estadístico de Prueba')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nSi definimos un nivel de significancia de 0.05, entonces podemos concluir que hay evidencia para rechazar la hipótesis nula. Por lo tanto, podemos concluir que la media de la población es distinta de 1.70 m.\nLa regigión crítica es aquella en la que se rechaza la hipótesis nula, debe sumar el nivel de significancia. Visualicemos la región crítica:\n\n\nCódigo\nx = np.linspace(-20, 20, 1000)\ny = stats.t.pdf(x, 999)\n\nplt.plot(x, y)\nplt.axvline(t, color='red', linestyle='--')\n\nt1 = stats.t.ppf(0.025, 999) # 0.025 porque es una prueba de dos colas\nt2 = stats.t.ppf(0.975, 999) # 0.975 porque es una prueba de dos colas\n\nplt.fill_between(x, y, where=(x &lt; t1), color='green', alpha=0.5)\nplt.fill_between(x, y, where=(x &gt; t2), color='green', alpha=0.5)\n\n\nplt.xlabel('Estadístico de Prueba')\nplt.ylabel('Densidad')\nplt.title('Distribución del Estadístico de Prueba')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nObservamos que el estadístico de prueba cae en la región crítica. Por lo tanto, podemos concluir que hay evidencia para rechazar la hipótesis nula.\nPodemos hacer este test con la función ttest_1samp de scipy.stats:\n\n\nCódigo\nnp.random.seed(123)\nx = np.random.normal(1.75, 0.10, 1000)\n\nresult = stats.ttest_1samp(x, 1.70)\nlower, upper = result.confidence_interval(confidence_level=0.95)\n\nprint(f'El valor p es {result.pvalue}')\nprint(f'El estadístico de prueba es {result.statistic}')\nprint(f\"Los grado de libertad son {result.df}\")\nprint(f\"El intervalo de confianza para la media es [{lower}, {upper}]\")\n\n\nEl valor p es 1.3453036210893457e-43\nEl estadístico de prueba es 14.54152651527113\nLos grado de libertad son 999\nEl intervalo de confianza para la media es [1.7398301232798288, 1.7522570495040128]",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Prueba de Hipótesis"
    ]
  },
  {
    "objectID": "statistics/hipothesis_testing.html#prueba-de-hipótesis-para-la-diferencia-de-medias-de-dos-poblaciones.",
    "href": "statistics/hipothesis_testing.html#prueba-de-hipótesis-para-la-diferencia-de-medias-de-dos-poblaciones.",
    "title": "Prueba de Hipótesis",
    "section": "Prueba de Hipótesis para la Diferencia de Medias de dos Poblaciones.",
    "text": "Prueba de Hipótesis para la Diferencia de Medias de dos Poblaciones.\nSupongamos que tenemos los datos de la estatura de 1000 hombres y 1000 mujeres y queremos saber si la media de los hombres es distinta de la media de las mujeres. Para ello, planteamos las siguientes hipótesis:\n\\[H_0: \\mu_1 - \\mu_2 = 0\\]\n\\[H_1: \\mu_1 - \\mu_2 \\neq 0\\]\nDonde \\(\\mu_1\\) es la media de los hombres y \\(\\mu_2\\) es la media de las mujeres. Supongamos que la media de los hombres es de 1.75 m, la desviación estándar de los hombres es de 0.10 m, la media de las mujeres es de 1.55 m y la desviación estándar de las mujeres es de 0.10 m. Para probar estas hipótesis, calculamos el estadístico de prueba:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}}\\]\nDonde \\(\\bar{x}_1\\) es la media de los hombres, \\(\\bar{x}_2\\) es la media de las mujeres, \\(s_1\\) es la desviación estándar de los hombres, \\(s_2\\) es la desviación estándar de las mujeres, \\(n_1\\) es el tamaño de la muestra de los hombres y \\(n_2\\) es el tamaño de la muestra de las mujeres. En este caso, el valor del estadístico de prueba es:\n\\[t = \\frac{1.75 - 1.55}{\\sqrt{0.10^2/1000 + 0.10^2/1000}} = 31.62\\]\nGeneremos datos con python y calculemos el estadístico de prueba:\n\n\nCódigo\nnp.random.seed(123)\nx1 = np.random.normal(1.75, 0.10, 1000)\n\nnp.random.seed(123)\nx2 = np.random.normal(1.55, 0.10, 1000)\n\nt = (x1.mean() - x2.mean()) / np.sqrt((x1.var() / 1000) + (x2.var() / 1000))\n\nprint(f'El estadístico de prueba es {t}')\n\n\nEl estadístico de prueba es 44.68616751661976\n\n\nPara calcular el valor p, necesitamos la distribución del estadístico de prueba. En este caso, la distribución del estadístico de prueba es una distribución t de Student con 1998 grados de libertad. Los grados se calculan como \\(n_1 + n_2 - 2\\). Visualicemos la distribución del estadístico de prueba, la región crítica y el valor p:\n\n\nCódigo\nx = np.linspace(-50, 50, 5000)\ny = stats.t.pdf(x, 1998)\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nax.plot(x, y)\nax.axvline(t, color='red', linestyle='--')\n\nt1 = stats.t.ppf(0.025, 1998) \nt2 = stats.t.ppf(0.975, 1998)\n\nax.fill_between(x, y, where=(x &lt; t1), color='green', alpha=0.5)\nax.fill_between(x, y, where=(x &gt; t2), color='green', alpha=0.5)\n\nplt.xlabel('Estadístico de Prueba')\nplt.ylabel('Densidad')\nplt.title('Distribución del Estadístico de Prueba')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nComo el valor p es muy pequeño, podemos concluir que hay evidencia para rechazar la hipótesis nula. Por lo tanto, podemos concluir que la media de los hombres es distinta de la media de las mujeres.\nPodemos hacer este test con la función ttest_ind de scipy.stats:\n\n\nCódigo\nresult = stats.ttest_ind(x1, x2)\n\nlower, upper = result.confidence_interval(confidence_level=0.95)\n\nprint(f'El valor p es {result.pvalue}')\nprint(f'El estadístico de prueba es {result.statistic}')\nprint(f\"Los grado de libertad son {result.df}\")\nprint(f\"El intervalo de confianza para la diferencia de medias es [{lower}, {upper}]\")\nprint(f\"Diff de medias {x1.mean() - x2.mean()}\")\n\n\nEl valor p es 1.0339591135264653e-302\nEl estadístico de prueba es 44.66381884429588\nLos grado de libertad son 1998.0\nEl intervalo de confianza para la diferencia de medias es [0.1912181624160999, 0.2087818375839]\nDiff de medias 0.19999999999999996\n\n\nCon statsmodels igualmente podemos hacer este test:\n\n\nCódigo\nimport statsmodels.api as sm\n\nttest = sm.stats.ttest_ind(x1, x2)\n\nprint(f'El estadístico de prueba es {ttest[0]}')\nprint(f'El valor p es {ttest[1]}')\nprint(f\"Los grado de libertad son {ttest[2]}\")\n\n\nEl estadístico de prueba es 44.663818844295825\nEl valor p es 1.033959113527759e-302\nLos grado de libertad son 1998.0\n\n\n\nTamaño del Efecto\nEl tamaño del efecto es una medida de la magnitud del efecto que se está estudiando. En este caso, el tamaño del efecto es:\n\\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s}\\]\nDonde \\(\\bar{x}_1\\) es la media de los hombres, \\(\\bar{x}_2\\) es la media de las mujeres, \\(s\\) es la desviación estándár combinada, que Jacob Cohen sugiere que se calcule como:\n\\[s = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\\]\nLa varianza para cada grupo se calcula como:\n\\[s_i^2 = \\frac{\\sum_{j=1}^{n_1}(x_{ij} - \\bar{x}_i)^2}{n_i - 1}\\]\nEn esta tabla podemos ver el tamaño del efecto para distintos valores de \\(d\\):\n\n\n\nTamaño del Efecto\nd\n\n\n\n\nMuy Pequeño\n0.01\n\n\nPequeño\n0.20\n\n\nMediano\n0.50\n\n\nGrande\n0.80\n\n\nMuy Grande\n1.20\n\n\nEnorme\n2.00\n\n\n\nVamos a crear una función que calcule el tamaño del efecto:\n\n\nCódigo\ndef effect_size(x1, x2):\n    n1 = len(x1)\n    n2 = len(x2)\n    \n    s1 = np.sqrt(np.sum((x1 - x1.mean())**2) / (n1 - 1))\n    s2 = np.sqrt(np.sum((x2 - x2.mean())**2) / (n2 - 1))\n    \n    s = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n    \n    d = (x1.mean() - x2.mean()) / s\n    \n    return d\n\nd = effect_size(x1, x2)\n\nprint(f'El tamaño del efecto es {d}')\n\n\nEl tamaño del efecto es 1.9974267014116336\n\n\nEl efecto es muy grande, por lo que podemos concluir que la diferencia entre las medias de los hombres y las mujeres es muy grande.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Prueba de Hipótesis"
    ]
  },
  {
    "objectID": "statistics/hipothesis_testing.html#prueba-de-hipótesis-para-más-de-dos-poblaciones.",
    "href": "statistics/hipothesis_testing.html#prueba-de-hipótesis-para-más-de-dos-poblaciones.",
    "title": "Prueba de Hipótesis",
    "section": "Prueba de Hipótesis para más de dos Poblaciones.",
    "text": "Prueba de Hipótesis para más de dos Poblaciones.\nSupongamos que tenemos los datos de la estatura de 1000 personas de tres países distintos y queremos saber si la media de las tres poblaciones es distinta. Para ello, planteamos las siguientes hipótesis:\n\\[H_0: \\mu_1 = \\mu_2 = \\mu_3\\]\n\\[H_1: \\mu_1 \\neq \\mu_2 \\neq \\mu_3\\]\nDonde \\(\\mu_1\\) es la media de las personas del país 1, \\(\\mu_2\\) es la media de las personas del país 2 y \\(\\mu_3\\) es la media de las personas del país 3. Supongamos que la media de las personas del país 1 es de 1.75 m, la desviación estándar de las personas del país 1 es de 0.10 m, la media de las personas del país 2 es de 1.55 m, la desviación estándar de las personas del país 2 es de 0.10 m, la media de las personas del país 3 es de 1.65 m y la desviación estándar de las personas del país 3 es de 0.10 m. Para probar estas hipótesis, calculamos el estadístico de prueba:\n\\[F = \\frac{MS_{\\text{entre}}}{MS_{\\text{dentro}}}\\]\nDonde \\(MS_{\\text{entre}}\\) es la media de las varianzas entre las poblaciones y \\(MS_{\\text{dentro}}\\) es la media de las varianzas dentro de las poblaciones.\n\\[MS_{\\text{entre}} = \\frac{\\sum_{i=1}^{k}n_i(\\bar{x}_i - \\bar{x})^2}{k - 1}\\]\n\\[MS_{\\text{dentro}} = \\frac{\\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(x_{ij} - \\bar{x}_i)^2}{n - k}\\]\nEn este caso, el valor del estadístico de prueba es:\n\n\nCódigo\nnp.random.seed(123)\nx1 = np.random.normal(1.75, 0.10, 1000)\nx2 = np.random.normal(1.55, 0.10, 1000)\nx3 = np.random.normal(1.65, 0.10, 1000)\n\nx = np.concatenate([x1, x2, x3])\n\nMs_between = (1000 * (x1.mean() - x.mean())**2 + 1000 * (x2.mean() - x.mean())**2 + 1000 * (x3.mean() - x.mean())**2) / 2\n\nMs_within = (np.sum((x1 - x1.mean())**2) + np.sum((x2 - x2.mean())**2) + np.sum((x3 - x3.mean())**2)) / 2997\n\nF = Ms_between / Ms_within\n\nprint(f'El estadístico de prueba es {F}')\n\n\nEl estadístico de prueba es 990.6545267572889\n\n\nPara calcular el valor p, necesitamos la distribución del estadístico de prueba. En este caso, la distribución del estadístico de prueba es una distribución F de Fisher con 2 y 2998 grados de libertad. Los grados de libertad se calculan como \\(k - 1\\) y \\(n - k\\), donde \\(k\\) es el número de poblaciones y \\(n\\) es el número total de observaciones. Visualicemos la distribución del estadístico de prueba, la región crítica y el valor p:\n\n\nCódigo\nx = np.linspace(0, 10, 1000)\ny = stats.f.pdf(x, 2, 2998)\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nax.plot(x, y, label=f'F(2, 2998), F = {round(F, 2)}')\n\nf1 = stats.f.ppf(0.025, 2, 2998)\nf2 = stats.f.ppf(0.975, 2, 2998)\n\nax.fill_between(x, y, where=(x &lt; f1), color='green', alpha=0.5)\nax.fill_between(x, y, where=(x &gt; f2), color='green', alpha=0.5)\n\nplt.xlabel('Estadístico de Prueba')\nplt.ylabel('Densidad')\nplt.title('Distribución del Estadístico de Prueba')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nPodemos concluir que hay evidencia para rechazar la hipótesis nula. Por lo tanto, podemos concluir que la media de las tres poblaciones es distinta.\nPodemos hacer este test con la función f_oneway de scipy.stats:\n\n\nCódigo\nresult = stats.f_oneway(x1, x2, x3)\n\nprint(f'El valor p es {result.pvalue}')\nprint(f'El estadístico de prueba es {result.statistic}')\n\n\nEl valor p es 0.0\nEl estadístico de prueba es 990.6545267572892\n\n\nCon statsmodels igualmente podemos hacer este test:\n\n\nCódigo\nimport statsmodels.api as sm\nimport pandas as pd\nx = np.concatenate([x1, x2, x3])\n\ndf = pd.DataFrame({'x': x, 'group': ['x1'] * 1000 + ['x2'] * 1000 + ['x3'] * 1000})\n\nlinear_model = sm.formula.ols('x ~ group', data=df).fit()\n\nanova = sm.stats.anova_lm(linear_model)\n\nprint(anova)\n\n\n              df     sum_sq   mean_sq           F  PR(&gt;F)\ngroup        2.0  19.056918  9.528459  990.654527     0.0\nResidual  2997.0  28.826185  0.009618         NaN     NaN\n\n\nLa interpretación es que hay evidencia para rechazar la hipótesis nula de igualdad de medias.\nPara averiguar cuales medias son distintas podemos hacer un test post-hoc, como el test de Tukey:\n\n\nCódigo\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\ntukey = pairwise_tukeyhsd(df['x'], df['group'])\n\nprint(tukey)\n\n\nMultiple Comparison of Means - Tukey HSD, FWER=0.05\n===================================================\ngroup1 group2 meandiff p-adj  lower   upper  reject\n---------------------------------------------------\n    x1     x2  -0.1952   0.0 -0.2055 -0.1849   True\n    x1     x3   -0.095   0.0 -0.1053 -0.0847   True\n    x2     x3   0.1002   0.0  0.0899  0.1105   True\n---------------------------------------------------\n\n\nEl resultado nos dice que las medias de las tres poblaciones son distintas entre sí de manera significativa.\nSi tuvieramos un grupo de control y varios grupos de tratamiento, podríamos hacer un test de Dunnett:\n\n\nCódigo\nfrom scipy.stats import dunnett\n\nresult = dunnett(x2, x3, control=x1)\n\nprint(result)\n\n\nDunnett's test (95.0% Confidence Interval)\nComparison               Statistic  p-value  Lower CI  Upper CI\n (Sample 0 - Control)    -44.507     0.000    -0.205    -0.185\n (Sample 1 - Control)    -21.662     0.000    -0.105    -0.085\n\n\n\nExisten otros test post-hoc, como el test de Bonferroni, el test de Scheffé, el test de Holm, entre otros. La elección del test post-hoc depende del objetivo del estudio.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Prueba de Hipótesis"
    ]
  },
  {
    "objectID": "statistics/data_manipulation.html",
    "href": "statistics/data_manipulation.html",
    "title": "Manipulación de datos con Pandas",
    "section": "",
    "text": "Una base de datos es un conjunto de datos organizados y relacionados entre sí. Existen diferentes tipos de bases de datos, como las bases de datos relacionales y las bases de datos no relacionales.\nLas bases de datos relacionales son las más comunes y se basan en el modelo relacional, que organiza los datos en tablas. Cada tabla tiene filas y columnas, donde cada fila representa un registro y cada columna representa un campo.\nLas bases de datos no relacionales son aquellas que no se basan en el modelo relacional y pueden tener diferentes estructuras, como documentos, gráficos, clave-valor, entre otros.\nExisten lenguajes de consulta para interactuar con las bases de datos, como SQL (Structured Query Language) para las bases de datos relacionales y NoSQL (Not only SQL) para las bases de datos no relacionales.\nAunque existen paqueterias en Python para interactuar con bases de datos, en este curso nos enfocaremos en la manipulación de datos con Pandas, una librería de Python que nos permite manipular datos de manera sencilla y eficiente. Los datos que cargarémos en este curso son datos en formato CSV, que es un formato de archivo que se utiliza para almacenar datos en forma de tabla.\nA diferencia de las bases de datos, los datos en formato CSV no tienen un motor de base de datos que nos permita interactuar con ellos. Por lo tanto, no podemos hacer consultas con SQL, pero podemos manipular los datos con Pandas.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Manipulación de datos con Pandas"
    ]
  },
  {
    "objectID": "statistics/data_manipulation.html#explorando-los-datos",
    "href": "statistics/data_manipulation.html#explorando-los-datos",
    "title": "Manipulación de datos con Pandas",
    "section": "Explorando los datos",
    "text": "Explorando los datos\nPrimero como ya vimos en la lección anterior, podemos ver las primeras filas de los datos con el método head(). En este caso tenemos datos de hongos, donde cada fila representa un hongo y cada columna una característica del hongo.\nLas columnas son las siguientes:\n\nCap Diameter: Diámetro del sombrero.\nCap Shape: Forma del sombrero.\nGill Attachment: Tipo de unión de las branquias.\nGill Color: Color de las branquias.\nStem Height: Altura del tallo.\nStem Width: Ancho del tallo.\nStem Color: Color del tallo.\nSeason: Temporada en la que se encontró el hongo.\nClass: Si es comestible (0) o venerenoso (1).\n\n\n\nCódigo\ndata.head()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nstem-color\nseason\nclass\n\n\n\n\n0\n1372\n2\n2\n10\n3.807467\n1545\n11\n1.804273\n1\n\n\n1\n1461\n2\n2\n10\n3.807467\n1557\n11\n1.804273\n1\n\n\n2\n1371\n2\n2\n10\n3.612496\n1566\n11\n1.804273\n1\n\n\n3\n1261\n6\n2\n10\n3.787572\n1566\n11\n1.804273\n1\n\n\n4\n1305\n6\n2\n10\n3.711971\n1464\n11\n0.943195\n1\n\n\n\n\n\n\n\n\nLo primero que podemos hacer es ver la forma de los datos, es decir, cuántas filas y columnas tenemos. Para esto usamos el atributo shape.\n\n\nCódigo\ndata.shape\n\n\n(54035, 9)\n\n\nTenemos 54039 y 9 columnas en nuestros datos. Podemos ver los nombres de las columnas con el atributo columns.\n\n\nCódigo\ndata.columns\n\n\nIndex(['cap-diameter', 'cap-shape', 'gill-attachment', 'gill-color',\n       'stem-height', 'stem-width', 'stem-color', 'season', 'class'],\n      dtype='object')\n\n\nTambién podemos ver los tipos de datos de las columnas con el atributo dtypes.\n\n\nCódigo\ndata.dtypes\n\n\ncap-diameter         int64\ncap-shape            int64\ngill-attachment      int64\ngill-color           int64\nstem-height        float64\nstem-width           int64\nstem-color           int64\nseason             float64\nclass                int64\ndtype: object\n\n\nEl tipo de dato es muy importante, ya que restringe las cosas que podemos hacer con los datos. En este dataset, tenemos datos tipo int64 y float64 que son enteros y décimales respectivamente. El tipo de dato más común en Pandas es object, que es un tipo de dato genérico que puede contener cualquier tipo de dato, no es recomendable tener columnas con este tipo de dato, ya que puede ser un indicio de que los datos no están limpios.\nCon el metodo info() podemos ver un resumen de los datos, incluyendo el número de valores no nulos y el uso de memoria.\n\n\nCódigo\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 54035 entries, 0 to 54034\nData columns (total 9 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   cap-diameter     54035 non-null  int64  \n 1   cap-shape        54035 non-null  int64  \n 2   gill-attachment  54035 non-null  int64  \n 3   gill-color       54035 non-null  int64  \n 4   stem-height      54035 non-null  float64\n 5   stem-width       54035 non-null  int64  \n 6   stem-color       54035 non-null  int64  \n 7   season           54035 non-null  float64\n 8   class            54035 non-null  int64  \ndtypes: float64(2), int64(7)\nmemory usage: 3.7 MB\n\n\nGracias a este método podemos ver que no tenemos valores nulos en nuestro dataset. Si tuvieramos valores nulos, tendríamos que decidir si eliminarlos, reemplazarlos o imputarlos (usar un valor promedio, mediana, moda, etc. para reemplazar los valores nulos).\nPara identificar los valores NA en los datos, podemos usar el método isna(), combinado con el método sum() para ver el número de valores NA en cada columna.\n\n\nCódigo\ndata.isna().sum()\n\n\ncap-diameter       0\ncap-shape          0\ngill-attachment    0\ngill-color         0\nstem-height        0\nstem-width         0\nstem-color         0\nseason             0\nclass              0\ndtype: int64\n\n\nPodemos identificar los valores únicos de una columna con el método unique(). Por ejemplo, si queremos ver los valores únicos de la columna class, podemos hacer lo siguiente:\n\n\nCódigo\ndata['class'].unique()\n\n\narray([1, 0])\n\n\nEn este caso, los valores únicos son 0 y 1, que representan si el hongo es comestible o venenoso. Podemos ver los valores únicos de todas las columnas con el método unique().",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Manipulación de datos con Pandas"
    ]
  },
  {
    "objectID": "statistics/data_manipulation.html#estadísticas-descriptivas",
    "href": "statistics/data_manipulation.html#estadísticas-descriptivas",
    "title": "Manipulación de datos con Pandas",
    "section": "Estadísticas descriptivas",
    "text": "Estadísticas descriptivas\nPodemos obtener estadísticas descriptivas de los datos numéricos con el método describe().\n\n\nCódigo\ndata.describe()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nstem-color\nseason\nclass\n\n\n\n\ncount\n54035.000000\n54035.000000\n54035.000000\n54035.000000\n54035.000000\n54035.000000\n54035.000000\n54035.000000\n54035.000000\n\n\nmean\n567.257204\n4.000315\n2.142056\n7.329509\n0.759110\n1051.081299\n8.418062\n0.952163\n0.549181\n\n\nstd\n359.883763\n2.160505\n2.228821\n3.200266\n0.650969\n782.056076\n3.262078\n0.305594\n0.497580\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000426\n0.000000\n0.000000\n0.027372\n0.000000\n\n\n25%\n289.000000\n2.000000\n0.000000\n5.000000\n0.270997\n421.000000\n6.000000\n0.888450\n0.000000\n\n\n50%\n525.000000\n5.000000\n1.000000\n8.000000\n0.593295\n923.000000\n11.000000\n0.943195\n1.000000\n\n\n75%\n781.000000\n6.000000\n4.000000\n10.000000\n1.054858\n1523.000000\n11.000000\n0.943195\n1.000000\n\n\nmax\n1891.000000\n6.000000\n6.000000\n11.000000\n3.835320\n3569.000000\n12.000000\n1.804273\n1.000000\n\n\n\n\n\n\n\n\nPor comodidad se suele usar el método transpose() para ver las estadísticas descriptivas de manera vertical, una abreviatura de esto es .T.\n\n\nCódigo\ndata.describe().T\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncap-diameter\n54035.0\n567.257204\n359.883763\n0.000000\n289.000000\n525.000000\n781.000000\n1891.000000\n\n\ncap-shape\n54035.0\n4.000315\n2.160505\n0.000000\n2.000000\n5.000000\n6.000000\n6.000000\n\n\ngill-attachment\n54035.0\n2.142056\n2.228821\n0.000000\n0.000000\n1.000000\n4.000000\n6.000000\n\n\ngill-color\n54035.0\n7.329509\n3.200266\n0.000000\n5.000000\n8.000000\n10.000000\n11.000000\n\n\nstem-height\n54035.0\n0.759110\n0.650969\n0.000426\n0.270997\n0.593295\n1.054858\n3.835320\n\n\nstem-width\n54035.0\n1051.081299\n782.056076\n0.000000\n421.000000\n923.000000\n1523.000000\n3569.000000\n\n\nstem-color\n54035.0\n8.418062\n3.262078\n0.000000\n6.000000\n11.000000\n11.000000\n12.000000\n\n\nseason\n54035.0\n0.952163\n0.305594\n0.027372\n0.888450\n0.943195\n0.943195\n1.804273\n\n\nclass\n54035.0\n0.549181\n0.497580\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\nAlgunas estadísticas descriptivas que obtenemos son:\n\ncount: Número de valores no nulos.\nmean: Media de los valores.\nstd: Desviación estándar de los valores.\nmin: Valor mínimo.\n25%: Primer cuartil.\n50%: Mediana.\n75%: Tercer cuartil.\nmax: Valor máximo.\n\nPodemos guardar estas estadísticas en un nuevo DataFrame para manipularlas más adelante.\n\n\nCódigo\nestadisticas = data.describe().T\n\n\nSi recuerdan sus cursos de estadística una medida de dispersión es el rango, que es la diferencia entre el valor máximo y el valor mínimo. Podemos calcular el rango de los datos restándo el valor máximo y el valor mínimo de cada columna y guardarlo en una nueva columna llamada rango.\n\n\nCódigo\nestadisticas['rango'] = estadisticas['max'] - estadisticas['min']\nestadisticas.head(9)\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\nrango\n\n\n\n\ncap-diameter\n54035.0\n567.257204\n359.883763\n0.000000\n289.000000\n525.000000\n781.000000\n1891.000000\n1891.000000\n\n\ncap-shape\n54035.0\n4.000315\n2.160505\n0.000000\n2.000000\n5.000000\n6.000000\n6.000000\n6.000000\n\n\ngill-attachment\n54035.0\n2.142056\n2.228821\n0.000000\n0.000000\n1.000000\n4.000000\n6.000000\n6.000000\n\n\ngill-color\n54035.0\n7.329509\n3.200266\n0.000000\n5.000000\n8.000000\n10.000000\n11.000000\n11.000000\n\n\nstem-height\n54035.0\n0.759110\n0.650969\n0.000426\n0.270997\n0.593295\n1.054858\n3.835320\n3.834894\n\n\nstem-width\n54035.0\n1051.081299\n782.056076\n0.000000\n421.000000\n923.000000\n1523.000000\n3569.000000\n3569.000000\n\n\nstem-color\n54035.0\n8.418062\n3.262078\n0.000000\n6.000000\n11.000000\n11.000000\n12.000000\n12.000000\n\n\nseason\n54035.0\n0.952163\n0.305594\n0.027372\n0.888450\n0.943195\n0.943195\n1.804273\n1.776901\n\n\nclass\n54035.0\n0.549181\n0.497580\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\nOtro estadístico importante es la varianza, que es una medida de dispersión que nos indica qué tan dispersos están los datos. Podemos calcular la varianza de los datos con el método var() y guardarla en una nueva columna llamada varianza.\n\n\nCódigo\nestadisticas['varianza'] = data.var()\nestadisticas.head(9)\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\nrango\nvarianza\n\n\n\n\ncap-diameter\n54035.0\n567.257204\n359.883763\n0.000000\n289.000000\n525.000000\n781.000000\n1891.000000\n1891.000000\n129516.323081\n\n\ncap-shape\n54035.0\n4.000315\n2.160505\n0.000000\n2.000000\n5.000000\n6.000000\n6.000000\n6.000000\n4.667783\n\n\ngill-attachment\n54035.0\n2.142056\n2.228821\n0.000000\n0.000000\n1.000000\n4.000000\n6.000000\n6.000000\n4.967642\n\n\ngill-color\n54035.0\n7.329509\n3.200266\n0.000000\n5.000000\n8.000000\n10.000000\n11.000000\n11.000000\n10.241701\n\n\nstem-height\n54035.0\n0.759110\n0.650969\n0.000426\n0.270997\n0.593295\n1.054858\n3.835320\n3.834894\n0.423760\n\n\nstem-width\n54035.0\n1051.081299\n782.056076\n0.000000\n421.000000\n923.000000\n1523.000000\n3569.000000\n3569.000000\n611611.705997\n\n\nstem-color\n54035.0\n8.418062\n3.262078\n0.000000\n6.000000\n11.000000\n11.000000\n12.000000\n12.000000\n10.641151\n\n\nseason\n54035.0\n0.952163\n0.305594\n0.027372\n0.888450\n0.943195\n0.943195\n1.804273\n1.776901\n0.093387\n\n\nclass\n54035.0\n0.549181\n0.497580\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.247586\n\n\n\n\n\n\n\n\nUn último estadístico es el rango intercuartil (IQR), que es la diferencia entre el tercer cuartil y el primer cuartil. Podemos calcular el IQR de los datos al restar las columnas 75% y 25% y guardar el resultado en una nueva columna llamada IQR.\n\n\nCódigo\nestadisticas['IQR'] = estadisticas['75%'] - estadisticas['25%']\n\nestadisticas.head(9)\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\nrango\nvarianza\nIQR\n\n\n\n\ncap-diameter\n54035.0\n567.257204\n359.883763\n0.000000\n289.000000\n525.000000\n781.000000\n1891.000000\n1891.000000\n129516.323081\n492.000000\n\n\ncap-shape\n54035.0\n4.000315\n2.160505\n0.000000\n2.000000\n5.000000\n6.000000\n6.000000\n6.000000\n4.667783\n4.000000\n\n\ngill-attachment\n54035.0\n2.142056\n2.228821\n0.000000\n0.000000\n1.000000\n4.000000\n6.000000\n6.000000\n4.967642\n4.000000\n\n\ngill-color\n54035.0\n7.329509\n3.200266\n0.000000\n5.000000\n8.000000\n10.000000\n11.000000\n11.000000\n10.241701\n5.000000\n\n\nstem-height\n54035.0\n0.759110\n0.650969\n0.000426\n0.270997\n0.593295\n1.054858\n3.835320\n3.834894\n0.423760\n0.783861\n\n\nstem-width\n54035.0\n1051.081299\n782.056076\n0.000000\n421.000000\n923.000000\n1523.000000\n3569.000000\n3569.000000\n611611.705997\n1102.000000\n\n\nstem-color\n54035.0\n8.418062\n3.262078\n0.000000\n6.000000\n11.000000\n11.000000\n12.000000\n12.000000\n10.641151\n5.000000\n\n\nseason\n54035.0\n0.952163\n0.305594\n0.027372\n0.888450\n0.943195\n0.943195\n1.804273\n1.776901\n0.093387\n0.054744\n\n\nclass\n54035.0\n0.549181\n0.497580\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.247586\n1.000000\n\n\n\n\n\n\n\n\nPodemos ver que hemos calculado el rango, la varianza y el IQR de los datos. Estos estadísticos nos dan una idea de la dispersión de los datos y nos ayudan a entender mejor los datos.\nOtro estadtico importante es la correlación, que nos indica la relación entre dos variables. Podemos calcular la correlación de los datos con el método corr().\n\n\nCódigo\ncorrelacion = data.corr()\ncorrelacion\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nstem-color\nseason\nclass\n\n\n\n\ncap-diameter\n1.000000\n0.204011\n0.200481\n0.186377\n0.135652\n0.828469\n0.121856\n0.113334\n-0.165676\n\n\ncap-shape\n0.204011\n1.000000\n0.043066\n0.131387\n-0.010393\n0.222494\n0.029035\n0.055442\n-0.133338\n\n\ngill-attachment\n0.200481\n0.043066\n1.000000\n0.100276\n-0.075284\n0.245300\n0.020073\n-0.040315\n-0.052541\n\n\ngill-color\n0.186377\n0.131387\n0.100276\n1.000000\n0.015057\n0.110283\n0.186090\n0.059965\n-0.063947\n\n\nstem-height\n0.135652\n-0.010393\n-0.075284\n0.015057\n1.000000\n0.098095\n0.002624\n-0.000292\n0.183354\n\n\nstem-width\n0.828469\n0.222494\n0.245300\n0.110283\n0.098095\n1.000000\n0.157394\n0.040679\n-0.182856\n\n\nstem-color\n0.121856\n0.029035\n0.020073\n0.186090\n0.002624\n0.157394\n1.000000\n0.010750\n-0.128339\n\n\nseason\n0.113334\n0.055442\n-0.040315\n0.059965\n-0.000292\n0.040679\n0.010750\n1.000000\n-0.082919\n\n\nclass\n-0.165676\n-0.133338\n-0.052541\n-0.063947\n0.183354\n-0.182856\n-0.128339\n-0.082919\n1.000000",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Manipulación de datos con Pandas"
    ]
  },
  {
    "objectID": "statistics/data_manipulation.html#filtrando-datos",
    "href": "statistics/data_manipulation.html#filtrando-datos",
    "title": "Manipulación de datos con Pandas",
    "section": "Filtrando datos",
    "text": "Filtrando datos\nPodemos filtrar los datos de acuerdo a una condición. Por ejemplo, si queremos ver los hongos que son venenosos, podemos filtrar los datos de la siguiente manera:\n\n\nCódigo\nvenenosos = data[data['class'] == 1]\nvenenosos.head()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nstem-color\nseason\nclass\n\n\n\n\n0\n1372\n2\n2\n10\n3.807467\n1545\n11\n1.804273\n1\n\n\n1\n1461\n2\n2\n10\n3.807467\n1557\n11\n1.804273\n1\n\n\n2\n1371\n2\n2\n10\n3.612496\n1566\n11\n1.804273\n1\n\n\n3\n1261\n6\n2\n10\n3.787572\n1566\n11\n1.804273\n1\n\n\n4\n1305\n6\n2\n10\n3.711971\n1464\n11\n0.943195\n1\n\n\n\n\n\n\n\n\nLa notación para realizar filtrado en pandas es df[df['columna'] == valor], donde df es el DataFrame, columna es la columna que queremos filtrar y valor es el valor que queremos filtrar.\nPodemos hacer filtrados más complejos, por ejemplo, si queremos ver los hongos venenosos que tienen un diámetro de sombrero mayor a su media, podemos hacer lo siguiente:\n\n\nCódigo\nvenenosos_grandes = data[(data['class'] == 1) & (data['cap-diameter'] &gt; data['cap-diameter'].mean())]\n\nvenenosos_grandes.head()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nstem-color\nseason\nclass\n\n\n\n\n0\n1372\n2\n2\n10\n3.807467\n1545\n11\n1.804273\n1\n\n\n1\n1461\n2\n2\n10\n3.807467\n1557\n11\n1.804273\n1\n\n\n2\n1371\n2\n2\n10\n3.612496\n1566\n11\n1.804273\n1\n\n\n3\n1261\n6\n2\n10\n3.787572\n1566\n11\n1.804273\n1\n\n\n4\n1305\n6\n2\n10\n3.711971\n1464\n11\n0.943195\n1\n\n\n\n\n\n\n\n\nLa sintaxis en este caso es df[(df['columna1'] == valor1) & (df['columna2'] &gt; valor2)], donde & es el operador lógico AND y cada condición va entre paréntesis. Es recomendable no usar más de dos condiciones en un filtro, ya que puede ser difícil de leer.\nOtras operaciones lógicas que podemos usar son | para el operador OR y ~ para el operador NOT. Por ejemplo, si queremos ver los hongos que son venenosos o que tienen un diámetro de sombrero mayor a su media, podemos hacer lo siguiente:\n\n\nCódigo\nvenenosos_o_grandes = data[(data['class'] == 1) | (data['cap-diameter'] &gt; data['cap-diameter'].mean())]\n\nvenenosos_o_grandes.head()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nstem-color\nseason\nclass\n\n\n\n\n0\n1372\n2\n2\n10\n3.807467\n1545\n11\n1.804273\n1\n\n\n1\n1461\n2\n2\n10\n3.807467\n1557\n11\n1.804273\n1\n\n\n2\n1371\n2\n2\n10\n3.612496\n1566\n11\n1.804273\n1\n\n\n3\n1261\n6\n2\n10\n3.787572\n1566\n11\n1.804273\n1\n\n\n4\n1305\n6\n2\n10\n3.711971\n1464\n11\n0.943195\n1",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Manipulación de datos con Pandas"
    ]
  },
  {
    "objectID": "statistics/data_manipulation.html#ordenando-datos",
    "href": "statistics/data_manipulation.html#ordenando-datos",
    "title": "Manipulación de datos con Pandas",
    "section": "Ordenando datos",
    "text": "Ordenando datos\nPodemos ordenar los datos de acuerdo a una columna. Por ejemplo, si queremos ordenar los hongos por su diámetro de sombrero de manera ascendente, podemos hacer lo siguiente:\n\n\nCódigo\ndata.sort_values(by='cap-diameter').head()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nstem-color\nseason\nclass\n\n\n\n\n11696\n0\n6\n0\n11\n1.019047\n21\n12\n0.943195\n1\n\n\n12214\n1\n2\n1\n6\n0.895698\n42\n7\n0.943195\n1\n\n\n11814\n1\n6\n0\n11\n1.198101\n1\n11\n0.888450\n1\n\n\n12206\n2\n2\n1\n6\n1.058837\n31\n7\n0.943195\n1\n\n\n35395\n2\n0\n0\n3\n1.452757\n174\n6\n0.943195\n1\n\n\n\n\n\n\n\n\nHemos utilizado el método sort_values() para ordenar los datos de acuerdo a la columna cap-diameter. Por defecto, el método ordena de manera ascendente, pero podemos cambiarlo a descendente con el argumento ascending=False. El argumento by es la columna por la que queremos ordenar los datos. Si queremos ordenar los datos de manera descendente, podemos hacer lo siguiente:\n\n\nCódigo\ndata.sort_values(by='cap-diameter', ascending=False).head()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nstem-color\nseason\nclass\n\n\n\n\n45984\n1891\n4\n4\n10\n3.413547\n3164\n11\n0.943195\n0\n\n\n26230\n1890\n2\n1\n5\n0.357683\n3149\n11\n1.804273\n1\n\n\n23968\n1890\n2\n0\n4\n0.839141\n2268\n11\n1.804273\n0\n\n\n26229\n1890\n6\n1\n10\n0.878931\n2774\n11\n0.943195\n1\n\n\n46039\n1890\n6\n4\n10\n3.628412\n3338\n11\n0.943195\n0\n\n\n\n\n\n\n\n\nPara ordenar con múltiples columnas, podemos pasar una lista de columnas al argumento by. Por ejemplo, si queremos ordenar los hongos por su diámetro de sombrero de manera ascendente y por su altura de tallo de manera descendente, podemos hacer lo siguiente:\n\n\nCódigo\ndata.sort_values(by=['cap-diameter', 'stem-height'], ascending=[True, False], kind=\"mergesort\").head(10)\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nstem-color\nseason\nclass\n\n\n\n\n11696\n0\n6\n0\n11\n1.019047\n21\n12\n0.943195\n1\n\n\n11814\n1\n6\n0\n11\n1.198101\n1\n11\n0.888450\n1\n\n\n12214\n1\n2\n1\n6\n0.895698\n42\n7\n0.943195\n1\n\n\n35395\n2\n0\n0\n3\n1.452757\n174\n6\n0.943195\n1\n\n\n12206\n2\n2\n1\n6\n1.058837\n31\n7\n0.943195\n1\n\n\n12207\n2\n6\n1\n6\n0.752455\n40\n12\n0.943195\n1\n\n\n11819\n3\n6\n0\n11\n1.142396\n10\n12\n0.888450\n1\n\n\n11701\n3\n6\n0\n11\n1.098627\n22\n11\n0.888450\n1\n\n\n11738\n4\n6\n0\n11\n1.023026\n23\n12\n0.943195\n1\n\n\n12274\n4\n6\n1\n6\n0.983236\n34\n7\n0.888450\n1\n\n\n\n\n\n\n\n\nCon el argumento kind podemos especificar el algoritmo de ordenamiento. En este caso hemos usado mergesort, que es un algoritmo de ordenamiento estable y eficiente. Otros algoritmos que podemos usar son quicksort y heapsort.\nEl ordenamiento con más de una columna es útil cuando queremos ordenar los datos de acuerdo a una columna y en caso de empate, ordenarlos de acuerdo a otra columna. En este caso, primero ordenamos los datos de acuerdo a la columna cap-diameter y en caso de empate, ordenamos los datos de acuerdo a la columna stem-height. Podemos comprobarlo viendo los primeros 10 datos.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Manipulación de datos con Pandas"
    ]
  },
  {
    "objectID": "statistics/data_manipulation.html#agrupando-datos",
    "href": "statistics/data_manipulation.html#agrupando-datos",
    "title": "Manipulación de datos con Pandas",
    "section": "Agrupando datos",
    "text": "Agrupando datos\nPodemos agrupar los datos de acuerdo a una columna. Por ejemplo, si queremos agrupar los hongos por su color de tallo y ver cuántos hongos hay de cada color, podemos hacer lo siguiente:\n\n\nCódigo\ndata.groupby('stem-color').size()\n\n\nstem-color\n0       173\n1      1918\n2      1059\n3      2626\n4       576\n5       226\n6     15316\n7      1848\n8      1025\n9       541\n10     1393\n11    20317\n12     7017\ndtype: int64\n\n\nEl método groupby() agrupa los datos de acuerdo a una columna y el método size() nos da el tamaño de cada grupo. En este caso, hemos agrupado los hongos por su color de tallo y hemos obtenido el tamaño de cada grupo.\nOtro método que podemos usar es count(), que nos da el número de valores no nulos de cada columna.\n\n\nCódigo\ndata.groupby('stem-color').count()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nseason\nclass\n\n\nstem-color\n\n\n\n\n\n\n\n\n\n\n\n\n0\n173\n173\n173\n173\n173\n173\n173\n173\n\n\n1\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n\n\n2\n1059\n1059\n1059\n1059\n1059\n1059\n1059\n1059\n\n\n3\n2626\n2626\n2626\n2626\n2626\n2626\n2626\n2626\n\n\n4\n576\n576\n576\n576\n576\n576\n576\n576\n\n\n5\n226\n226\n226\n226\n226\n226\n226\n226\n\n\n6\n15316\n15316\n15316\n15316\n15316\n15316\n15316\n15316\n\n\n7\n1848\n1848\n1848\n1848\n1848\n1848\n1848\n1848\n\n\n8\n1025\n1025\n1025\n1025\n1025\n1025\n1025\n1025\n\n\n9\n541\n541\n541\n541\n541\n541\n541\n541\n\n\n10\n1393\n1393\n1393\n1393\n1393\n1393\n1393\n1393\n\n\n11\n20317\n20317\n20317\n20317\n20317\n20317\n20317\n20317\n\n\n12\n7017\n7017\n7017\n7017\n7017\n7017\n7017\n7017\n\n\n\n\n\n\n\n\nLa diferencia es que size() nos da el tamaño de cada grupo, mientras que count() nos da el número de valores no nulos de cada columna.\nPara hacer operaciones más complejas, podemos usar el método agg(), que nos permite aplicar una o más funciones a cada grupo. Por ejemplo, si queremos obtener la media y la mediana del diámetro de sombrero de cada grupo, podemos hacer lo siguiente:\n\n\nCódigo\ndata.groupby('stem-color')['cap-diameter'].agg(['mean', 'median'])\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\n\n\nstem-color\n\n\n\n\n\n\n0\n957.127168\n962.0\n\n\n1\n498.565172\n458.5\n\n\n2\n338.590179\n331.0\n\n\n3\n454.637091\n395.0\n\n\n4\n385.489583\n38.0\n\n\n5\n377.615044\n384.0\n\n\n6\n544.769130\n518.0\n\n\n7\n375.433442\n377.0\n\n\n8\n726.983415\n756.0\n\n\n9\n632.057301\n431.0\n\n\n10\n685.408471\n765.0\n\n\n11\n659.824531\n637.0\n\n\n12\n453.907653\n438.0\n\n\n\n\n\n\n\n\nEn este caso, hemos agrupado los hongos por su color de tallo y hemos obtenido la media y la mediana del diámetro de sombrero de cada grupo.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Manipulación de datos con Pandas"
    ]
  },
  {
    "objectID": "statistics/data_manipulation.html#guardando-datos",
    "href": "statistics/data_manipulation.html#guardando-datos",
    "title": "Manipulación de datos con Pandas",
    "section": "Guardando datos",
    "text": "Guardando datos\nPodemos guardar los datos en un archivo CSV con el método to_csv(). Por ejemplo, si queremos guardar los datos en un archivo llamado mushroom.csv, podemos hacer lo siguiente:\n\n\nCódigo\n#data.to_csv('mushroom.csv', index=False)\n\n\nEl argumento index=False evita que se guarde el índice en el archivo CSV. Existen otros formatos en los que podemos guardar los datos, como Excel, JSON, entre otros. Para guardar los datos en un archivo Excel, podemos usar el método to_excel().\n\n\nCódigo\n#data.to_excel('mushroom.xlsx', index=False)\n\n\nPara guardar los datos en un archivo JSON, podemos usar el método to_json().\n\n\nCódigo\n#data.to_json('mushroom.json', orient='records')",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Manipulación de datos con Pandas"
    ]
  },
  {
    "objectID": "statistics/data_manipulation.html#ejercicios",
    "href": "statistics/data_manipulation.html#ejercicios",
    "title": "Manipulación de datos con Pandas",
    "section": "Ejercicios",
    "text": "Ejercicios\nCreen un documento de Jupyter Notebook por equipo y resuelvan los siguientes ejercicios, utilicen Markdown para explicar sus respuestas y comentarios en el código. Se sugiere la creación de funciones para resolver los ejercicios.\n\nCarga los datos de Iris desde la siguiente liga “https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv” y muestra las primeras 10 filas, muestra la forma de los datos, los nombres de las columnas, los tipos de datos, el tamaño de los datos y si hay valores nulos (NA).\nCalcula las estadísticas descriptivas de los datos.\nFiltra los datos para obtener las flores que tienen un ancho de pétalo mayor a su media, mediana, media y mediana. Impreme las primeras 10 filas.\nOrdena los datos de acuerdo a la longitud de sépalo de manera ascendente y de acuerdo a la longitud de pétalo de manera descendente. Muestra las primeras 10 filas.\nFiltra los datos para obtener las flores que tienen un ancho de pétalo mayor a su media y que son de la especie setosa. Muestra las primeras 10 filas.\nAgrupa los datos por especie y calcula los estadísticos descriptivos de cada grupo (media, mediana, moda, varianza, rango, IQR). Muestra los resultados.\nGuarda los datos de los estadísticos descriptivos del ejercicio 2 en un archivo CSV, Excel y JSON.\nCalcula la correlación de los datos y muestra los resultados.\nSugiere que gráficos podrías hacer con los datos y que información podrías obtener de ellos.\nExplica ¿qué análisis podrías hacer con los datos?, ¿qué preguntas podrías responder con los datos?",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Manipulación de datos con Pandas"
    ]
  },
  {
    "objectID": "neural_neworks/neural_networks.html",
    "href": "neural_neworks/neural_networks.html",
    "title": "Introducción a Redes Neuronales",
    "section": "",
    "text": "Las redes neuronales artificiales son un modelo computacional inspirado en el cerebro humano. Están compuestas por nodos llamados neuronas que están conectados entre sí. Cada conexión entre neuronas tiene un peso asociado que se ajusta durante el entrenamiento del modelo. Estos pesos son los parámetros que se ajustan para que el modelo pueda realizar predicciones, es decir, son la memoria del modelo y representan la importancia de cada conexión.\nLas redes neuronales artificiales se dividen en capas, cada capa está compuesta por un conjunto de neuronas. La primera capa se llama capa de entrada, la última capa se llama capa de salida y las capas intermedias se llaman capas ocultas. La capa de entrada recibe los datos de entrada, la capa de salida produce la predicción y las capas ocultas procesan la información.\n\n\n\nRed Neuronal\n\n\n\n\nCada neurona recibe una serie de entradas, las multiplica por los pesos asociados a cada conexión y aplica una función de activación. La función de activación es una función no lineal que se encarga de introducir no linealidades en el modelo.\nMatemáticamente, el comportamiento de una neurona se puede expresar de la siguiente forma:\n\\[y = f(\\sum_{i=1}^{n} x_i \\cdot w_i + b)\\]\nDonde \\(x_i\\) son las entradas, \\(w_i\\) son los pesos asociados a cada conexión, \\(b\\) es el sesgo y \\(f\\) es la función de activación. Si usaramos una función de activación lineal, la red neuronal sería equivalente a un modelo de regresión lineal.\nEntonces podemos usar una red neuronal para regresión lineal:\n\\[\\begin{align*}\ny &= f(\\sum_{i=1}^{n} x_i \\cdot w_i + b) \\hspace{1cm} \\text{Donde } f(x) = x \\\\\ny &= \\sum_{i=1}^{n} x_i \\cdot w_i + b\n\\end{align*}\\]\nExisten diversas funciones de activación, algunas de las más comunes son:\n\nFunción Sigmoide. \\[f(x) = \\frac{1}{1 + e^{-x}}\\]\nFunción ReLU. \\[f(x) = \\max(0, x)\\]\nFunción Tangente Hiperbólica. \\[f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\]\nFunción Softmax. \\[f(x) = \\frac{e^x}{\\sum_{i=1}^{n} e^{x_i}}\\]\nFunción de Identidad. \\[f(x) = x\\]\n\nCada función de activación tiene sus propias características y se utiliza en diferentes contextos. Por ejemplo, la función sigmoide se utiliza en la capa de salida de una red neuronal para clasificación binaria, la función ReLU se utiliza en las capas ocultas y la función softmax se utiliza en la capa de salida para clasificación multiclase.\nVisualicemos el comportamiento de algunas funciones de activación:\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 4), sharex=True)\n\nx = np.linspace(-5, 5, 100)\n\n# Funciones de activación\nsigmoid = 1 / (1 + np.exp(-x))\nrelu = np.maximum(0, x)\ntanh = np.tanh(x)\nsoftmax = np.exp(x) / np.sum(np.exp(x))\n\n# Gráficas\nax[0, 0].plot(x, sigmoid)\nax[0, 0].set_title(\"Función Sigmoide\")\n\nax[0, 1].plot(x, relu)\nax[0, 1].set_title(\"Función ReLU\")\n\nax[1, 0].plot(x, tanh)\nax[1, 0].set_title(\"Función Tangente Hiperbólica\")\n\nax[1, 1].plot(x, softmax)\nax[1, 1].set_title(\"Función Softmax\")\n\nfor i in range(2):\n    for j in range(2):\n        ax[i, j].grid( linestyle='--', linewidth=0.5, alpha=0.5, color='grey')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nHagamos lo que hace una neurona con una función de activación sigmoide.\n\n\nCódigo\n# Función de activación sigmoide\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Datos de entrada\nX = np.random.randn(10, 1)\n\n# Pesos y sesgo\nnp.random.seed(1014)\nweights = np.random.randn(1, 10)\nbias = np.random.randn(1)\n\n# Salida de la neurona\ny = sigmoid(np.dot(X.T, weights.T) + bias)\n\nprint(y)\n\n\n[[0.20899694]]\n\n\nEstamos realizando la siguiente operación:\n\\[y = f(\\sum_{i=1}^{n} x_i \\cdot w_i + b)\\]\nDonde \\(f(x) = \\frac{1}{1 + e^{-x}}\\) es la función sigmoide. En este caso, estamos utilizando una neurona con 10 entradas y una salida.\nLo que busca simular o modelar el comportamiento de una neurona biológica. La neurona biológica recibe señales eléctricas de otras neuronas a través de las dendritas, las procesa en el cuerpo celular y envía una señal eléctrica a través del axón. La señal eléctrica se transmite a través de las sinapsis, que son las conexiones entre las neuronas.\nAhora veamos cómo se comporta una red neuronal con una capa oculta y una capa de salida. Para esto, vamos a implementar una red neuronal para regresión lineal con pesos y sesgos aleatorios.\n\n\nCódigo\nimport numpy as np\n\nclass NeuralNetwork():\n    def __init__(self, input_size, hidden_size, output_size, seed=1014):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        # Pesos y sesgos\n        np.random.seed(seed)\n        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n        self.bias_input_hidden = np.random.randn(hidden_size)\n        \n        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n        self.bias_hidden_output = np.random.randn(output_size)\n        \n    def forward(self, X, activation):\n        # Capa oculta\n        hidden = np.dot(X, self.weights_input_hidden) + self.bias_input_hidden\n        hidden = activation(hidden)\n        \n        # Capa de salida\n        output = np.dot(hidden, self.weights_hidden_output) + self.bias_hidden_output\n        \n        return output\n\n\nUsemos la red neuronal para predecir un conjunto de datos y con una función de activación identidad.\n\n\nCódigo\n# Datos de entrada\nX = np.random.randn(10, 1)\n\n# Parámetros de la red neuronal\ninput_size = 1\nhidden_size = 10\noutput_size = 1\n\n# Red Neuronal\nnn = NeuralNetwork(input_size, hidden_size, output_size)\n\n# Función de activación identidad\ndef identity(x):\n    return x\n\n# Predicciones\ny_pred = nn.forward(X, identity)\n\nprint(y_pred)\n\n\n[[-1.81046445]\n [-1.98312314]\n [-1.86441218]\n [-2.03739283]\n [-1.77027647]\n [-1.92937854]\n [-1.87128124]\n [-1.94289359]\n [-2.20644573]\n [-1.93053922]]\n\n\nEn este caso tenemos una red neuronal con una capa oculta de 10 neuronas y una capa de salida de 1 neurona. La función de activación de la capa oculta es la función identidad y la función de activación de la capa de salida también es la función identidad.\nLos pesos de la red son:\n\n\nCódigo\nprint(f\"Pesos capa oculta: {nn.weights_input_hidden}\\n\")\nprint(f\"Sesgos capa oculta: {nn.bias_input_hidden}\\n\")\nprint(f\"Pesos capa de salida: {nn.weights_hidden_output.T}\\n\")\nprint(f\"Sesgos capa de salida: {nn.bias_hidden_output}\\n\")\n\n\nPesos capa oculta: [[ 0.75943278 -1.00725987 -0.64499024 -0.26674068  0.29125552  0.14820587\n   0.6382988   0.46738854  0.53122954  1.1840206 ]]\n\nSesgos capa oculta: [-1.25173295 -1.30489407  0.20194032 -0.83407915  0.67556507 -1.65562438\n -0.26710189 -0.77413114 -0.14915279  2.15093091]\n\nPesos capa de salida: [[-0.25697236  0.50673502 -1.31687341  1.71747235 -0.12242967 -0.06419002\n   0.47479916 -0.01225672  1.10530347 -0.54019387]]\n\nSesgos capa de salida: [1.4985733]\n\n\n\nPodemos dibujar la red neuronal con los pesos y sesgos asociados a cada conexión.\n\n\nCódigo\nimport itertools\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n# Colores para las capas\nsubset_colors = ['blue', 'red', 'green']\n\ndef multilayered_graph(input_size, hidden_size, output_size, weights_input_hidden, weights_hidden_output):\n    # Crear los rangos para las capas\n    subset_sizes = [input_size, hidden_size, output_size]\n    extents = nx.utils.pairwise(itertools.accumulate((0,) + tuple(subset_sizes)))\n    layers = [range(start, end) for start, end in extents]\n    \n    # Crear el gráfico\n    G = nx.Graph()\n    for i, layer in enumerate(layers):\n        G.add_nodes_from(layer, layer=i)\n        \n    # Añadir los bordes con pesos para capa de entrada a oculta\n    for i, j in itertools.product(layers[0], layers[1]):\n        G.add_edge(i, j, weight=round(weights_input_hidden[i, j - layers[1][0]], 3))\n        \n    # Añadir los bordes con pesos para capa oculta a salida\n    for i, j in itertools.product(layers[1], layers[2]):\n        G.add_edge(i, j, weight=round(weights_hidden_output[i - layers[1][0], j - layers[2][0]], 3))\n    \n    return G\n\n# Crear el gráfico con los pesos\nG = multilayered_graph(input_size, hidden_size, output_size, nn.weights_input_hidden, nn.weights_hidden_output)\n\n# Colores para los nodos según su capa\ncolor = [subset_colors[data[\"layer\"]] for node, data in G.nodes(data=True)]\n\n# Posición de los nodos\npos = nx.multipartite_layout(G, subset_key=\"layer\")\n\n# Dibujar el gráfico\nplt.figure(figsize=(10, 6))\nnx.draw(G, pos, with_labels=False, node_color=color, node_size=1500, font_size=10, font_weight='bold')\n\n# Dibujar los bordes con los pesos\nedge_labels = nx.get_edge_attributes(G, 'weight')\nnx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCada una de las neuronas realiza la operación que hemos visto anteriormente. Sin embargo, aquí solo hemos decidido los pesos y sesgos de la red neuronal de forma aleatoria. En la práctica, estos pesos y sesgos se ajustan durante el entrenamiento de la red neuronal, es decir, la red neuronal aprende a partir de los datos.\n\n\n\nEl entrenamiento de una red neuronal consiste en ajustar los pesos y sesgos de la red para minimizar una función de pérdida. La función de pérdida mide la diferencia entre las predicciones del modelo y los valores reales. Durante el entrenamiento, los pesos y sesgos se ajustan iterativamente utilizando un algoritmo de optimización.\nExisten diversos algoritmos de optimización, algunos de los más comunes son:\n\nDescenso del Gradiente : Actualiza los pesos en la dirección opuesta al gradiente de la función de pérdida.\nAdam: Utiliza una combinación de descenso del gradiente y adaptación de la tasa de aprendizaje.\nRMSprop: Se adapta a la tasa de aprendizaje para cada parámetro.\nAdagrad: Ajusta la tasa de aprendizaje para cada parámetro en función de la magnitud de los gradientes.\n\nEl algoritmo tipico es el descenso del gradiente. La idea es ajustar los pesos y sesgos de la red neuronal en la dirección opuesta al gradiente de la función de pérdida. El gradiente de la función de pérdida se calcula utilizando la regla de la cadena y el algoritmo de retropropagación. Pero en la práctica, se utiliza una variante del descenso del gradiente llamada descenso del gradiente estocástico o el algoritmo Adam.\nAlgunas funciones de pérdida comunes son:\n\nError Cuadrático Medio. \\[L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nEntropía Cruzada. \\[L(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\\]\nError Absoluto Medio. \\[L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\]\n\nEl algoritmo de entrenamiento de una red neuronal se puede resumir en los siguientes pasos:\n\nInicializar los pesos y sesgos de la red neuronal.\nCalcular la salida de la red neuronal.\nCalcular la función de pérdida.\nCalcular el gradiente de la función de pérdida.\nActualizar los pesos y sesgos utilizando un algoritmo de optimización.\nRepetir los pasos 2-5 hasta que se alcance un número de iteraciones o se cumpla un criterio de parada.\n\n\n\nVamos a ver cómo se actualizan los pesos y sesgos de una neurona durante el entrenamiento. Para esto, vamos a implementar una neurona con una función de activación sigmoide y vamos a entrenar la neurona para realizar una regresión lineal y usar la función de pérdida de error cuadrático medio.\nPara obtener el gradiente de la función de pérdida, vamos a utilizar la regla de la cadena y el algoritmo de retropropagación. La regla de la cadena se utiliza para calcular el gradiente de una función compuesta y el algoritmo de retropropagación se utiliza para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal.\n\n\nLa regla de la cadena se utiliza para calcular el gradiente de una función compuesta. Si tenemos una función \\(f(g(x))\\), el gradiente de \\(f\\) con respecto a \\(x\\) se puede calcular como:\n\\[\\frac{\\partial f(g(x))}{\\partial x} = \\frac{\\partial f(g(x))}{\\partial g(x)} \\cdot \\frac{\\partial g(x)}{\\partial x}\\]\nEn nuestro caso tenemos una función de pérdida \\(L(y, \\hat{y})\\) y una función de activación \\(f(x)\\). Entonces, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal se puede calcular como:\n\\[\\frac{\\partial L(y, \\hat{y})}{\\partial w_i} = \\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i}\\]\nDonde \\(z = \\sum_{i=1}^{n} x_i \\cdot w_i + b\\) es la entrada de la neurona y \\(\\hat{y} = f(z)\\) es la salida de la neurona.\nHagamos la primera parte de la regla de la cadena, es decir, el gradiente de la función de pérdida con respecto a la salida de la neurona.\n\\[\\begin{align*}\n\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} &= \\frac{\\partial}{\\partial \\hat{y}} \\left( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\right) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\hat{y}} (y_i - \\hat{y}_i)^2 \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} -2(y_i - \\hat{y}_i) \\\\\n&= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i)\n\\end{align*}\\]\nLa segunda parte de la regla de la cadena es el gradiente de la salida de la neurona con respecto a la entrada de la neurona.\n\\[\\begin{align*}\n\\frac{\\partial \\hat{y}}{\\partial z} &= \\frac{\\partial}{\\partial z} \\left( \\frac{1}{1 + e^{-z}} \\right) \\\\\n&= \\frac{e^{-z}}{(1 + e^{-z})^2} \\\\\n&= \\frac{1}{1 + e^{-z}} \\cdot \\left(1 - \\frac{1}{1 + e^{-z}} \\right) \\\\\n&= \\hat{y} \\cdot (1 - \\hat{y})\n\\end{align*}\\]\nLa tercera parte de la regla de la cadena es el gradiente de la entrada de la neurona con respecto a los pesos.\n\\[\\begin{align*}\n\\frac{\\partial z}{\\partial w_i} &= \\frac{\\partial}{\\partial w_i} \\left( \\sum_{i=1}^{n} x_i \\cdot w_i + b \\right) \\\\\n&= x_i\n\\end{align*}\\]\nAhora para el sesgo.\n\\[\\begin{align*}\n\\frac{\\partial z}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left( \\sum_{i=1}^{n} x_i \\cdot w_i + b \\right) \\\\\n&= 1\n\\end{align*}\\]\nEntonces, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal se puede calcular como:\n\\[\\begin{align*}\n\\frac{\\partial L(y, \\hat{y})}{\\partial w_i} &= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i) \\cdot \\hat{y}_i \\cdot (1 - \\hat{y}_i) \\cdot x_i \\\\\n\\frac{\\partial L(y, \\hat{y})}{\\partial b} &= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i) \\cdot \\hat{y}_i \\cdot (1 - \\hat{y}_i)\n\\end{align*}\\]\n\n\nPara actualizar los pesos y sesgos de la red neuronal, utilizamos el algoritmo de descenso del gradiente. La actualización de los pesos y sesgos se realiza de la siguiente forma:\n\\[\\begin{align*}\nw_i &:= w_i - \\alpha \\cdot \\frac{\\partial L(y, \\hat{y})}{\\partial w_i} \\\\\nb &:= b - \\alpha \\cdot \\frac{\\partial L(y, \\hat{y})}{\\partial b}\n\\end{align*}\\]\nDonde \\(\\alpha\\) es la tasa de aprendizaje, que es un hiperparámetro del modelo. La tasa de aprendizaje controla la magnitud de la actualización de los pesos y sesgos. Si la tasa de aprendizaje es muy pequeña, el modelo puede tardar mucho tiempo en converger. Si la tasa de aprendizaje es muy grande, el modelo puede divergir.\n\n\n\n\nLa retropropagación es un algoritmo que se utiliza para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal, usamos la regla de la cadena para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal y lo propagamos hacia atrás a través de la red neuronal.\nPara cada capa de la red neuronal, calculamos el gradiente de la función de pérdida con respecto a los pesos y sesgos de la capa utilizando la regla de la cadena y el gradiente de la capa anterior. Luego, actualizamos los pesos y sesgos de la capa. Este proceso se repite para todas las capas de la red neuronal. Un hermoso gif creado por Michael Pyrcz muestra cómo funciona la retropropagación.\n\n\n\nRetropropagación del Gradiente\n\n\n\n\n\n\nHagamos una red neuronal con 1 neurona en la capa oculta y 5 neuronas en la capa de salida, usaremos la función de activación sigmoide para la capa oculta y la función de activación identidad para la capa de salida. Vamos a entrenar la red neuronal para realizar una regresión lineal y usaremos la función de pérdida de error cuadrático medio.\n\n\nSimulemos datos de entrada y salida para entrenar la red neuronal.\n\n\nCódigo\nimport numpy as np\nimport pandas as pd\n\n# Datos de entrada\nX = np.random.normal(0, 5, (40, 1))\n\n# Datos de salida\ny = 2 * X + 3 + np.random.normal(0, 1, (40, 1))\n\ndf = pd.DataFrame(np.concatenate([X, y], axis=1), columns=[\"X\", \"y\"])\ndf.head()\n\n\n\n\n\n\n\n\n\n\nX\ny\n\n\n\n\n0\n1.705603\n5.973504\n\n\n1\n-6.149390\n-8.670173\n\n\n2\n5.814690\n13.924261\n\n\n3\n2.384975\n6.429876\n\n\n4\n0.502252\n2.931682\n\n\n\n\n\n\n\n\n\n\n\nVamos a implementar algunas funciones auxiliares para la red neuronal.\n\n\nCódigo\n# Función de activación sigmoide\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Función de pérdida de error cuadrático medio\ndef mse(y, y_pred):\n    return np.mean((y - y_pred) ** 2)\n\ndef weight_derivative_hidden(X, y, y_pred):\n    \"\"\"\n    Derivada de los pesos de la capa oculta\n    \"\"\"\n    return -2 * np.mean((y - y_pred) * y_pred * (1 - y_pred) * X)\n\ndef bias_derivative_hidden(y, y_pred):\n    \"\"\"\n    Derivada del sesgo de la capa oculta\n    \"\"\"\n    return -2 * np.mean((y - y_pred) * y_pred * (1 - y_pred))\n\ndef weight_derivative_output(hidden, y, y_pred):\n    \"\"\"\n    Derivada de los pesos de la capa de salida para la función de pérdida de error cuadrático medio y función de activación identidad\n    \"\"\"\n    return -2 * np.mean((y - y_pred) * X)\n\ndef bias_derivative_output(y, y_pred):\n    \"\"\"\n    Derivada del sesgo de la capa de salida para la función de pérdida de error cuadrático medio y función de activación identidad\n    \"\"\"\n    return -2 * np.mean(y - y_pred)\n\n# Inicialización de los pesos y sesgos\ndef initialize_weights(input_size, hidden_size, output_size, seed=1014):\n    np.random.seed(seed)\n    weights_input_hidden = np.random.randn(input_size, hidden_size)\n    bias_input_hidden = np.random.randn(hidden_size)\n    \n    weights_hidden_output = np.random.randn(hidden_size, output_size)\n    bias_hidden_output = np.random.randn(output_size)\n    \n    return weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output\n\n\n\n\n\nVamos a entrenar la red neuronal utilizando el algoritmo de retropropagación. Durante el entrenamiento, vamos a calcular la función de pérdida, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal y vamos a actualizar los pesos y sesgos utilizando el algoritmo de descenso del gradiente.\n\n\nCódigo\ndef learning( X, y, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, learning_rate=0.01):\n    # Capa oculta\n    hidden = np.dot(X, weights_input_hidden) + bias_input_hidden\n    hidden = sigmoid(hidden)\n    \n    # Capa de salida\n    output = np.dot(hidden, weights_hidden_output) + bias_hidden_output\n    \n    # Función de pérdida\n    loss = mse(y, output)\n    \n    # Gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal\n    weight_der_hidden = weight_derivative_hidden(X, y, output)\n    bias_der_hidden = bias_derivative_hidden(y, output)\n    \n    weight_der_output = weight_derivative_output(hidden, y, output)\n    bias_der_output = bias_derivative_output(y, output)\n    \n    # Actualización de los pesos y sesgos\n    weights_input_hidden -= learning_rate * weight_der_hidden\n    bias_input_hidden -= learning_rate * bias_der_hidden\n    \n    weights_hidden_output -= learning_rate * weight_der_output\n    bias_hidden_output -= learning_rate * bias_der_output\n    \n    return weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, loss\n\n# Creamos un bucle para entrenar la red neuronal\n\ninput_size = 1\nhidden_size = 5\noutput_size = 1\n\nweights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output = initialize_weights(input_size, hidden_size, output_size)\n\nlearning_rate = 0.0001\nepochs = 1500\n\nX_normalized = (X - X.mean()) / X.std()\nY_normalized = (y - y.mean()) / y.std()\n\nfor epoch in range(epochs):\n    weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, loss = learning(\n    X_normalized, Y_normalized, \n    weights_input_hidden, \n    bias_input_hidden, \n    weights_hidden_output, \n    bias_hidden_output, \n    learning_rate)\n    \n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}: Loss {loss}\")\n    elif epoch == epochs - 1:\n        print(f\"Epoch {epoch}: Loss {loss}\")\n\n\nEpoch 0: Loss 9.962358063298385\nEpoch 100: Loss 6.389020521754107\nEpoch 200: Loss 4.640115312713357\nEpoch 300: Loss 3.593583112789311\nEpoch 400: Loss 2.893864014205564\nEpoch 500: Loss 2.4040869381649284\nEpoch 600: Loss 2.0676644655384124\nEpoch 700: Loss 1.8628933379595423\nEpoch 800: Loss 1.7838514284596045\nEpoch 900: Loss 1.830931639664496\nEpoch 1000: Loss 2.0075290899566314\nEpoch 1100: Loss 2.3237647638230507\nEpoch 1200: Loss 2.8109028707617965\nEpoch 1300: Loss 3.559636339546768\nEpoch 1400: Loss 4.8448697380727035\nEpoch 1499: Loss 7.692389417773325\n\n\n\n\n\n\nUna ves que hemos entrenado la red neuronal, podemos hacer predicciones con la red neuronal que es el objetivo de crear un modelo de aprendizaje automático.\n\n\nCódigo\ndef predict(X, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output):\n    hidden = np.dot(X, weights_input_hidden) + bias_input_hidden\n    hidden = sigmoid(hidden)\n    \n    output = np.dot(hidden, weights_hidden_output) + bias_hidden_output\n    \n    return output\n\ny_pred = predict(X_normalized, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output)\n\ndf_normalized = pd.DataFrame(np.concatenate([X_normalized, Y_normalized, y_pred], axis=1), columns=[\"X\", \"y\", \"y_pred\"])\n\ndf_normalized.head()\n\n\n\n\n\n\n\n\n\n\nX\ny\ny_pred\n\n\n\n\n0\n0.223278\n0.172289\n1.025840\n\n\n1\n-1.426558\n-1.375204\n3.560325\n\n\n2\n1.086337\n1.012498\n0.068435\n\n\n3\n0.365971\n0.220517\n0.829005\n\n\n4\n-0.029469\n-0.149160\n1.409019\n\n\n\n\n\n\n\n\nA nivel profesional, se utilizan librerías como TensorFlow, PyTorch o Keras para implementar redes neuronales. Estas librerías proporcionan una interfaz de alto nivel para construir y entrenar redes neuronales de forma eficiente. Sin embargo, es importante entender cómo funcionan las redes neuronales a nivel de bajo nivel para poder depurar y optimizar los modelos.",
    "crumbs": [
      "Redes Neuronales",
      "Introducción a Redes Neuronales"
    ]
  },
  {
    "objectID": "neural_neworks/neural_networks.html#comportamiento-de-una-neurona",
    "href": "neural_neworks/neural_networks.html#comportamiento-de-una-neurona",
    "title": "Introducción a Redes Neuronales",
    "section": "",
    "text": "Cada neurona recibe una serie de entradas, las multiplica por los pesos asociados a cada conexión y aplica una función de activación. La función de activación es una función no lineal que se encarga de introducir no linealidades en el modelo.\nMatemáticamente, el comportamiento de una neurona se puede expresar de la siguiente forma:\n\\[y = f(\\sum_{i=1}^{n} x_i \\cdot w_i + b)\\]\nDonde \\(x_i\\) son las entradas, \\(w_i\\) son los pesos asociados a cada conexión, \\(b\\) es el sesgo y \\(f\\) es la función de activación. Si usaramos una función de activación lineal, la red neuronal sería equivalente a un modelo de regresión lineal.\nEntonces podemos usar una red neuronal para regresión lineal:\n\\[\\begin{align*}\ny &= f(\\sum_{i=1}^{n} x_i \\cdot w_i + b) \\hspace{1cm} \\text{Donde } f(x) = x \\\\\ny &= \\sum_{i=1}^{n} x_i \\cdot w_i + b\n\\end{align*}\\]\nExisten diversas funciones de activación, algunas de las más comunes son:\n\nFunción Sigmoide. \\[f(x) = \\frac{1}{1 + e^{-x}}\\]\nFunción ReLU. \\[f(x) = \\max(0, x)\\]\nFunción Tangente Hiperbólica. \\[f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\]\nFunción Softmax. \\[f(x) = \\frac{e^x}{\\sum_{i=1}^{n} e^{x_i}}\\]\nFunción de Identidad. \\[f(x) = x\\]\n\nCada función de activación tiene sus propias características y se utiliza en diferentes contextos. Por ejemplo, la función sigmoide se utiliza en la capa de salida de una red neuronal para clasificación binaria, la función ReLU se utiliza en las capas ocultas y la función softmax se utiliza en la capa de salida para clasificación multiclase.\nVisualicemos el comportamiento de algunas funciones de activación:\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 4), sharex=True)\n\nx = np.linspace(-5, 5, 100)\n\n# Funciones de activación\nsigmoid = 1 / (1 + np.exp(-x))\nrelu = np.maximum(0, x)\ntanh = np.tanh(x)\nsoftmax = np.exp(x) / np.sum(np.exp(x))\n\n# Gráficas\nax[0, 0].plot(x, sigmoid)\nax[0, 0].set_title(\"Función Sigmoide\")\n\nax[0, 1].plot(x, relu)\nax[0, 1].set_title(\"Función ReLU\")\n\nax[1, 0].plot(x, tanh)\nax[1, 0].set_title(\"Función Tangente Hiperbólica\")\n\nax[1, 1].plot(x, softmax)\nax[1, 1].set_title(\"Función Softmax\")\n\nfor i in range(2):\n    for j in range(2):\n        ax[i, j].grid( linestyle='--', linewidth=0.5, alpha=0.5, color='grey')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nHagamos lo que hace una neurona con una función de activación sigmoide.\n\n\nCódigo\n# Función de activación sigmoide\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Datos de entrada\nX = np.random.randn(10, 1)\n\n# Pesos y sesgo\nnp.random.seed(1014)\nweights = np.random.randn(1, 10)\nbias = np.random.randn(1)\n\n# Salida de la neurona\ny = sigmoid(np.dot(X.T, weights.T) + bias)\n\nprint(y)\n\n\n[[0.20899694]]\n\n\nEstamos realizando la siguiente operación:\n\\[y = f(\\sum_{i=1}^{n} x_i \\cdot w_i + b)\\]\nDonde \\(f(x) = \\frac{1}{1 + e^{-x}}\\) es la función sigmoide. En este caso, estamos utilizando una neurona con 10 entradas y una salida.\nLo que busca simular o modelar el comportamiento de una neurona biológica. La neurona biológica recibe señales eléctricas de otras neuronas a través de las dendritas, las procesa en el cuerpo celular y envía una señal eléctrica a través del axón. La señal eléctrica se transmite a través de las sinapsis, que son las conexiones entre las neuronas.\nAhora veamos cómo se comporta una red neuronal con una capa oculta y una capa de salida. Para esto, vamos a implementar una red neuronal para regresión lineal con pesos y sesgos aleatorios.\n\n\nCódigo\nimport numpy as np\n\nclass NeuralNetwork():\n    def __init__(self, input_size, hidden_size, output_size, seed=1014):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        # Pesos y sesgos\n        np.random.seed(seed)\n        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n        self.bias_input_hidden = np.random.randn(hidden_size)\n        \n        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n        self.bias_hidden_output = np.random.randn(output_size)\n        \n    def forward(self, X, activation):\n        # Capa oculta\n        hidden = np.dot(X, self.weights_input_hidden) + self.bias_input_hidden\n        hidden = activation(hidden)\n        \n        # Capa de salida\n        output = np.dot(hidden, self.weights_hidden_output) + self.bias_hidden_output\n        \n        return output\n\n\nUsemos la red neuronal para predecir un conjunto de datos y con una función de activación identidad.\n\n\nCódigo\n# Datos de entrada\nX = np.random.randn(10, 1)\n\n# Parámetros de la red neuronal\ninput_size = 1\nhidden_size = 10\noutput_size = 1\n\n# Red Neuronal\nnn = NeuralNetwork(input_size, hidden_size, output_size)\n\n# Función de activación identidad\ndef identity(x):\n    return x\n\n# Predicciones\ny_pred = nn.forward(X, identity)\n\nprint(y_pred)\n\n\n[[-1.81046445]\n [-1.98312314]\n [-1.86441218]\n [-2.03739283]\n [-1.77027647]\n [-1.92937854]\n [-1.87128124]\n [-1.94289359]\n [-2.20644573]\n [-1.93053922]]\n\n\nEn este caso tenemos una red neuronal con una capa oculta de 10 neuronas y una capa de salida de 1 neurona. La función de activación de la capa oculta es la función identidad y la función de activación de la capa de salida también es la función identidad.\nLos pesos de la red son:\n\n\nCódigo\nprint(f\"Pesos capa oculta: {nn.weights_input_hidden}\\n\")\nprint(f\"Sesgos capa oculta: {nn.bias_input_hidden}\\n\")\nprint(f\"Pesos capa de salida: {nn.weights_hidden_output.T}\\n\")\nprint(f\"Sesgos capa de salida: {nn.bias_hidden_output}\\n\")\n\n\nPesos capa oculta: [[ 0.75943278 -1.00725987 -0.64499024 -0.26674068  0.29125552  0.14820587\n   0.6382988   0.46738854  0.53122954  1.1840206 ]]\n\nSesgos capa oculta: [-1.25173295 -1.30489407  0.20194032 -0.83407915  0.67556507 -1.65562438\n -0.26710189 -0.77413114 -0.14915279  2.15093091]\n\nPesos capa de salida: [[-0.25697236  0.50673502 -1.31687341  1.71747235 -0.12242967 -0.06419002\n   0.47479916 -0.01225672  1.10530347 -0.54019387]]\n\nSesgos capa de salida: [1.4985733]\n\n\n\nPodemos dibujar la red neuronal con los pesos y sesgos asociados a cada conexión.\n\n\nCódigo\nimport itertools\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n# Colores para las capas\nsubset_colors = ['blue', 'red', 'green']\n\ndef multilayered_graph(input_size, hidden_size, output_size, weights_input_hidden, weights_hidden_output):\n    # Crear los rangos para las capas\n    subset_sizes = [input_size, hidden_size, output_size]\n    extents = nx.utils.pairwise(itertools.accumulate((0,) + tuple(subset_sizes)))\n    layers = [range(start, end) for start, end in extents]\n    \n    # Crear el gráfico\n    G = nx.Graph()\n    for i, layer in enumerate(layers):\n        G.add_nodes_from(layer, layer=i)\n        \n    # Añadir los bordes con pesos para capa de entrada a oculta\n    for i, j in itertools.product(layers[0], layers[1]):\n        G.add_edge(i, j, weight=round(weights_input_hidden[i, j - layers[1][0]], 3))\n        \n    # Añadir los bordes con pesos para capa oculta a salida\n    for i, j in itertools.product(layers[1], layers[2]):\n        G.add_edge(i, j, weight=round(weights_hidden_output[i - layers[1][0], j - layers[2][0]], 3))\n    \n    return G\n\n# Crear el gráfico con los pesos\nG = multilayered_graph(input_size, hidden_size, output_size, nn.weights_input_hidden, nn.weights_hidden_output)\n\n# Colores para los nodos según su capa\ncolor = [subset_colors[data[\"layer\"]] for node, data in G.nodes(data=True)]\n\n# Posición de los nodos\npos = nx.multipartite_layout(G, subset_key=\"layer\")\n\n# Dibujar el gráfico\nplt.figure(figsize=(10, 6))\nnx.draw(G, pos, with_labels=False, node_color=color, node_size=1500, font_size=10, font_weight='bold')\n\n# Dibujar los bordes con los pesos\nedge_labels = nx.get_edge_attributes(G, 'weight')\nnx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCada una de las neuronas realiza la operación que hemos visto anteriormente. Sin embargo, aquí solo hemos decidido los pesos y sesgos de la red neuronal de forma aleatoria. En la práctica, estos pesos y sesgos se ajustan durante el entrenamiento de la red neuronal, es decir, la red neuronal aprende a partir de los datos.",
    "crumbs": [
      "Redes Neuronales",
      "Introducción a Redes Neuronales"
    ]
  },
  {
    "objectID": "neural_neworks/neural_networks.html#entrenamiento-de-una-red-neuronal",
    "href": "neural_neworks/neural_networks.html#entrenamiento-de-una-red-neuronal",
    "title": "Introducción a Redes Neuronales",
    "section": "",
    "text": "El entrenamiento de una red neuronal consiste en ajustar los pesos y sesgos de la red para minimizar una función de pérdida. La función de pérdida mide la diferencia entre las predicciones del modelo y los valores reales. Durante el entrenamiento, los pesos y sesgos se ajustan iterativamente utilizando un algoritmo de optimización.\nExisten diversos algoritmos de optimización, algunos de los más comunes son:\n\nDescenso del Gradiente : Actualiza los pesos en la dirección opuesta al gradiente de la función de pérdida.\nAdam: Utiliza una combinación de descenso del gradiente y adaptación de la tasa de aprendizaje.\nRMSprop: Se adapta a la tasa de aprendizaje para cada parámetro.\nAdagrad: Ajusta la tasa de aprendizaje para cada parámetro en función de la magnitud de los gradientes.\n\nEl algoritmo tipico es el descenso del gradiente. La idea es ajustar los pesos y sesgos de la red neuronal en la dirección opuesta al gradiente de la función de pérdida. El gradiente de la función de pérdida se calcula utilizando la regla de la cadena y el algoritmo de retropropagación. Pero en la práctica, se utiliza una variante del descenso del gradiente llamada descenso del gradiente estocástico o el algoritmo Adam.\nAlgunas funciones de pérdida comunes son:\n\nError Cuadrático Medio. \\[L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nEntropía Cruzada. \\[L(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\\]\nError Absoluto Medio. \\[L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\]\n\nEl algoritmo de entrenamiento de una red neuronal se puede resumir en los siguientes pasos:\n\nInicializar los pesos y sesgos de la red neuronal.\nCalcular la salida de la red neuronal.\nCalcular la función de pérdida.\nCalcular el gradiente de la función de pérdida.\nActualizar los pesos y sesgos utilizando un algoritmo de optimización.\nRepetir los pasos 2-5 hasta que se alcance un número de iteraciones o se cumpla un criterio de parada.\n\n\n\nVamos a ver cómo se actualizan los pesos y sesgos de una neurona durante el entrenamiento. Para esto, vamos a implementar una neurona con una función de activación sigmoide y vamos a entrenar la neurona para realizar una regresión lineal y usar la función de pérdida de error cuadrático medio.\nPara obtener el gradiente de la función de pérdida, vamos a utilizar la regla de la cadena y el algoritmo de retropropagación. La regla de la cadena se utiliza para calcular el gradiente de una función compuesta y el algoritmo de retropropagación se utiliza para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal.\n\n\nLa regla de la cadena se utiliza para calcular el gradiente de una función compuesta. Si tenemos una función \\(f(g(x))\\), el gradiente de \\(f\\) con respecto a \\(x\\) se puede calcular como:\n\\[\\frac{\\partial f(g(x))}{\\partial x} = \\frac{\\partial f(g(x))}{\\partial g(x)} \\cdot \\frac{\\partial g(x)}{\\partial x}\\]\nEn nuestro caso tenemos una función de pérdida \\(L(y, \\hat{y})\\) y una función de activación \\(f(x)\\). Entonces, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal se puede calcular como:\n\\[\\frac{\\partial L(y, \\hat{y})}{\\partial w_i} = \\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i}\\]\nDonde \\(z = \\sum_{i=1}^{n} x_i \\cdot w_i + b\\) es la entrada de la neurona y \\(\\hat{y} = f(z)\\) es la salida de la neurona.\nHagamos la primera parte de la regla de la cadena, es decir, el gradiente de la función de pérdida con respecto a la salida de la neurona.\n\\[\\begin{align*}\n\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} &= \\frac{\\partial}{\\partial \\hat{y}} \\left( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\right) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\hat{y}} (y_i - \\hat{y}_i)^2 \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} -2(y_i - \\hat{y}_i) \\\\\n&= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i)\n\\end{align*}\\]\nLa segunda parte de la regla de la cadena es el gradiente de la salida de la neurona con respecto a la entrada de la neurona.\n\\[\\begin{align*}\n\\frac{\\partial \\hat{y}}{\\partial z} &= \\frac{\\partial}{\\partial z} \\left( \\frac{1}{1 + e^{-z}} \\right) \\\\\n&= \\frac{e^{-z}}{(1 + e^{-z})^2} \\\\\n&= \\frac{1}{1 + e^{-z}} \\cdot \\left(1 - \\frac{1}{1 + e^{-z}} \\right) \\\\\n&= \\hat{y} \\cdot (1 - \\hat{y})\n\\end{align*}\\]\nLa tercera parte de la regla de la cadena es el gradiente de la entrada de la neurona con respecto a los pesos.\n\\[\\begin{align*}\n\\frac{\\partial z}{\\partial w_i} &= \\frac{\\partial}{\\partial w_i} \\left( \\sum_{i=1}^{n} x_i \\cdot w_i + b \\right) \\\\\n&= x_i\n\\end{align*}\\]\nAhora para el sesgo.\n\\[\\begin{align*}\n\\frac{\\partial z}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left( \\sum_{i=1}^{n} x_i \\cdot w_i + b \\right) \\\\\n&= 1\n\\end{align*}\\]\nEntonces, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal se puede calcular como:\n\\[\\begin{align*}\n\\frac{\\partial L(y, \\hat{y})}{\\partial w_i} &= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i) \\cdot \\hat{y}_i \\cdot (1 - \\hat{y}_i) \\cdot x_i \\\\\n\\frac{\\partial L(y, \\hat{y})}{\\partial b} &= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i) \\cdot \\hat{y}_i \\cdot (1 - \\hat{y}_i)\n\\end{align*}\\]\n\n\nPara actualizar los pesos y sesgos de la red neuronal, utilizamos el algoritmo de descenso del gradiente. La actualización de los pesos y sesgos se realiza de la siguiente forma:\n\\[\\begin{align*}\nw_i &:= w_i - \\alpha \\cdot \\frac{\\partial L(y, \\hat{y})}{\\partial w_i} \\\\\nb &:= b - \\alpha \\cdot \\frac{\\partial L(y, \\hat{y})}{\\partial b}\n\\end{align*}\\]\nDonde \\(\\alpha\\) es la tasa de aprendizaje, que es un hiperparámetro del modelo. La tasa de aprendizaje controla la magnitud de la actualización de los pesos y sesgos. Si la tasa de aprendizaje es muy pequeña, el modelo puede tardar mucho tiempo en converger. Si la tasa de aprendizaje es muy grande, el modelo puede divergir.\n\n\n\n\nLa retropropagación es un algoritmo que se utiliza para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal, usamos la regla de la cadena para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal y lo propagamos hacia atrás a través de la red neuronal.\nPara cada capa de la red neuronal, calculamos el gradiente de la función de pérdida con respecto a los pesos y sesgos de la capa utilizando la regla de la cadena y el gradiente de la capa anterior. Luego, actualizamos los pesos y sesgos de la capa. Este proceso se repite para todas las capas de la red neuronal. Un hermoso gif creado por Michael Pyrcz muestra cómo funciona la retropropagación.\n\n\n\nRetropropagación del Gradiente\n\n\n\n\n\n\nHagamos una red neuronal con 1 neurona en la capa oculta y 5 neuronas en la capa de salida, usaremos la función de activación sigmoide para la capa oculta y la función de activación identidad para la capa de salida. Vamos a entrenar la red neuronal para realizar una regresión lineal y usaremos la función de pérdida de error cuadrático medio.\n\n\nSimulemos datos de entrada y salida para entrenar la red neuronal.\n\n\nCódigo\nimport numpy as np\nimport pandas as pd\n\n# Datos de entrada\nX = np.random.normal(0, 5, (40, 1))\n\n# Datos de salida\ny = 2 * X + 3 + np.random.normal(0, 1, (40, 1))\n\ndf = pd.DataFrame(np.concatenate([X, y], axis=1), columns=[\"X\", \"y\"])\ndf.head()\n\n\n\n\n\n\n\n\n\n\nX\ny\n\n\n\n\n0\n1.705603\n5.973504\n\n\n1\n-6.149390\n-8.670173\n\n\n2\n5.814690\n13.924261\n\n\n3\n2.384975\n6.429876\n\n\n4\n0.502252\n2.931682\n\n\n\n\n\n\n\n\n\n\n\nVamos a implementar algunas funciones auxiliares para la red neuronal.\n\n\nCódigo\n# Función de activación sigmoide\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Función de pérdida de error cuadrático medio\ndef mse(y, y_pred):\n    return np.mean((y - y_pred) ** 2)\n\ndef weight_derivative_hidden(X, y, y_pred):\n    \"\"\"\n    Derivada de los pesos de la capa oculta\n    \"\"\"\n    return -2 * np.mean((y - y_pred) * y_pred * (1 - y_pred) * X)\n\ndef bias_derivative_hidden(y, y_pred):\n    \"\"\"\n    Derivada del sesgo de la capa oculta\n    \"\"\"\n    return -2 * np.mean((y - y_pred) * y_pred * (1 - y_pred))\n\ndef weight_derivative_output(hidden, y, y_pred):\n    \"\"\"\n    Derivada de los pesos de la capa de salida para la función de pérdida de error cuadrático medio y función de activación identidad\n    \"\"\"\n    return -2 * np.mean((y - y_pred) * X)\n\ndef bias_derivative_output(y, y_pred):\n    \"\"\"\n    Derivada del sesgo de la capa de salida para la función de pérdida de error cuadrático medio y función de activación identidad\n    \"\"\"\n    return -2 * np.mean(y - y_pred)\n\n# Inicialización de los pesos y sesgos\ndef initialize_weights(input_size, hidden_size, output_size, seed=1014):\n    np.random.seed(seed)\n    weights_input_hidden = np.random.randn(input_size, hidden_size)\n    bias_input_hidden = np.random.randn(hidden_size)\n    \n    weights_hidden_output = np.random.randn(hidden_size, output_size)\n    bias_hidden_output = np.random.randn(output_size)\n    \n    return weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output\n\n\n\n\n\nVamos a entrenar la red neuronal utilizando el algoritmo de retropropagación. Durante el entrenamiento, vamos a calcular la función de pérdida, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal y vamos a actualizar los pesos y sesgos utilizando el algoritmo de descenso del gradiente.\n\n\nCódigo\ndef learning( X, y, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, learning_rate=0.01):\n    # Capa oculta\n    hidden = np.dot(X, weights_input_hidden) + bias_input_hidden\n    hidden = sigmoid(hidden)\n    \n    # Capa de salida\n    output = np.dot(hidden, weights_hidden_output) + bias_hidden_output\n    \n    # Función de pérdida\n    loss = mse(y, output)\n    \n    # Gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal\n    weight_der_hidden = weight_derivative_hidden(X, y, output)\n    bias_der_hidden = bias_derivative_hidden(y, output)\n    \n    weight_der_output = weight_derivative_output(hidden, y, output)\n    bias_der_output = bias_derivative_output(y, output)\n    \n    # Actualización de los pesos y sesgos\n    weights_input_hidden -= learning_rate * weight_der_hidden\n    bias_input_hidden -= learning_rate * bias_der_hidden\n    \n    weights_hidden_output -= learning_rate * weight_der_output\n    bias_hidden_output -= learning_rate * bias_der_output\n    \n    return weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, loss\n\n# Creamos un bucle para entrenar la red neuronal\n\ninput_size = 1\nhidden_size = 5\noutput_size = 1\n\nweights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output = initialize_weights(input_size, hidden_size, output_size)\n\nlearning_rate = 0.0001\nepochs = 1500\n\nX_normalized = (X - X.mean()) / X.std()\nY_normalized = (y - y.mean()) / y.std()\n\nfor epoch in range(epochs):\n    weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, loss = learning(\n    X_normalized, Y_normalized, \n    weights_input_hidden, \n    bias_input_hidden, \n    weights_hidden_output, \n    bias_hidden_output, \n    learning_rate)\n    \n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}: Loss {loss}\")\n    elif epoch == epochs - 1:\n        print(f\"Epoch {epoch}: Loss {loss}\")\n\n\nEpoch 0: Loss 9.962358063298385\nEpoch 100: Loss 6.389020521754107\nEpoch 200: Loss 4.640115312713357\nEpoch 300: Loss 3.593583112789311\nEpoch 400: Loss 2.893864014205564\nEpoch 500: Loss 2.4040869381649284\nEpoch 600: Loss 2.0676644655384124\nEpoch 700: Loss 1.8628933379595423\nEpoch 800: Loss 1.7838514284596045\nEpoch 900: Loss 1.830931639664496\nEpoch 1000: Loss 2.0075290899566314\nEpoch 1100: Loss 2.3237647638230507\nEpoch 1200: Loss 2.8109028707617965\nEpoch 1300: Loss 3.559636339546768\nEpoch 1400: Loss 4.8448697380727035\nEpoch 1499: Loss 7.692389417773325\n\n\n\n\n\n\nUna ves que hemos entrenado la red neuronal, podemos hacer predicciones con la red neuronal que es el objetivo de crear un modelo de aprendizaje automático.\n\n\nCódigo\ndef predict(X, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output):\n    hidden = np.dot(X, weights_input_hidden) + bias_input_hidden\n    hidden = sigmoid(hidden)\n    \n    output = np.dot(hidden, weights_hidden_output) + bias_hidden_output\n    \n    return output\n\ny_pred = predict(X_normalized, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output)\n\ndf_normalized = pd.DataFrame(np.concatenate([X_normalized, Y_normalized, y_pred], axis=1), columns=[\"X\", \"y\", \"y_pred\"])\n\ndf_normalized.head()\n\n\n\n\n\n\n\n\n\n\nX\ny\ny_pred\n\n\n\n\n0\n0.223278\n0.172289\n1.025840\n\n\n1\n-1.426558\n-1.375204\n3.560325\n\n\n2\n1.086337\n1.012498\n0.068435\n\n\n3\n0.365971\n0.220517\n0.829005\n\n\n4\n-0.029469\n-0.149160\n1.409019\n\n\n\n\n\n\n\n\nA nivel profesional, se utilizan librerías como TensorFlow, PyTorch o Keras para implementar redes neuronales. Estas librerías proporcionan una interfaz de alto nivel para construir y entrenar redes neuronales de forma eficiente. Sin embargo, es importante entender cómo funcionan las redes neuronales a nivel de bajo nivel para poder depurar y optimizar los modelos.",
    "crumbs": [
      "Redes Neuronales",
      "Introducción a Redes Neuronales"
    ]
  },
  {
    "objectID": "neural_neworks/ann_python.html",
    "href": "neural_neworks/ann_python.html",
    "title": "Redes Neuronales Artificiales en Python",
    "section": "",
    "text": "Existen muchas librerías en Python que permiten implementar redes neuronales artificiales. En este tutorial, vamos a utilizar la librería scikit-learn, keras y tensorflow para implementar una red neuronal artificial para resolver problemas de clasificación y regresión.",
    "crumbs": [
      "Redes Neuronales",
      "Redes Neuronales Artificiales en Python"
    ]
  },
  {
    "objectID": "neural_neworks/ann_python.html#scikit-learn",
    "href": "neural_neworks/ann_python.html#scikit-learn",
    "title": "Redes Neuronales Artificiales en Python",
    "section": "scikit-learn",
    "text": "scikit-learn\nscikit-learn es una librería de Python que permite implementar algoritmos de aprendizaje supervisado y no supervisado. En particular, scikit-learn incluye una clase llamada MLPClassifier que permite implementar redes neuronales artificiales para problemas de clasificación.\n\nEjemplo de clasificación\nEn este ejemplo, vamos a utilizar la base de datos iris para entrenar una red neuronal artificial que permita clasificar las flores en tres categorías: setosa, versicolor y virginica.\n\n\nCódigo\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\n\n# Cargar la base de datos iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\nprint(iris.target_names)\nprint(iris.feature_names)\nprint(X.shape)\nprint(y.shape)\n\n\n['setosa' 'versicolor' 'virginica']\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n(150, 4)\n(150,)\n\n\nImplementamos la red neuronal artificial y la entrenamos con la base de datos iris.\n\n\nCódigo\n# Dividir la base de datos en entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Crear la red neuronal artificial\nclf = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42, \n                    activation='relu', solver='sgd', learning_rate='adaptive', \n                    learning_rate_init=0.001)\n\n# Entrenar la red neuronal artificial\nclf.fit(X_train, y_train)\n\n# Evaluar la red neuronal artificial\nscore = clf.score(X_test, y_test)\nprint(score)\n\n\n0.9333333333333333\n\n\nLa clase MLPClassifier tiene varios parámetros que permiten configurar la red neuronal artificial. En este ejemplo, utilizamos los siguientes parámetros:\n\nhidden_layer_sizes: Número de neuronas en cada capa oculta.\nmax_iter: Número máximo de iteraciones para entrenar la red neuronal artificial.\nrandom_state: Semilla para la generación de números aleatorios.\nsolver: Algoritmo de optimización para entrenar la red neuronal artificial.\nactivation: Función de activación para las neuronas en las capas ocultas.\nlearning_rate: Tasa de aprendizaje para actualizar los pesos de la red neuronal artificial.\nlearning_rate_init: Tasa de aprendizaje inicial para actualizar los pesos de la red neuronal artificial.\n\nEl método score permite evaluar la precisión de la red neuronal artificial en la base de datos de prueba.\nPodemos obtener las predicciones de la red neuronal artificial para la base de datos de prueba.\n\n\nCódigo\n# Obtener las predicciones de la red neuronal artificial\ny_pred = clf.predict(X_test)\nprint(y_pred)\n\n\n[1 0 2 1 1 0 1 2 2 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 1 2 2 2 0 0]\n\n\nPara evaluar la red neuronal artificial, podemos calcular la matriz de confusión.\n\n\nCódigo\nfrom sklearn.metrics import confusion_matrix\n\n# Calcular la matriz de confusión\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n\n[[10  0  0]\n [ 0  8  1]\n [ 0  1 10]]\n\n\nPodemos visualizar la matriz de confusión utilizando la librería seaborn.\n\n\nCódigo\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Visualizar la matriz de confusión\nsns.heatmap(cm, annot=True, fmt='d', \n            xticklabels=iris.target_names, \n            yticklabels=iris.target_names,\n            cmap='Blues')\nplt.xlabel('Predicción')\nplt.ylabel('Real')\nplt.show()\n\n\n\n\n\n\n\n\n\nCon este mapa de color podemos visualizar la matriz de confusión. Las filas representan las categorías reales y las columnas representan las categorías predichas. Los valores en la diagonal principal representan las predicciones correctas y esperamos que un buen modelo tenga valores altos en la diagonal principal.\nPOdemos usar el área bajo la curva ROC para evaluar la red neuronal artificial.\n\n\nCódigo\nfrom sklearn.metrics import roc_auc_score\n\n# Calcular el área bajo la curva ROC\ny_prob = clf.predict_proba(X_test)\nroc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\nprint(roc_auc)\n\n\n0.9966414352379265\n\n\nAquí el parámetro multi_class se refiere a la estrategia de codificación de clases. En este caso, utilizamos ovr que significa “one-vs-rest”, que evalúa cada clase en comparación con el resto de las clases y es sensible a datos desequilibrados.\nEntre más cercano a 1 sea el valor del área bajo la curva ROC, mejor será el modelo, ya que significa que el modelo es capaz de distinguir entre las diferentes clases.",
    "crumbs": [
      "Redes Neuronales",
      "Redes Neuronales Artificiales en Python"
    ]
  },
  {
    "objectID": "neural_neworks/ann_python.html#pytorch",
    "href": "neural_neworks/ann_python.html#pytorch",
    "title": "Redes Neuronales Artificiales en Python",
    "section": "PyTorch",
    "text": "PyTorch\nPyTorch es una librería de Python que permite implementar redes neuronales artificiales de manera eficiente. En particular, PyTorch incluye una clase llamada nn.Module que permite definir la arquitectura de la red neuronal artificial.\n\nUso básico de PyTorch\nPara utilizar PyTorch, primero debemos instalar la librería, el comando que recomienda la página de PyTorch es el siguiente para Windows y sin el uso de GPU.\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\nTensores\nEn PyTorch, los datos se almacenan en tensores, que son arreglos multidimensionales similares a los arreglos de NumPy. Podemos crear tensores en PyTorch de la siguiente manera.\n\n\nCódigo\nimport torch\n\n# Crear un tensor de ceros\nx = torch.zeros(2, 3)\n\nprint(x)\n\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nPodemos crear tensores a partir de arreglos de NumPy.\n\n\nCódigo\nimport numpy as np\n\n# Crear un arreglo de NumPy\narr = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Crear un tensor de PyTorch a partir de un arreglo de NumPy\nx = torch.tensor(arr)\n\nprint(x)\n\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\nLos tensores pueden ser de diferentes tipos de datos, como float32, int64, bool, etc.\n\n\nCódigo\n# Crear un tensor de enteros\nx = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int64)\n\nprint(x)\n\n# Crear un tensor de booleanos\nx = torch.tensor([[True, False], [False, True]])\n\nprint(x)\n\n# Crear un tensor de punto flotante\nx = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)\n\nprint(x)\n\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntensor([[ True, False],\n        [False,  True]])\ntensor([[1., 2.],\n        [3., 4.]])\n\n\nPodemos tener tensores de cualquier dimensión.\n\n\nCódigo\n# Crear un tensor de 3 dimensiones\nx = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\nprint(x)\n\n\ntensor([[[1, 2],\n         [3, 4]],\n\n        [[5, 6],\n         [7, 8]]])\n\n\n\n\nOperaciones con tensores\nPodemos realizar operaciones matemáticas con tensores en PyTorch.\n\n\nCódigo\n# Crear dos tensores\nx = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\ny = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n\n# Sumar los tensores\nz = x + y\n\nprint(z)\n\n# Multiplicar los tensores\nz = x * y\nprint(z)\n\n# Multiplicar un tensor por un escalar\nz = 2 * x\nprint(z)\n\n# Calcular la transpuesta de un tensor\nz = x.t()\nprint(z)\n\n# Calcular el producto punto de dos tensores\nz = torch.dot(x.view(-1), y.view(-1))\nprint(z)\n\n# Calcular el producto matricial de dos tensores\nz = torch.mm(x, y)\n\nprint(z)\n\n# Calcular la inversa de un tensor\nz = torch.inverse(x)\n\nprint(z)\n\n# Calcular la norma de un tensor\nz = torch.norm(x)\n\nprint(z)\n\n# Calcular la media de un tensor\nz = torch.mean(x)\n\nprint(z)\n\n# Calcular la desviación estándar de un tensor\nz = torch.std(x)\n\nprint(z)\n\n\ntensor([[ 6.,  8.],\n        [10., 12.]])\ntensor([[ 5., 12.],\n        [21., 32.]])\ntensor([[2., 4.],\n        [6., 8.]])\ntensor([[1., 3.],\n        [2., 4.]])\ntensor(70.)\ntensor([[19., 22.],\n        [43., 50.]])\ntensor([[-2.0000,  1.0000],\n        [ 1.5000, -0.5000]])\ntensor(5.4772)\ntensor(2.5000)\ntensor(1.2910)\n\n\n\n\nMétodos de tensores\nLos tensores en PyTorch tienen varios métodos útiles.\n\n\nCódigo\n# Crear un tensor de 3 dimensiones\nx = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n# Obtener la forma del tensor\nprint(f\"Shape: {x.shape}\")\n\n# Obtener el número de elementos en el tensor\nprint(f\"Size: {x.size()}\")\n\n# Obtener el tipo de datos del tensor\nprint(f\"Data type: {x.dtype}\")\n\n# Obtener el dispositivo en el que se almacena el tensor\nprint(f\"Device: {x.device}\")\n\n# Obtener el número de dimensiones del tensor\nprint(f\"Number of dimensions: {x.dim()}\")\n\n# Obtener el número de elementos en una dimensión específica\nprint(f\"Number of elements in the first dimension: {x.size(0)}\")\n\n# Obtener el elemento en una posición específica\nprint(f\"Element at position (0, 1, 1): {x[0, 1, 1]}\")\n\n# Obtener un subtensor\nprint(f\"Subtensor: {x[0, :, :]}\")\n\n# Cambiar la forma del tensor\nprint(f\"Reshape: {x.view(2, 4)}\")\n\n# Aplanar el tensor\nprint(f\"Flatten: {x.view(-1)}\")\n\n# Concatenar tensores\ny = torch.tensor([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\nz = torch.cat((x, y), dim=0)\n\nprint(z)\n\n# Dividir un tensor\nz1, z2 = torch.chunk(z, 2, dim=0)\n\nprint(f\"z1: {z1}\")\nprint(f\"z2: {z2}\")\n\n# Calcular la suma acumulada de un tensor\nz = torch.cumsum(x, dim=0)\n\nprint(f\"Cumulative sum: {z}\")\n\n\nShape: torch.Size([2, 2, 2])\nSize: torch.Size([2, 2, 2])\nData type: torch.int64\nDevice: cpu\nNumber of dimensions: 3\nNumber of elements in the first dimension: 2\nElement at position (0, 1, 1): 4\nSubtensor: tensor([[1, 2],\n        [3, 4]])\nReshape: tensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])\nFlatten: tensor([1, 2, 3, 4, 5, 6, 7, 8])\ntensor([[[ 1,  2],\n         [ 3,  4]],\n\n        [[ 5,  6],\n         [ 7,  8]],\n\n        [[ 9, 10],\n         [11, 12]],\n\n        [[13, 14],\n         [15, 16]]])\nz1: tensor([[[1, 2],\n         [3, 4]],\n\n        [[5, 6],\n         [7, 8]]])\nz2: tensor([[[ 9, 10],\n         [11, 12]],\n\n        [[13, 14],\n         [15, 16]]])\nCumulative sum: tensor([[[ 1,  2],\n         [ 3,  4]],\n\n        [[ 6,  8],\n         [10, 12]]])\n\n\n\n\n\nEstructura de una red neuronal artificial en PyTorch\nVeamos las distintas partes de una red neuronal artificial en PyTorch.\n\nEjemplo de clasificación\nEn este ejemplo, vamos a utilizar la base de datos iris para entrenar una red neuronal artificial que permita clasificar las flores en tres categorías: setosa, versicolor y virginica.\n\n\nCódigo\nimport torch\nimport torch.nn as nn\n\n# Crear la red neuronal artificial\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(4, 10)\n        self.fc2 = nn.Linear(10, 10)\n        self.fc3 = nn.Linear(10, 3)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        x = self.softmax(x)\n        return x\n\nmodel = MLP()\n\nprint(model)\n\n\nMLP(\n  (fc1): Linear(in_features=4, out_features=10, bias=True)\n  (fc2): Linear(in_features=10, out_features=10, bias=True)\n  (fc3): Linear(in_features=10, out_features=3, bias=True)\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n)\n\n\nPara crear la red neuronal artificial, definimos una clase llamada MLP que hereda de nn.Module. En el método __init__, definimos las capas de la red neuronal artificial y las funciones de activación. En el método forward, definimos la arquitectura de la red neuronal artificial.\nDentro del método __init__, usamos la función super para inicializar la clase base nn.Module. Luego, definimos las capas de la red neuronal artificial utilizando la clase nn.Linear que representa una capa de neuronas completamente conectada. En este caso, definimos tres capas de neuronas con 4, 10 y 3 neuronas respectivamente.\nTambién definimos las funciones de activación nn.ReLU y nn.Softmax para las capas ocultas y de salida respectivamente. La función de activación ReLU se utiliza para introducir no linealidades en la red neuronal artificial, mientras que la función de activación Softmax se utiliza para obtener probabilidades de las clases y recive como argumento la dimensión en la que se calcula el softmax.\nEn el método forward, definimos la arquitectura de la red neuronal artificial. Primero aplicamos la capa fc1 seguida de la función de activación ReLU. Luego aplicamos la capa fc2 seguida de la función de activación ReLU. Finalmente aplicamos la capa fc3 seguida de la función de activación Softmax.\nLa función Linear aplicará una transformación lineal a los datos de entrada: \\(y = xA^T + b\\), donde \\(x\\) es la entrada, \\(A\\) es la matriz de pesos y \\(b\\) es el vector de sesgos.\nEs importante que las neuronas entre capas tengan la misma cantidad de neuronas que la capa anterior y la capa siguiente. En este caso, la capa de entrada tiene 4 neuronas, la capa oculta tiene 10 neuronas y la capa de salida tiene 3 neuronas.\nVamos a usar la base de datos iris para ver que resultados obtenemos.\n\n\nCódigo\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Cargar la base de datos iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\nmodel = MLP()\n\n# Convertir los datos a tensores de PyTorch\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.long)\n\nprint(X.shape)\nprint(y.shape)\n\n# Obtener las predicciones de la red neuronal artificial\ny_pred = model(X)\n\nprint(y_pred)\n\n\ntorch.Size([150, 4])\ntorch.Size([150])\ntensor([[0.3815, 0.2707, 0.3478],\n        [0.3707, 0.2763, 0.3529],\n        [0.3767, 0.2748, 0.3485],\n        [0.3761, 0.2767, 0.3472],\n        [0.3848, 0.2700, 0.3452],\n        [0.3857, 0.2675, 0.3468],\n        [0.3805, 0.2740, 0.3455],\n        [0.3804, 0.2723, 0.3473],\n        [0.3722, 0.2792, 0.3486],\n        [0.3762, 0.2751, 0.3487],\n        [0.3846, 0.2679, 0.3474],\n        [0.3825, 0.2732, 0.3442],\n        [0.3740, 0.2762, 0.3498],\n        [0.3763, 0.2772, 0.3465],\n        [0.3872, 0.2628, 0.3499],\n        [0.3947, 0.2610, 0.3442],\n        [0.3835, 0.2665, 0.3500],\n        [0.3790, 0.2712, 0.3498],\n        [0.3833, 0.2669, 0.3498],\n        [0.3871, 0.2685, 0.3445],\n        [0.3782, 0.2714, 0.3503],\n        [0.3820, 0.2700, 0.3479],\n        [0.3859, 0.2704, 0.3438],\n        [0.3707, 0.2750, 0.3542],\n        [0.3841, 0.2740, 0.3418],\n        [0.3710, 0.2765, 0.3525],\n        [0.3759, 0.2736, 0.3505],\n        [0.3813, 0.2706, 0.3481],\n        [0.3782, 0.2713, 0.3505],\n        [0.3783, 0.2756, 0.3461],\n        [0.3751, 0.2763, 0.3487],\n        [0.3722, 0.2719, 0.3559],\n        [0.3988, 0.2640, 0.3371],\n        [0.3958, 0.2623, 0.3418],\n        [0.3737, 0.2756, 0.3506],\n        [0.3738, 0.2735, 0.3527],\n        [0.3777, 0.2690, 0.3532],\n        [0.3881, 0.2698, 0.3420],\n        [0.3741, 0.2779, 0.3480],\n        [0.3796, 0.2719, 0.3485],\n        [0.3793, 0.2713, 0.3494],\n        [0.3538, 0.2849, 0.3613],\n        [0.3791, 0.2759, 0.3450],\n        [0.3735, 0.2737, 0.3529],\n        [0.3867, 0.2701, 0.3432],\n        [0.3691, 0.2772, 0.3537],\n        [0.3901, 0.2682, 0.3417],\n        [0.3781, 0.2754, 0.3465],\n        [0.3854, 0.2683, 0.3463],\n        [0.3773, 0.2730, 0.3496],\n        [0.3250, 0.2925, 0.3825],\n        [0.3263, 0.2936, 0.3801],\n        [0.3203, 0.2957, 0.3840],\n        [0.3146, 0.3016, 0.3837],\n        [0.3154, 0.3000, 0.3847],\n        [0.3272, 0.2929, 0.3800],\n        [0.3270, 0.2936, 0.3794],\n        [0.3322, 0.2924, 0.3754],\n        [0.3230, 0.2938, 0.3832],\n        [0.3244, 0.2970, 0.3785],\n        [0.3196, 0.2988, 0.3816],\n        [0.3244, 0.2961, 0.3795],\n        [0.3173, 0.2974, 0.3853],\n        [0.3239, 0.2944, 0.3816],\n        [0.3294, 0.2935, 0.3772],\n        [0.3242, 0.2940, 0.3818],\n        [0.3271, 0.2943, 0.3786],\n        [0.3342, 0.2883, 0.3776],\n        [0.3119, 0.3071, 0.3809],\n        [0.3256, 0.2940, 0.3803],\n        [0.3213, 0.2986, 0.3800],\n        [0.3233, 0.2955, 0.3811],\n        [0.3136, 0.3026, 0.3839],\n        [0.3275, 0.2911, 0.3814],\n        [0.3241, 0.2940, 0.3818],\n        [0.3223, 0.2953, 0.3824],\n        [0.3160, 0.2979, 0.3861],\n        [0.3157, 0.3008, 0.3834],\n        [0.3213, 0.2972, 0.3815],\n        [0.3307, 0.2913, 0.3780],\n        [0.3234, 0.2956, 0.3810],\n        [0.3272, 0.2933, 0.3795],\n        [0.3259, 0.2940, 0.3801],\n        [0.3147, 0.3010, 0.3843],\n        [0.3287, 0.2937, 0.3776],\n        [0.3317, 0.2916, 0.3767],\n        [0.3216, 0.2956, 0.3828],\n        [0.3131, 0.3023, 0.3846],\n        [0.3331, 0.2904, 0.3765],\n        [0.3200, 0.2984, 0.3816],\n        [0.3268, 0.2932, 0.3801],\n        [0.3265, 0.2931, 0.3804],\n        [0.3234, 0.2953, 0.3813],\n        [0.3282, 0.2944, 0.3774],\n        [0.3248, 0.2950, 0.3802],\n        [0.3364, 0.2879, 0.3757],\n        [0.3294, 0.2921, 0.3784],\n        [0.3257, 0.2934, 0.3809],\n        [0.3285, 0.2949, 0.3766],\n        [0.3266, 0.2940, 0.3794],\n        [0.3149, 0.3109, 0.3742],\n        [0.3139, 0.3066, 0.3794],\n        [0.3139, 0.3074, 0.3787],\n        [0.3150, 0.3018, 0.3833],\n        [0.3141, 0.3085, 0.3774],\n        [0.3133, 0.3066, 0.3801],\n        [0.3142, 0.3049, 0.3809],\n        [0.3139, 0.3020, 0.3841],\n        [0.3121, 0.3071, 0.3808],\n        [0.3163, 0.3076, 0.3762],\n        [0.3162, 0.3037, 0.3801],\n        [0.3134, 0.3073, 0.3793],\n        [0.3143, 0.3078, 0.3779],\n        [0.3125, 0.3112, 0.3763],\n        [0.3129, 0.3155, 0.3716],\n        [0.3153, 0.3091, 0.3757],\n        [0.3154, 0.3012, 0.3834],\n        [0.3179, 0.2985, 0.3836],\n        [0.3100, 0.3151, 0.3749],\n        [0.3120, 0.3055, 0.3826],\n        [0.3147, 0.3090, 0.3762],\n        [0.3145, 0.3075, 0.3780],\n        [0.3123, 0.3070, 0.3807],\n        [0.3139, 0.3061, 0.3800],\n        [0.3162, 0.3033, 0.3805],\n        [0.3160, 0.2988, 0.3852],\n        [0.3146, 0.3049, 0.3805],\n        [0.3159, 0.3019, 0.3821],\n        [0.3133, 0.3093, 0.3774],\n        [0.3154, 0.2978, 0.3868],\n        [0.3130, 0.3059, 0.3811],\n        [0.3184, 0.2957, 0.3859],\n        [0.3130, 0.3113, 0.3757],\n        [0.3172, 0.2980, 0.3848],\n        [0.3172, 0.2967, 0.3861],\n        [0.3127, 0.3121, 0.3752],\n        [0.3162, 0.3076, 0.3762],\n        [0.3161, 0.2998, 0.3841],\n        [0.3160, 0.3020, 0.3820],\n        [0.3148, 0.3070, 0.3782],\n        [0.3140, 0.3122, 0.3739],\n        [0.3143, 0.3118, 0.3739],\n        [0.3139, 0.3066, 0.3794],\n        [0.3148, 0.3083, 0.3769],\n        [0.3149, 0.3114, 0.3737],\n        [0.3138, 0.3124, 0.3738],\n        [0.3123, 0.3104, 0.3773],\n        [0.3149, 0.3060, 0.3791],\n        [0.3167, 0.3059, 0.3775],\n        [0.3164, 0.3010, 0.3826]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\nQue nos regresa un tensor con las probabilidades de cada clase para cada observación. Para obtener la clase predicha, podemos usar la función argmax de PyTorch.\n\n\nCódigo\n# Obtener la clase predicha\ny_pred = torch.argmax(y_pred, dim=1)\n\nprint(y_pred)\n\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n        0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2])\n\n\n\n\nFunciones de activación en PyTorch\nEn PyTorch, las funciones de activación se pueden utilizar como capas en una red neuronal artificial. Las funciones de activación más comunes son ReLU, Sigmoid y Softmax.\n\n\nCódigo\nimport torch\n\n# Crear un tensor de entrada\nx = torch.tensor([[1.0, -1.0], [-1.0, 1.0]])\n\n# Función de activación ReLU\nrelu = torch.nn.ReLU()\ny = relu(x)\n\nprint(f\"ReLU: {y}\")\n\n# Función de activación Sigmoid\nsigmoid = torch.nn.Sigmoid()\ny = sigmoid(x)\n\nprint(f\"Sigmoid: {y}\")\n\n# Función de activación Softmax\nsoftmax = torch.nn.Softmax(dim=1)\ny = softmax(x)\n\nprint(f\"Softmax: {y}\")\n\n# Función de activación Tanh\ntanh = torch.nn.Tanh()\ny = tanh(x)\n\nprint(f\"Tanh: {y}\")\n\n# Función de activación LeakyReLU\nleaky_relu = torch.nn.LeakyReLU(negative_slope=0.01)\ny = leaky_relu(x)\n\nprint(f\"LeakyReLU: {y}\")\n\n\nReLU: tensor([[1., 0.],\n        [0., 1.]])\nSigmoid: tensor([[0.7311, 0.2689],\n        [0.2689, 0.7311]])\nSoftmax: tensor([[0.8808, 0.1192],\n        [0.1192, 0.8808]])\nTanh: tensor([[ 0.7616, -0.7616],\n        [-0.7616,  0.7616]])\nLeakyReLU: tensor([[ 1.0000, -0.0100],\n        [-0.0100,  1.0000]])\n\n\n\n\n\nEntrenamiento de la red neuronal\nPara entrenar la red neuronal artificial, primero debemos definir una función de pérdida y un optimizador.\n\nFunción de pérdida\nLa función de pérdida mide la diferencia entre las predicciones de la red neuronal artificial y las etiquetas reales. Existen diversas funciones de pérdida para problemas de clasificación y regresión.\n\nFunciones de pérdida para problemas de clasificación:\n\nnn.CrossEntropyLoss: Utilizada para problemas de clasificación multiclase.\nnn.BCELoss: Utilizada para problemas de clasificación binaria.\nnn.NLLLoss: Utilizada para problemas de clasificación multiclase con salida logarítmica.\n\nFunciones de pérdida para problemas de regresión:\n\nnn.MSELoss: Utilizada para problemas de regresión de mínimos cuadrados.\nnn.L1Loss: Utilizada para problemas de regresión de mínimos absolutos.\nnn.SmoothL1Loss: Utilizada para problemas de regresión de mínimos suavizados.\n\n\n\n\nCódigo\nimport torch.nn as nn\n\ny_pred = torch.tensor([[0.1, 0.2, 0.7], [0.8, 0.1, 0.1], [0.2, 0.6, 0.2]])\ny = torch.tensor([2, 0, 1])\n\n# Crear la función de pérdida\ncriterion = nn.CrossEntropyLoss()\n\n# Calcular la pérdida\nloss = criterion(y_pred, y)\n\nprint(loss)\n\n\ntensor(0.7694)\n\n\nNos regresa el error de la red neuronal. Para minimizar la función de pérdida, utilizamos un optimizador.\n\n\nOptimizador\nEl optimizador ajusta los pesos de la red neuronal artificial para minimizar la función de pérdida. Existen diversos optimizadores que se pueden utilizar para entrenar una red neuronal artificial.\n\nOptimizadores basados en gradiente:\n\ntorch.optim.SGD: Descenso de gradiente estocástico.\ntorch.optim.Adam: Algoritmo de optimización basado en el método de Adam.\ntorch.optim.RMSprop: Algoritmo de optimización basado en el método de RMSprop.\n\n\nCadad optimizador tiene sus propios hiperparámetros que se pueden ajustar para mejorar el rendimiento de la red neuronal artificial.\n\n\nCódigo\nimport torch.optim as optim\n\n# Crear el optimizador\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Realizar la retropropagación\noptimizer.zero_grad()\n\n# loss.backward()\n\n# Actualizar los pesos\noptimizer.step()\n\n\nEn este caso, utilizamos el optimizador SGD con una tasa de aprendizaje de 0.01. Primero, llamamos al método zero_grad para restablecer los gradientes de los pesos de la red neuronal artificial. Luego, llamamos al método backward para calcular los gradientes de la función de pérdida con respecto a los pesos. Finalmente, llamamos al método step para actualizar los pesos de la red neuronal artificial utilizando el algoritmo de optimización.\nEstos pasos se repiten varias veces para entrenar la red neuronal artificial en un conjunto de datos.\n\n\n\nPreparación de los datos\nPara entrenar una red neuronal artificial, primero debemos preparar los datos en tensores de PyTorch.\n\n\nCódigo\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Cargar la base de datos iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Dividir la base de datos en entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1014)\n\n# Convertir los datos a tensores de PyTorch\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.long)\ny_test = torch.tensor(y_test, dtype=torch.long)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n\ntorch.Size([120, 4])\ntorch.Size([30, 4])\ntorch.Size([120])\ntorch.Size([30])\n\n\nLos datos tipo torch.long es equivalente a int64 en NumPy y se utiliza para representar las etiquetas de las clases.\n\n\nRed Neuronal con PyTorch\nVamos a definir una red neuronal de 2 capas ocultas con 4 neuronas cada capa y una capa de salida con 3 neuronas.\n\n\nCódigo\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(4, 4)\n        self.fc2 = nn.Linear(4, 4)\n        self.fc3 = nn.Linear(4, 3)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        x = self.softmax(x)\n        return x\n    \n    def predict(self, x):\n        with torch.no_grad():\n            y = self.forward(x)\n            return torch.argmax(y, dim=1)\n    \nmodel = NeuralNetwork()\n\nprint(model)\n\n\nNeuralNetwork(\n  (fc1): Linear(in_features=4, out_features=4, bias=True)\n  (fc2): Linear(in_features=4, out_features=4, bias=True)\n  (fc3): Linear(in_features=4, out_features=3, bias=True)\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n)\n\n\nAhora creemos una función para entrenar la red neuronal.\n\n\nCódigo\ndef train(model, X_train, y_train, criterion, optimizer, epochs):\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        y_pred = model(X_train)\n        loss = criterion(y_pred, y_train)\n        loss.backward()\n        optimizer.step()\n        \n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n\n\nAhora vamos a entrenar la red neuronal.\n\n\nCódigo\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\ntrain(model, X_train, y_train, criterion, optimizer, epochs=1000)\n\n\nEpoch 50/1000, Loss: 1.0754790306091309\nEpoch 100/1000, Loss: 1.07102370262146\nEpoch 150/1000, Loss: 1.067678689956665\nEpoch 200/1000, Loss: 1.0643763542175293\nEpoch 250/1000, Loss: 1.0608757734298706\nEpoch 300/1000, Loss: 1.0570424795150757\nEpoch 350/1000, Loss: 1.052795648574829\nEpoch 400/1000, Loss: 1.0484215021133423\nEpoch 450/1000, Loss: 1.043760895729065\nEpoch 500/1000, Loss: 1.0385349988937378\nEpoch 550/1000, Loss: 1.0325710773468018\nEpoch 600/1000, Loss: 1.0256762504577637\nEpoch 650/1000, Loss: 1.0176128149032593\nEpoch 700/1000, Loss: 1.0081112384796143\nEpoch 750/1000, Loss: 0.9968506693840027\nEpoch 800/1000, Loss: 0.9835156798362732\nEpoch 850/1000, Loss: 0.9678962826728821\nEpoch 900/1000, Loss: 0.9499657154083252\nEpoch 950/1000, Loss: 0.9237939119338989\nEpoch 1000/1000, Loss: 0.9007318615913391\n\n\nFinalmente, vamos a evaluar la red neuronal en la base de datos de prueba.\n\n\nCódigo\ny_pred = model.predict(X_test)\n\nprint(y_pred)\n\n\ntensor([2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2,\n        0, 2, 2, 0, 2, 2])\n\n\nCalculamos la precisión de la red neuronal.\n\n\nCódigo\naccuracy = torch.sum(y_pred == y_test).item() / len(y_test)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\nAccuracy: 0.5\n\n\nComo vemos, la red tiene un precisión muy baja, esto se debe a que la red neuronal es muy compleja para el problema que estamos tratando de resolver. Dismunuyamos la complejidad de la red neuronal.\n\n\nCódigo\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(4, 2)\n        self.fc2 = nn.Linear(2, 3)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n    \n    def predict(self, x):\n        with torch.no_grad():\n            y = self.forward(x)\n            return torch.argmax(y, dim=1)\n\nmodel = NeuralNetwork()\n\nprint(model)\n\n\nNeuralNetwork(\n  (fc1): Linear(in_features=4, out_features=2, bias=True)\n  (fc2): Linear(in_features=2, out_features=3, bias=True)\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n)\n\n\nEntrenamos la red neuronal.\n\n\nCódigo\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\ntrain(model, X_train, y_train, criterion, optimizer, epochs=1500)\n\n\nEpoch 50/1500, Loss: 1.0786900520324707\nEpoch 100/1500, Loss: 1.0306339263916016\nEpoch 150/1500, Loss: 0.9199546575546265\nEpoch 200/1500, Loss: 0.8805767297744751\nEpoch 250/1500, Loss: 0.8552428483963013\nEpoch 300/1500, Loss: 0.8381394147872925\nEpoch 350/1500, Loss: 0.8256418704986572\nEpoch 400/1500, Loss: 0.8157903552055359\nEpoch 450/1500, Loss: 0.8073647022247314\nEpoch 500/1500, Loss: 0.7996431589126587\nEpoch 550/1500, Loss: 0.792230486869812\nEpoch 600/1500, Loss: 0.7848939299583435\nEpoch 650/1500, Loss: 0.7775012850761414\nEpoch 700/1500, Loss: 0.7699931859970093\nEpoch 750/1500, Loss: 0.7623674273490906\nEpoch 800/1500, Loss: 0.7546666860580444\nEpoch 850/1500, Loss: 0.7469660043716431\nEpoch 900/1500, Loss: 0.7393587231636047\nEpoch 950/1500, Loss: 0.7319421172142029\nEpoch 1000/1500, Loss: 0.7248038053512573\nEpoch 1050/1500, Loss: 0.7180129289627075\nEpoch 1100/1500, Loss: 0.711614727973938\nEpoch 1150/1500, Loss: 0.7056317925453186\nEpoch 1200/1500, Loss: 0.7000667452812195\nEpoch 1250/1500, Loss: 0.6949079632759094\nEpoch 1300/1500, Loss: 0.6901333332061768\nEpoch 1350/1500, Loss: 0.6857159733772278\nEpoch 1400/1500, Loss: 0.6816268563270569\nEpoch 1450/1500, Loss: 0.6778366565704346\nEpoch 1500/1500, Loss: 0.6743176579475403\n\n\nEvaluamos la red neuronal.\n\n\nCódigo\ny_pred = model.predict(X_test)\n\naccuracy = torch.sum(y_pred == y_test).item() / len(y_test)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\nAccuracy: 0.9666666666666667\n\n\nEsto mejora la precisión de la red neuronal. Podemos visualizar la matriz de confusión.\n\n\nCódigo\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, y_pred)\n\nsns.heatmap(cm, annot=True, fmt='d', \n            xticklabels=iris.target_names, \n            yticklabels=iris.target_names,\n            cmap='Blues')",
    "crumbs": [
      "Redes Neuronales",
      "Redes Neuronales Artificiales en Python"
    ]
  },
  {
    "objectID": "neural_neworks/ann_python.html#red-neuronal-con-pytorch",
    "href": "neural_neworks/ann_python.html#red-neuronal-con-pytorch",
    "title": "Redes Neuronales Artificiales en Python",
    "section": "Red Neuronal con PyTorch",
    "text": "Red Neuronal con PyTorch\nVamos a definir una red neuronal de 2 capas ocultas con 4 neuronas cada capa y una capa de salida con 3 neuronas.\n\n\nCódigo\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(4, 4)\n        self.fc2 = nn.Linear(4, 4)\n        self.fc3 = nn.Linear(4, 3)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        x = self.softmax(x)\n        return x\n    \n    def predict(self, x):\n        with torch.no_grad():\n            y = self.forward(x)\n            return torch.argmax(y, dim=1)\n    \nmodel = NeuralNetwork()\n\nprint(model)\n\n\nNeuralNetwork(\n  (fc1): Linear(in_features=4, out_features=4, bias=True)\n  (fc2): Linear(in_features=4, out_features=4, bias=True)\n  (fc3): Linear(in_features=4, out_features=3, bias=True)\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n)\n\n\nAhora creemos una función para entrenar la red neuronal.\n\n\nCódigo\ndef train(model, X_train, y_train, criterion, optimizer, epochs):\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        y_pred = model(X_train)\n        loss = criterion(y_pred, y_train)\n        loss.backward()\n        optimizer.step()\n        \n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n\n\nAhora vamos a entrenar la red neuronal.\n\n\nCódigo\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\ntrain(model, X_train, y_train, criterion, optimizer, epochs=1000)\n\n\nEpoch 50/1000, Loss: 1.0901236534118652\nEpoch 100/1000, Loss: 1.078803300857544\nEpoch 150/1000, Loss: 1.0607396364212036\nEpoch 200/1000, Loss: 1.0435291528701782\nEpoch 250/1000, Loss: 1.0200098752975464\nEpoch 300/1000, Loss: 0.9902514219284058\nEpoch 350/1000, Loss: 0.9576820731163025\nEpoch 400/1000, Loss: 0.92452472448349\nEpoch 450/1000, Loss: 0.8949669599533081\nEpoch 500/1000, Loss: 0.8739414811134338\nEpoch 550/1000, Loss: 0.8571217060089111\nEpoch 600/1000, Loss: 0.8431174159049988\nEpoch 650/1000, Loss: 0.8313065767288208\nEpoch 700/1000, Loss: 0.8212433457374573\nEpoch 750/1000, Loss: 0.8125001788139343\nEpoch 800/1000, Loss: 0.8048416376113892\nEpoch 850/1000, Loss: 0.7980007529258728\nEpoch 900/1000, Loss: 0.7917746305465698\nEpoch 950/1000, Loss: 0.7859945297241211\nEpoch 1000/1000, Loss: 0.7805151343345642\n\n\nFinalmente, vamos a evaluar la red neuronal en la base de datos de prueba.\n\n\nCódigo\ny_pred = model.predict(X_test)\n\nprint(y_pred)\n\n\ntensor([2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2,\n        0, 2, 2, 0, 2, 2])\n\n\nCalculamos la precisión de la red neuronal.\n\n\nCódigo\naccuracy = torch.sum(y_pred == y_test).item() / len(y_test)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\nAccuracy: 0.5\n\n\nComo vemos, la red tiene un precisión muy baja, esto se debe a que la red neuronal es muy compleja para el problema que estamos tratando de resolver. Dismunuyamos la complejidad de la red neuronal.\n\n\nCódigo\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(4, 2)\n        self.fc2 = nn.Linear(2, 3)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n    \n    def predict(self, x):\n        with torch.no_grad():\n            y = self.forward(x)\n            return torch.argmax(y, dim=1)\n\nmodel = NeuralNetwork()\n\nprint(model)\n\n\nNeuralNetwork(\n  (fc1): Linear(in_features=4, out_features=2, bias=True)\n  (fc2): Linear(in_features=2, out_features=3, bias=True)\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n)\n\n\nEntrenamos la red neuronal.\n\n\nCódigo\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\ntrain(model, X_train, y_train, criterion, optimizer, epochs=1500)\n\n\nEpoch 50/1500, Loss: 0.8520609140396118\nEpoch 100/1500, Loss: 0.8257223963737488\nEpoch 150/1500, Loss: 0.8081858158111572\nEpoch 200/1500, Loss: 0.7843711972236633\nEpoch 250/1500, Loss: 0.7425604462623596\nEpoch 300/1500, Loss: 0.700783908367157\nEpoch 350/1500, Loss: 0.6672554016113281\nEpoch 400/1500, Loss: 0.6447580456733704\nEpoch 450/1500, Loss: 0.6301285624504089\nEpoch 500/1500, Loss: 0.6202855706214905\nEpoch 550/1500, Loss: 0.6133307814598083\nEpoch 600/1500, Loss: 0.6081899404525757\nEpoch 650/1500, Loss: 0.6042429208755493\nEpoch 700/1500, Loss: 0.6011154055595398\nEpoch 750/1500, Loss: 0.59857177734375\nEpoch 800/1500, Loss: 0.5964599847793579\nEpoch 850/1500, Loss: 0.5946758389472961\nEpoch 900/1500, Loss: 0.5931462645530701\nEpoch 950/1500, Loss: 0.5918186902999878\nEpoch 1000/1500, Loss: 0.5906540751457214\nEpoch 1050/1500, Loss: 0.5896230936050415\nEpoch 1100/1500, Loss: 0.5887027978897095\nEpoch 1150/1500, Loss: 0.58787602186203\nEpoch 1200/1500, Loss: 0.5871283411979675\nEpoch 1250/1500, Loss: 0.5864485502243042\nEpoch 1300/1500, Loss: 0.5858272910118103\nEpoch 1350/1500, Loss: 0.5852569341659546\nEpoch 1400/1500, Loss: 0.5847312211990356\nEpoch 1450/1500, Loss: 0.5842447876930237\nEpoch 1500/1500, Loss: 0.5837932825088501\n\n\nEvaluamos la red neuronal.\n\n\nCódigo\ny_pred = model.predict(X_test)\n\naccuracy = torch.sum(y_pred == y_test).item() / len(y_test)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\nAccuracy: 0.9666666666666667\n\n\nEsto mejora la precisión de la red neuronal. Podemos visualizar la matriz de confusión.\n\n\nCódigo\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, y_pred)\n\nsns.heatmap(cm, annot=True, fmt='d', \n            xticklabels=iris.target_names, \n            yticklabels=iris.target_names,\n            cmap='Blues')",
    "crumbs": [
      "Redes Neuronales",
      "Redes Neuronales Artificiales en Python"
    ]
  },
  {
    "objectID": "intro_python/Variables.html",
    "href": "intro_python/Variables.html",
    "title": "Declaración de Variables y Operaciones",
    "section": "",
    "text": "En Python utilizamos variables para almacenar información en la memoria de la computadora. Las variables son como cajas en las que podemos guardar información, cada caja tiene un nombre y un contenido. Existen varias reglas para dar nombre a las variables en Python:\n\nEl nombre debe comenzar con una letra o un guión bajo.\nNo puede comenzar con un número.\nPuede contener letras, números y guiones bajos.\n\nEl proceso de crear (dar valor) a una variable se conoce como declaración de variables y es sensible a mayúsculas y minúsculas. Esto significa que si declaramos las variables nombre, Nombre y NOMBRE serán diferentes.\n\n\nCódigo\nnombre = \"Christian\"\nNombre = \"Jimena\"\nNOMBRE = \"Luis\"\n\nprint(nombre == Nombre)\nprint(Nombre == NOMBRE)\nprint(NOMBRE == nombre)\n\n\nFalse\nFalse\nFalse\n\n\nPor lo cual podemos ver que las variables nombre, Nombre y NOMBRE son diferentes, pues al realizar la comparación entre ellas, el resultado es False, también podemos observar la forma de declarar variables en Python, la cual es nombre_variable = valor_variable.\nPodemos sobrescribir el contenido de una variable en cualquier momento.\n\n\nCódigo\nprint(nombre)\nnombre = \"Cesar\"\nprint(nombre)\n\n\nChristian\nCesar\n\n\nEn el ejemplo anterior, hemos cambiado el contenido de la variable nombre de “Christian” a “Cesar”. En Python, podemos asignar cualquier tipo de dato a una variable. Los tipos de datos más comunes en Python son:\n\nint: Números enteros.\nfloat: Números decimales.\nstr: Cadenas de texto.\nbool: Valores booleanos (verdadero o falso).\nlist: Listas de elementos.\ntuple: Tuplas de elementos.\ndict: Diccionarios de elementos.\n\n\n\n\n\nLos números enteros son números que no tienen parte decimal. En Python, podemos declarar un número entero de la siguiente manera:\n\n\nCódigo\nnumero_entero = 10\nprint(numero_entero)\n\n\n10\n\n\nOtra forma de declarar un número entero es utilizando la función int() para convertir un número decimal o un booleano a un número entero. Veamos un ejemplo:\n\n\nCódigo\nnumero_entero = int(10.5)\nprint(numero_entero)\nnumero_entero = int(10.6)\nprint(numero_entero)\n\nnumero_entero = int(True)\nprint(numero_entero)\n\n\n10\n10\n1\n\n\nLos números decimales son números que tienen parte decimal. En Python, podemos declarar un número decimal de la siguiente manera:\n\n\nCódigo\nnumero_decimal = 10.5\nprint(numero_decimal)\n\n\n10.5\n\n\nOtra forma de declarar un número decimal es utilizando la función float() para convertir un número entero o un booleano a un número decimal.\n\n\nCódigo\nnumero_decimal = float(10)\nprint(numero_decimal)\n\nnumero_decimal = float(True)\nprint(numero_decimal)\n\n\n10.0\n1.0\n\n\nIgualmente podemos utilizar la nota científica para declarar un número decimal en Python con la letra e seguida de un número entero por ejemplo, 22e10 es equivalente a \\(22 * 10^10\\). En código:\n\n\nCódigo\nnumero_decimal = 1.5e2\nprint(numero_decimal)\n\nnumero_decimal = 1.5e-2\nprint(numero_decimal)\n\n\n150.0\n0.015\n\n\n\n\n\nLas cadenas de texto son una secuencia de carácteres (letras, números y símbolos). En Python, se pueden declarar cadenas de texto utilizando comillas simples ' o comillas dobles \". Veamos un ejemplo:\n\n\nCódigo\ncadena_texto = \"Hola Mundo\"\nprint(cadena_texto)\n\n\nHola Mundo\n\n\nOtra forma de declarar una cadena de texto es utilizando la función str() para convertir un número entero, decimal o booleano a una cadena de texto. También podemos combinar las comillas simples y dobles para poder usar una o la otra dentro de la cadena de texto. Por ejemplo:\n\n\nCódigo\ntexto= 'No te procupes, solo es un \"amigo\" / \"amiga\"'\nprint(texto)\n\n\nNo te procupes, solo es un \"amigo\" / \"amiga\"\n\n\nTambén podemos imprimir saltos de línea (\\n), tabulaciones (\\t) y caracteres especiales en una cadena de texto. Por ejemplo:\n\n\nCódigo\ntexto = \"Las rosas son rojas,\\nLas violetas son azules,\\nViva el team frio,\\nQuiero un pan \\U0001F35E\\n\"\n\nprint(texto)\n\ntexto = \"\\n\\t Aqui no se habla\\n\\tdel formato APA\\n\\tNo, no, no 🫠\"\n\nprint(texto)\n\n\nLas rosas son rojas,\nLas violetas son azules,\nViva el team frio,\nQuiero un pan 🍞\n\n\n     Aqui no se habla\n    del formato APA\n    No, no, no 🫠\n\n\n\n\n\nLos valores booleanos son valores lógicos que pueden ser True (verdadero) o False (falso). En Python, podemos declarar un valor booleano de la siguiente manera:\n\n\nCódigo\nvalor_booleano = True\nprint(valor_booleano)\n\n\nTrue\n\n\nIgualmente existe la función bool() pero su uso es poco utilizado, ya que los valores booleanos se utilizan principalmente en condicionales y bucles, además es necesario conocer que cosas se consideran True y False en Python.\n\n\nCódigo\nvalor_booleano = bool(0)\nprint(valor_booleano)\nvalor_booleano = bool(1)\nprint(valor_booleano)\n\nvalor_booleano = bool([])\nprint(valor_booleano)\nvalor_booleano = bool([1, 2, 3])\nprint(valor_booleano)\n\n\nFalse\nTrue\nFalse\nTrue\n\n\nSi recuerdan sus clases de Lógica, podrán crear una tabla de verdad para evaluar proposiciones lógicas en Python.\n\n\n\nLa documentación de código es una práctica común en programación y una buena costumbre para escribir código limpio y legible. En Python, podemos documentar nuestro código utilizando comentarios.\nLos comentarios son líneas de código que no se ejecutan y se utilizan para explicar el código. En Python, los comentarios se crean utilizando el símbolo # y cualquier texto que se escriba después de este símbolo se considera un comentario y no se ejecuta.\n\n\nCódigo\n# Esto es un comentario\nprint(\"Hola Mundo\") # Aquí podemos comentar que hace esta línea de código\n\n\nHola Mundo\n\n\nTambién podemos crear comentarios multilínea utilizando tres comillas simples ''' o tres comillas dobles \"\"\", se utiliza mucho esta forma de comentar para documentar funciones o clases, como lo veremos más adelante.\n\n\n\nEn el siguiente bloque podrán ejecutar código de Python usando su navegador web. Igualmente pueden hacerlo en su computadora o en la nube utilizando Google Colab o Kaggle. Para cada ejercicio deber de escribir su propio código, crear un comentario con sus datos, crear su solución y tomar una captura de pantalla del resultado. Finalmente, subir la captura a la asignación correspondiente en Google Classroom.\n\nDeclarar una variable de cada tipo de dato en Python (entero, decimal, cadena de texto y booleano) e imprimir su valor.\nDeclarar una variable con el nombre poema y asignarle un poema de su elección. Imprimir el poema en la consola con el formato adecuado. (Pueden utilizar emojis si lo desean).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Fundamentos de Python",
      "Declaración de Variables y Operaciones"
    ]
  },
  {
    "objectID": "intro_python/Variables.html#tipos-de-variables",
    "href": "intro_python/Variables.html#tipos-de-variables",
    "title": "Declaración de Variables y Operaciones",
    "section": "",
    "text": "Los números enteros son números que no tienen parte decimal. En Python, podemos declarar un número entero de la siguiente manera:\n\n\nCódigo\nnumero_entero = 10\nprint(numero_entero)\n\n\n10\n\n\nOtra forma de declarar un número entero es utilizando la función int() para convertir un número decimal o un booleano a un número entero. Veamos un ejemplo:\n\n\nCódigo\nnumero_entero = int(10.5)\nprint(numero_entero)\nnumero_entero = int(10.6)\nprint(numero_entero)\n\nnumero_entero = int(True)\nprint(numero_entero)\n\n\n10\n10\n1\n\n\nLos números decimales son números que tienen parte decimal. En Python, podemos declarar un número decimal de la siguiente manera:\n\n\nCódigo\nnumero_decimal = 10.5\nprint(numero_decimal)\n\n\n10.5\n\n\nOtra forma de declarar un número decimal es utilizando la función float() para convertir un número entero o un booleano a un número decimal.\n\n\nCódigo\nnumero_decimal = float(10)\nprint(numero_decimal)\n\nnumero_decimal = float(True)\nprint(numero_decimal)\n\n\n10.0\n1.0\n\n\nIgualmente podemos utilizar la nota científica para declarar un número decimal en Python con la letra e seguida de un número entero por ejemplo, 22e10 es equivalente a \\(22 * 10^10\\). En código:\n\n\nCódigo\nnumero_decimal = 1.5e2\nprint(numero_decimal)\n\nnumero_decimal = 1.5e-2\nprint(numero_decimal)\n\n\n150.0\n0.015\n\n\n\n\n\nLas cadenas de texto son una secuencia de carácteres (letras, números y símbolos). En Python, se pueden declarar cadenas de texto utilizando comillas simples ' o comillas dobles \". Veamos un ejemplo:\n\n\nCódigo\ncadena_texto = \"Hola Mundo\"\nprint(cadena_texto)\n\n\nHola Mundo\n\n\nOtra forma de declarar una cadena de texto es utilizando la función str() para convertir un número entero, decimal o booleano a una cadena de texto. También podemos combinar las comillas simples y dobles para poder usar una o la otra dentro de la cadena de texto. Por ejemplo:\n\n\nCódigo\ntexto= 'No te procupes, solo es un \"amigo\" / \"amiga\"'\nprint(texto)\n\n\nNo te procupes, solo es un \"amigo\" / \"amiga\"\n\n\nTambén podemos imprimir saltos de línea (\\n), tabulaciones (\\t) y caracteres especiales en una cadena de texto. Por ejemplo:\n\n\nCódigo\ntexto = \"Las rosas son rojas,\\nLas violetas son azules,\\nViva el team frio,\\nQuiero un pan \\U0001F35E\\n\"\n\nprint(texto)\n\ntexto = \"\\n\\t Aqui no se habla\\n\\tdel formato APA\\n\\tNo, no, no 🫠\"\n\nprint(texto)\n\n\nLas rosas son rojas,\nLas violetas son azules,\nViva el team frio,\nQuiero un pan 🍞\n\n\n     Aqui no se habla\n    del formato APA\n    No, no, no 🫠\n\n\n\n\n\nLos valores booleanos son valores lógicos que pueden ser True (verdadero) o False (falso). En Python, podemos declarar un valor booleano de la siguiente manera:\n\n\nCódigo\nvalor_booleano = True\nprint(valor_booleano)\n\n\nTrue\n\n\nIgualmente existe la función bool() pero su uso es poco utilizado, ya que los valores booleanos se utilizan principalmente en condicionales y bucles, además es necesario conocer que cosas se consideran True y False en Python.\n\n\nCódigo\nvalor_booleano = bool(0)\nprint(valor_booleano)\nvalor_booleano = bool(1)\nprint(valor_booleano)\n\nvalor_booleano = bool([])\nprint(valor_booleano)\nvalor_booleano = bool([1, 2, 3])\nprint(valor_booleano)\n\n\nFalse\nTrue\nFalse\nTrue\n\n\nSi recuerdan sus clases de Lógica, podrán crear una tabla de verdad para evaluar proposiciones lógicas en Python.\n\n\n\nLa documentación de código es una práctica común en programación y una buena costumbre para escribir código limpio y legible. En Python, podemos documentar nuestro código utilizando comentarios.\nLos comentarios son líneas de código que no se ejecutan y se utilizan para explicar el código. En Python, los comentarios se crean utilizando el símbolo # y cualquier texto que se escriba después de este símbolo se considera un comentario y no se ejecuta.\n\n\nCódigo\n# Esto es un comentario\nprint(\"Hola Mundo\") # Aquí podemos comentar que hace esta línea de código\n\n\nHola Mundo\n\n\nTambién podemos crear comentarios multilínea utilizando tres comillas simples ''' o tres comillas dobles \"\"\", se utiliza mucho esta forma de comentar para documentar funciones o clases, como lo veremos más adelante.\n\n\n\nEn el siguiente bloque podrán ejecutar código de Python usando su navegador web. Igualmente pueden hacerlo en su computadora o en la nube utilizando Google Colab o Kaggle. Para cada ejercicio deber de escribir su propio código, crear un comentario con sus datos, crear su solución y tomar una captura de pantalla del resultado. Finalmente, subir la captura a la asignación correspondiente en Google Classroom.\n\nDeclarar una variable de cada tipo de dato en Python (entero, decimal, cadena de texto y booleano) e imprimir su valor.\nDeclarar una variable con el nombre poema y asignarle un poema de su elección. Imprimir el poema en la consola con el formato adecuado. (Pueden utilizar emojis si lo desean).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Fundamentos de Python",
      "Declaración de Variables y Operaciones"
    ]
  },
  {
    "objectID": "intro_python/Variables.html#operaciones-aritméticas",
    "href": "intro_python/Variables.html#operaciones-aritméticas",
    "title": "Declaración de Variables y Operaciones",
    "section": "Operaciones aritméticas",
    "text": "Operaciones aritméticas\nLas operaciones aritméticas son operaciones matemáticas que se realizan con números. En Python, podemos realizar operaciones aritméticas con variables de tipo entero y decimal, como si fuese una calculadora.\n\n\nCódigo\nfrom math import sqrt\n# Sumemos dos numeros\nnumero1 = 10\nnumero2 = 5\n\nsuma = numero1 + numero2 # 10 + 5 = 15\nprint(suma)\n\n# Restemos dos numeros\nresta = numero1 - numero2 # 10 - 5 = 5\nprint(resta)\n\n# Multipliquemos dos numeros\nmultiplicacion = numero1 * numero2 # 10 * 5 = 50\nprint(multiplicacion)\n\n# Dividamos dos numeros\ndivision = numero1 / numero2 # 10 / 5 = 2.0\nprint(division)\n\n# Dividamos y solo tomemos la parte entera\ndivision_entera = numero1 // numero2 # 10 // 5 = 2\nprint(division_entera)\n\n# Calculemos el residuo de la división\nmodulo = numero1 % numero2 # 10 % 5 = 0\nprint(modulo)\n\n# Elevemos un numero a una potencia\npotencia = numero1 ** numero2 # 10 ^ 5 = 100000\nprint(potencia)\n\n# Calculemos la raiz cuadrada de un numero\nraiz_cuadrada = numero1 ** 0.5 # sqrt(10) = 3.1622776601683795\nraiz_cuadrada2 = sqrt(numero2) # sqrt(5) = 2.23606797749979\nprint(raiz_cuadrada)\nprint(raiz_cuadrada2)\n\n\n15\n5\n50\n2.0\n2\n0\n100000\n3.1622776601683795\n2.23606797749979",
    "crumbs": [
      "Fundamentos de Python",
      "Declaración de Variables y Operaciones"
    ]
  },
  {
    "objectID": "intro_python/Variables.html#operaciones-de-comparación",
    "href": "intro_python/Variables.html#operaciones-de-comparación",
    "title": "Declaración de Variables y Operaciones",
    "section": "Operaciones de comparación",
    "text": "Operaciones de comparación\nLas operaciones de comparación son operaciones que se utilizan para comparar dos valores y devuelven un valor booleano (True o False).\n\n\nCódigo\n# Comparar si dos numeros son iguales\nnumero1 = 148\nnumero2 = 47\n\ncomparacion = numero1 == numero2\nprint(comparacion)\n\n# Comparar si dos numeros son diferentes\ncomparacion = numero1 != numero2\nprint(comparacion)\n\n# Comparar si un numero es mayor que otro\ncomparacion = numero1 &gt; numero2\nprint(comparacion)\n\n# Comparar si un numero es menor que otro\ncomparacion = numero1 &lt; numero2\nprint(comparacion)\n\n# Comparar si un numero es mayor o igual que otro\ncomparacion = numero1 &gt;= numero2\nprint(comparacion)\n\n# Comparar si un numero es menor o igual que otro\ncomparacion = numero1 &lt;= numero2\nprint(comparacion)\n\n\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse",
    "crumbs": [
      "Fundamentos de Python",
      "Declaración de Variables y Operaciones"
    ]
  },
  {
    "objectID": "intro_python/Variables.html#operaciones-con-cadenas-de-texto",
    "href": "intro_python/Variables.html#operaciones-con-cadenas-de-texto",
    "title": "Declaración de Variables y Operaciones",
    "section": "Operaciones con cadenas de texto",
    "text": "Operaciones con cadenas de texto\nLas operaciones con cadenas de texto son operaciones que se utilizan para concatenar, multiplicar y comparar cadenas de texto. Se utilizan principalmente para manipular texto en Python.\nPara concatenar dos cadenas de texto, simplemente se utilizan el operador +.\n\n\nCódigo\n# Concatenar dos cadenas de texto\ncadena1 = \"Hola\"\ncadena2 = \"Mundo\"\n\nconcatenacion = cadena1 + \" \" + cadena2\nprint(concatenacion)\n\n\nHola Mundo\n\n\nPara multiplicar una cadena de texto por un número entero, simplemente se utiliza el operador *.\n\n\nCódigo\n# Multiplicar una cadena de texto\ncadena = \"Me gusta programar en Python\\n\"\n\nmultiplicacion = cadena * 5\nprint(multiplicacion)\n\n\nMe gusta programar en Python\nMe gusta programar en Python\nMe gusta programar en Python\nMe gusta programar en Python\nMe gusta programar en Python\n\n\n\nPara comparar dos cadenas de texto, simplemente se utilizan los operadores de comparación.\n\n\nCódigo\n# Comparar dos cadenas de texto\ncadena1 = \"Muchas gracias\"\ncadena2 = \"De nada\"\n\ncomparacion = cadena1 == cadena2 # Comparar si dos cadenas son iguales\nprint(comparacion)\n\ncomparacion = cadena1 != cadena2 # Comparar si dos cadenas son diferentes\nprint(comparacion)\n\ncomparacion = cadena1 &gt; cadena2 # Comparar si una cadena tiene más caracteres que otra\nprint(comparacion)\n\ncomparacion = cadena1 &lt; cadena2 # Comparar si una cadena tiene menos caracteres que otra\nprint(comparacion)\n\ncomparacion = cadena1 &gt;= cadena2 # Comparar si una cadena tiene más o igual cantidad de caracteres que otra\nprint(comparacion)\n\ncomparacion = cadena1 &lt;= cadena2 # Comparar si una cadena tiene menos o igual cantidad de caracteres que otra\nprint(comparacion)\n\n\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n\nEjercicios 1.2\n\n Instrucciones \n\nEn el siguiente bloque podrán ejecutar código de Python usando su navegador web. Igualmente pueden hacerlo en su computadora o en la nube utilizando Google Colab o Kaggle. Para cada ejercicio deber de escribir su propio código, crear un comentario con sus datos, crear su solución y tomar una captura de pantalla del resultado. Finalmente, subir la captura a la asignación correspondiente en Google Classroom.\n\nSabiendo que el Teorema de Pitágoras es \\(a^2 + b^2 = c^2\\), calcular la hipotenusa de un triángulo rectángulo con catetos de 3 y 4 unidades.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSi la velocidad de la luz es de \\(299,792,458\\) m/s, y el tiempo que tarda en llegar la luz del Sol a la Tierra es de \\(8.3\\) minutos, calcular la distancia en kilómetros que hay entre el Sol y la Tierra.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCrea dos variables con tu nombre y apellido, y concaténalas en una sola variable. Imprime el resultado.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCrea una variable con tu canción favorita y multiplica por 4 la parte de la letra que más te gusta. Imprime el resultado.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCrea una variable con tu película favorita y compara si la longitud de la cadena de texto es mayor a 10 caracteres. Imprime el resultado.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Fundamentos de Python",
      "Declaración de Variables y Operaciones"
    ]
  },
  {
    "objectID": "intro_python/data_structures.html",
    "href": "intro_python/data_structures.html",
    "title": "Estructuras de Datos",
    "section": "",
    "text": "Los datos son la base de la ciencia, son la materia prima con la que trabajamos. Dada su importancia es necesario contar con herramientas que nos permitan manipularlos de manera eficiente. En este sentido, las estructuras de datos son una herramienta fundamental en la programación. Se definen como formas de organizar y almacenar datos en la memoria de un computador, de tal manera que se puedan acceder y modificar de manera eficiente.\nEn esta lección veremos algunas de las estructuras de datos más comunes en Python, como las listas, tuplas, conjuntos, diccionarios, arreglos de NumPy y DataFrames de Pandas, así como las propiedades y métodos que las caracterizan.",
    "crumbs": [
      "Fundamentos de Python",
      "Estructuras de Datos"
    ]
  },
  {
    "objectID": "intro_python/data_structures.html#indexación",
    "href": "intro_python/data_structures.html#indexación",
    "title": "Estructuras de Datos",
    "section": "Indexación",
    "text": "Indexación\nPara acceder a elementos de una lista se utiliza la indexación. Los índices en Python comienzan en 0, por lo que el primer elemento de una lista tiene índice 0, el segundo índice 1 y así sucesivamente. Para acceder a un elemento de una lista se utiliza la sintaxis nombredelista[indice].\n\n\nCódigo\nprint(lista[0])\nprint(lista[1])\nprint(lista[2])\n\n# Acceder al último elemento\nprint(lista[-1])\n\n\n1\n2\n3\nhola",
    "crumbs": [
      "Fundamentos de Python",
      "Estructuras de Datos"
    ]
  },
  {
    "objectID": "intro_python/data_structures.html#slicing",
    "href": "intro_python/data_structures.html#slicing",
    "title": "Estructuras de Datos",
    "section": "Slicing",
    "text": "Slicing\nEl slicing es una técnica que permite acceder a subconjuntos de elementos de una lista. Se utiliza la sintaxis nombredelista[inicio:fin], donde inicio es el índice del primer elemento que se quiere incluir y fin es el índice del primer elemento que no se quiere incluir.\n\n\nCódigo\nprint(lista[0:3])\nprint(lista[3:])\n\n\n[1, 2, 3]\n[4, 5, 6, [7, 8, 9], 'hola']\n\n\nSi no se especifica el índice de inicio, se asume que es 0. Si no se especifica el índice de fin, se asume que es el último elemento de la lista.",
    "crumbs": [
      "Fundamentos de Python",
      "Estructuras de Datos"
    ]
  },
  {
    "objectID": "intro_python/data_structures.html#modificación",
    "href": "intro_python/data_structures.html#modificación",
    "title": "Estructuras de Datos",
    "section": "Modificación",
    "text": "Modificación\nPara modificar un elemento de una lista se utiliza la indexación y la asignación.\n\n\nCódigo\nlista[0] = 10\nprint(lista)\n\n\n[10, 2, 3, 4, 5, 6, [7, 8, 9], 'hola']",
    "crumbs": [
      "Fundamentos de Python",
      "Estructuras de Datos"
    ]
  },
  {
    "objectID": "intro_python/data_structures.html#eliminación",
    "href": "intro_python/data_structures.html#eliminación",
    "title": "Estructuras de Datos",
    "section": "Eliminación",
    "text": "Eliminación\nPara eliminar un elemento de una lista se utiliza el método remove() y se especifica el elemento del elemento que se quiere eliminar.\n\n\nCódigo\nlista.remove(10)\nprint(lista)\n\n\n[2, 3, 4, 5, 6, [7, 8, 9], 'hola']\n\n\nSi queremos eliminar el último elemento de una lista se puede utilizar el método pop().\n\n\nCódigo\nlista.pop()\nprint(lista)\n\n\n'hola'\n\n\n[2, 3, 4, 5, 6, [7, 8, 9]]",
    "crumbs": [
      "Fundamentos de Python",
      "Estructuras de Datos"
    ]
  },
  {
    "objectID": "intro_python/data_structures.html#resumen",
    "href": "intro_python/data_structures.html#resumen",
    "title": "Estructuras de Datos",
    "section": "Resumen",
    "text": "Resumen\nLas listas tienen un gran número de métodos que permiten realizar operaciones sobre ellas. Un resumen de los métodos más comunes es el siguiente:\n\nappend(): Agrega un elemento al final de la lista.\nextend(): Agrega varios elementos al final de la lista.\ninsert(): Agrega un elemento en una posición específica.\nremove(): Elimina un elemento de la lista.\npop(): Elimina un elemento de la lista y lo devuelve.\nindex(): Devuelve el índice de un elemento en la lista.\ncount(): Cuenta el número de veces que un elemento aparece en la lista.\nsort(): Ordena los elementos de la lista en orden ascendente.\nreverse(): Invierte el orden de los elementos de la lista.\ncopy(): Crea una copia de la lista.\n\nVale la pena hacer enfásis en que no es necesario aprenderse todos los métodos de memoria, sino que es más importante entender para qué y cómo se pueden utilizar las listas, los métodos se pueden consultar en la documentación de Python, buscando en Google o preguntandole a ChatGPT.",
    "crumbs": [
      "Fundamentos de Python",
      "Estructuras de Datos"
    ]
  },
  {
    "objectID": "intro_python/data_structures.html#ejericios",
    "href": "intro_python/data_structures.html#ejericios",
    "title": "Estructuras de Datos",
    "section": "Ejericios",
    "text": "Ejericios\n\nCrea una lista con los números del 1 al 10 y calcula la suma de los elementos. Pista: utiliza la función sum() de Python.\nCrea una tupla con los nombres de los días de la semana y accede al tercer día. Intenta modificar el tercer día y observa qué sucede.\nCrea una variable con la letra de una canción que te guste, separa las palabras en una lista, guarda esa lista en una variable y accede a la primera y última palabra. Pista: utiliza el método split() de Python.\nConvierte la lista de palabras en un conjunto y observa qué sucede. ¿Qué diferencias encuentras entre la lista y el conjunto? Intenta agregar una palabra al conjunto y observa qué sucede. Pista: utiliza la función set() de Python y el método add() de los conjuntos.\nCrea una función para contar palabras en un texto, cuenta las palabras distintas en la letra de una canción que te guste y almacenalas en un diccionario con la palabra como clave y el número de veces que aparece como valor. Pista: utiliza el método split() para separar las palabras",
    "crumbs": [
      "Fundamentos de Python",
      "Estructuras de Datos"
    ]
  },
  {
    "objectID": "intro_python/data_structures.html#slicing-en-dataframes",
    "href": "intro_python/data_structures.html#slicing-en-dataframes",
    "title": "Estructuras de Datos",
    "section": "Slicing en DataFrames",
    "text": "Slicing en DataFrames\nPor ahora veamos cómo acceder a elementos de un DataFrame. Podemos acceder a las primeras y últimas filas de un DataFrame utilizando los métodos head() y tail(), respectivamente\n\n\nCódigo\n# Cargo un conjunto de datos de ejemplo\ndf = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv') \n\ndf.head() # Imprimir las primeras filas del DataFrame\n\ndf.tail() # Imprimir las últimas filas del DataFrame\n\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\n\n\n\nPodemos acceder a las columnas de un DataFrame utilizando la indexación con corchetes [] y el nombre de la columna entre comillas simples o dobles.\n\n\nCódigo\ndf['sepal_length'] # Acceder a la columna 'sepal_length'\n\n\n0      5.1\n1      4.9\n2      4.7\n3      4.6\n4      5.0\n      ... \n145    6.7\n146    6.3\n147    6.5\n148    6.2\n149    5.9\nName: sepal_length, Length: 150, dtype: float64\n\n\nOtra manera de acceder a una columna es utilizando la notación de punto . y el nombre de la columna, pero esta forma no es muy usada dado que complica el acceso a columnas con nombres que contienen espacios o caracteres especiales.\n\n\nCódigo\ndf.sepal_length # Acceder a la columna 'sepal_length'\n\n\n0      5.1\n1      4.9\n2      4.7\n3      4.6\n4      5.0\n      ... \n145    6.7\n146    6.3\n147    6.5\n148    6.2\n149    5.9\nName: sepal_length, Length: 150, dtype: float64\n\n\nTambién podemos acceder a filas y columnas utilizando la función loc[] y los nombres de las filas y columnas.\n\n\nCódigo\ndf.loc[:, 'sepal_length'] # Acceder al elemento en la fila 0 y columna 'sepal_length'\n# Recuerden que si usamos : estamos seleccionando todas las filas, primero se seleccionan las filas y luego las columnas\n\n\n0      5.1\n1      4.9\n2      4.7\n3      4.6\n4      5.0\n      ... \n145    6.7\n146    6.3\n147    6.5\n148    6.2\n149    5.9\nName: sepal_length, Length: 150, dtype: float64\n\n\nSi queremos seleccionar varias columnas podemos pasar una lista con los nombres de las columnas.\n\n\nCódigo\ncol_names = ['sepal_length', 'sepal_width']\ndf.loc[:, col_names] # Acceder a las columnas 'sepal_length' y 'sepal_width'\n\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\n\n\n\n\n0\n5.1\n3.5\n\n\n1\n4.9\n3.0\n\n\n2\n4.7\n3.2\n\n\n3\n4.6\n3.1\n\n\n4\n5.0\n3.6\n\n\n...\n...\n...\n\n\n145\n6.7\n3.0\n\n\n146\n6.3\n2.5\n\n\n147\n6.5\n3.0\n\n\n148\n6.2\n3.4\n\n\n149\n5.9\n3.0\n\n\n\n\n150 rows × 2 columns\n\n\n\n\nSi queremos seleccionar las filas 5, 14, 23 y las columnas ‘sepal_length’ y ‘sepal_width’ podemos hacerlo de la siguiente manera.\n\n\nCódigo\nfilas = [5, 14, 23]\ncol_names = ['sepal_length', 'sepal_width']\n\ndf.loc[filas, col_names] # Acceder a las filas 5, 14, 23 y las columnas 'sepal_length' y 'sepal_width'\n\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\n\n\n\n\n5\n5.4\n3.9\n\n\n14\n5.8\n4.0\n\n\n23\n5.1\n3.3",
    "crumbs": [
      "Fundamentos de Python",
      "Estructuras de Datos"
    ]
  },
  {
    "objectID": "intro_python/data_structures.html#modificación-de-dataframes",
    "href": "intro_python/data_structures.html#modificación-de-dataframes",
    "title": "Estructuras de Datos",
    "section": "Modificación de DataFrames",
    "text": "Modificación de DataFrames\nPodemos agregar una nueva columna a un DataFrame utilizando la notación de corchetes [] y el nombre de la nueva columna y asignándole una lista con los valores de la columna.\nCreemos una nueva columna que contenga la suma de las columnas ‘sepal_length’ y ‘sepal_width’.\n\n\nCódigo\ndf['sepal_sum'] = df['sepal_length'] + df['sepal_width']\ndf.head()\n\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\nsepal_sum\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n8.6\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n7.9\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n7.9\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n7.7\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n8.6\n\n\n\n\n\n\n\n\nPara eliminar una columna de un DataFrame se utiliza el método drop() y se pasa el nombre de la columna y el eje en el que se quiere eliminar, los ejes son 0 para filas y 1 para columnas.\n\n\nCódigo\ndf.drop('sepal_sum', axis=1, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\nEl parámetro inplace=True indica que la operación se realice sobre el DataFrame original, si se omite se crea una copia del DataFrame con la columna eliminada, pero si esta copia no se asigna a una variable se perderá.\nSi queremos modificar un valor de un DataFrame se localiza el elemento con loc[] y se asigna el nuevo valor.\n\n\nCódigo\ndf.head()\ndf.loc[0, 'sepal_length'] = 10\ndf.head()\n\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n10.0\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa",
    "crumbs": [
      "Fundamentos de Python",
      "Estructuras de Datos"
    ]
  },
  {
    "objectID": "proyecto.html",
    "href": "proyecto.html",
    "title": "Proyecto Final: Ciencia de Datos en Python",
    "section": "",
    "text": "Instructor: Christian Badillo\nIntegrantes del equipo:"
  },
  {
    "objectID": "proyecto.html#descripción-del-proyecto",
    "href": "proyecto.html#descripción-del-proyecto",
    "title": "Proyecto Final: Ciencia de Datos en Python",
    "section": "Descripción del proyecto",
    "text": "Descripción del proyecto\nInstrucciones para el proyecto final del curso de Ciencia de Datos en Python.\nEn este proyecto trabajarán en diversos problemas para poner en práctica los conocimientos adquiridos en el curso. Los problemas están diseñados para que apliquen las técnicas de ciencia de datos aprendidas en el curso y que resuelvan problemas reales similares a los que se enfrentarían en un trabajo de ciencia de datos. La dificultad de los problemas es variado, siendo la parte 1 la más complicada, la parte 2 la más sencilla y la parte 3 intermedia. Deben de responder al menos 2 de las 3 partes del proyecto.\nSe calificara la limpieza de los datos, el uso de las técnicas de ciencia de datos, la calidad de las gráficas y la presentación de los resultados. Además de la claridad y calidad del código."
  },
  {
    "objectID": "proyecto.html#parte-1-limpieza-manejo-exploración-visualización-y-análisis-de-regresión-de-datos",
    "href": "proyecto.html#parte-1-limpieza-manejo-exploración-visualización-y-análisis-de-regresión-de-datos",
    "title": "Proyecto Final: Ciencia de Datos en Python",
    "section": "Parte 1: Limpieza, Manejo, Exploración, Visualización y Análisis de Regresión de Datos",
    "text": "Parte 1: Limpieza, Manejo, Exploración, Visualización y Análisis de Regresión de Datos\nEn esta primera parte aplicarán los conocimientos adquiridos en el curso para limpiar, manejar, explorar, visualizar y analizar un conjunto de datos, los datos a utilizar son los de la competencia de predicción de precios de viviendas de Kaggle.\n\nRealicen una limpieza de los datos.\nRealicen un análisis descriptivo de los datos.\nRealicen un análisis visual de los datos de la relación de cada variable con la variable objetivo.\nRealicen un análisis de regresión de los datos.\nInterpreten los resultados obtenidos.\nPredigan los precios de las viviendas utilizando regresión lineal para los datos de prueba (Verifiquen la integridad de los datos de prueba).\n\n\n\nCódigo\nimport pandas as pd\n\n# Url de datos de entrenamiento\nurl_train = 'https://raw.githubusercontent.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/main/data/house-prices-advanced-regression-techniques/train.csv'\n\n# Url de datos de prueba\nurl_test = 'https://raw.githubusercontent.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/main/data/house-prices-advanced-regression-techniques/test.csv'\n\ndata_train = pd.read_csv(url_train)\n\ndata_train.head(10)\n\n\n\n\nCódigo\ndata_predict = pd.read_csv(url_test)\n\ndata_predict.head(10)"
  },
  {
    "objectID": "proyecto.html#parte-2-pruebas-de-hipótesis",
    "href": "proyecto.html#parte-2-pruebas-de-hipótesis",
    "title": "Proyecto Final: Ciencia de Datos en Python",
    "section": "Parte 2: Pruebas de Hipótesis",
    "text": "Parte 2: Pruebas de Hipótesis\nUsando los datos del rendimiento acádemico de los estudiantes, realicen pruebas de hipótesis para responder a las siguientes preguntas:\n\n¿Existe evidencia de mejoria en la puntuación de matemáticas para los estudiantes que tomaron un curso de preparación para el examen? ¿Para lectura? ¿Para escritura?\n¿Existe evidencia de que los estudiantes con almuerzo gratis tienen un rendimiento acádemico menor que los que tienen un almuerzo estándar? Toma el rendimiento como la media de las tres materias.\n¿Existe evidencia de que el rendimiento acádemico es distinto entre los grupos étnicos? ¿\n¿Existe evidencia de que los estudiantes de género femenino tienen un rendimiento acádemico distinto al de los estudiantes de género masculino?\n¿Hay evidencia de que el nivel de educación de los padres afecta el rendimiento de los estudiantes?\n\nInterpreten los resultados y saquen conclusiones de cada una de las pruebas y justifiquen el uso de la prueba que utilizaron.\n\n\nCódigo\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/main/data/hyphotesis_testing/StudentsPerformance.csv\"\n\ndata = pd.read_csv(url)\ndata.head(10)"
  },
  {
    "objectID": "proyecto.html#parte-3-clasificación",
    "href": "proyecto.html#parte-3-clasificación",
    "title": "Proyecto Final: Ciencia de Datos en Python",
    "section": "Parte 3: Clasificación",
    "text": "Parte 3: Clasificación\nUsando los datos de los hongos, realicen un análisis de clasificación para predecir si un pasajero sobrevive o no sobrevive, utilizando los siguientes modelos:\n\nRegresión Logística.\nRedes Neuronales.\n\nDividan los datos en entrenamiento y prueba en una proporción de 70% y 30% respectivamente. Usen la función train_test_split de sklearn para dividir los datos.\n\nRealicen un análisis descriptivo de los datos.\nRealicen un análisis visual de los datos.\nRealicen la evaluación de los modelos usando la matriz de confusión y las métricas de precisión, exactitud, sensibilidad y especificidad. Interpreten los resultados obtenidos. ¿Cuál modelo es mejor? Justifiquen su respuesta.\n\n\n\nCódigo\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/main/data/mushroom_cleaned.csv\"\n\ndata = pd.read_csv(url)\n\ndata.head(10)"
  },
  {
    "objectID": "intro_python/Cloud.html",
    "href": "intro_python/Cloud.html",
    "title": "Instalación de Python",
    "section": "",
    "text": "Python  es un lenguaje de programación de alto nivel, interpretado y orientado a objetos con semántica dinámica. Es un lenguaje de programación muy popular en la actualidad debido a su simplicidad y versatilidad. Y ampliamente utilizado en la ciencia de datos, inteligencia artificial, desarrollo web, desarrollo de videojuegos, entre otros.\nVamos a deshilar un poco la definición anterior para entender mejor lo que significa.\n\n\nUn lenguaje de programación de alto nivel es un lenguaje de programación que se encuentra más cerca del lenguaje humano que del lenguaje de máquina. Por ejemplo para imprimir el resultado de la suma de dos números en Python se escribe:\n\n\nCódigo\na = 5\nb = 3\n\nprint(a + b)\n\n\n8\n\n\nCualquier persona que entienda aritmética básica y sepa un poco de inglés puede entender lo que hace este código. Esto hace que los programas escritos en Python sean más fáciles de leer y de escribir que los programas escritos en lenguajes de programación de bajo nivel como el lenguaje ensamblador  o el lenguaje C++  . Por ejemplo en C++ el mismo programa se escribiría de la siguiente forma:\n#include &lt;iostream&gt;\n\nint main() {\n    int a = 5;\n    int b = 3;\n    std::cout &lt;&lt; a + b &lt;&lt; std::endl;\n    return 0;\n} \nY en lenguaje ensamblador se escribiría de la siguiente forma:\nsection .data\n    a db 5\n    b db 3\nsection .text\n    global _start\n_start:\n    mov al, [a]\n    mov bl, [b]\n    add al, bl\n    mov dl, al\n    mov eax, 4\n    mov ebx, 1\n    mov ecx, dl\n    mov edx, 1\n    int 0x80\n    mov eax, 1\n    xor ebx, ebx\n    int 0x80\n\n\n\nUn lenguaje de programación interpretado es un lenguaje de programación que utiliza un programa llamado intérprete para ejecutar el código fuente. El intérprete de Python se encarga de leer el código fuente escrito en Python y ejecutarlo línea por línea.\nPor otro lado, los lenguajes de programación compilados utilizan un programa llamado compilador para traducir el código fuente a un lenguaje de máquina que la computadora pueda entender. El código fuente compilado se convierte en un archivo ejecutable que puede ser ejecutado por la computadora lo que da como resultado un programa más rápido que un programa interpretado y con mayor control sobre el hardware.\n\n\n\nPython es un lenguaje de programación orientado a objetos lo que significa que todo en Python es un objeto. Un objeto es una entidad que tiene un estado y un comportamiento. Por ejemplo, un objeto de la clase Persona tiene un estado que incluye el nombre, la edad y la dirección de la persona y un comportamiento que incluye caminar, hablar y comer.\nEl ejemplo más claro de la orientación a objetos en Python es la creación de clases y objetos. Una clase es una plantilla que define las propiedades y los métodos de un objeto y un objeto es una instancia de una clase. Por ejemplo, la clase Persona define las propiedades y los métodos de una persona y un objeto de la clase Persona es una persona en particular.\nEsto en Python se ve de la siguiente forma:\n\n\nCódigo\nclass Persona:\n    def __init__(self, nombre, edad, direccion):\n        self.nombre = nombre\n        self.edad = edad\n        self.direccion = direccion\n\n    def caminar(self):\n        print(f\"{self.nombre} está caminando\")\n\n    def hablar(self):\n        print(f\"{self.nombre} está hablando\")\n\n    def comer(self):\n        print(f\"{self.nombre} está comiendo\")\n\npersona = Persona(\"Juan\", 25, \"Calle 123\")\npersona.caminar()\npersona.hablar()\npersona.comer()\n\n\nJuan está caminando\nJuan está hablando\nJuan está comiendo\n\n\n\n\n\nPython es un lenguaje de programación con semántica dinámica lo que significa que el tipo de una variable se determina en tiempo de ejecución y no en tiempo de compilación. Por ejemplo, en Python no es necesario declarar el tipo de una variable antes de utilizarla como en otros lenguajes de programación como C++ o Java. Por ejemplo, en Python se puede escribir:\n\n\nCódigo\na = 5\nb = \"Hola\"\nc = 3.14\n\n\nEn este caso, Python determina que a es un entero, b es una cadena de texto y c es un número de punto flotante en tiempo de ejecución.\nEn otro lenguaes de programación como C++ o Java se tendría que declarar el tipo de las variables antes de utilizarlas. Por ejemplo, en C++ se tendría que escribir:\nint a = 5;\nstd::string b = \"Hola\";\nfloat c = 3.14;\nEl tipo puede ser modificado en cualquier momento, por ejemplo:\n\n\nCódigo\na = 5\nprint(a)\na = \"Hola\"\nprint(a)\na = 3.14\nprint(a)\n\n\n5\nHola\n3.14",
    "crumbs": [
      "Fundamentos de Python",
      "Instalación de Python"
    ]
  },
  {
    "objectID": "intro_python/Cloud.html#interpretado",
    "href": "intro_python/Cloud.html#interpretado",
    "title": "Instalación de Python",
    "section": "",
    "text": "Un lenguaje de programación interpretado es un lenguaje de programación que utiliza un programa llamado intérprete para ejecutar el código fuente. El intérprete de Python se encarga de leer el código fuente escrito en Python y ejecutarlo línea por línea.\nPor otro lado, los lenguajes de programación compilados utilizan un programa llamado compilador para traducir el código fuente a un lenguaje de máquina que la computadora pueda entender. El código fuente compilado se convierte en un archivo ejecutable que puede ser ejecutado por la computadora lo que da como resultado un programa más rápido que un programa interpretado y con mayor control sobre el hardware.",
    "crumbs": [
      "Fundamentos de Python",
      "Instalación de Python"
    ]
  },
  {
    "objectID": "intro_python/Cloud.html#orientado-a-objetos",
    "href": "intro_python/Cloud.html#orientado-a-objetos",
    "title": "Instalación de Python",
    "section": "",
    "text": "Python es un lenguaje de programación orientado a objetos lo que significa que todo en Python es un objeto. Un objeto es una entidad que tiene un estado y un comportamiento. Por ejemplo, un objeto de la clase Persona tiene un estado que incluye el nombre, la edad y la dirección de la persona y un comportamiento que incluye caminar, hablar y comer.\nEl ejemplo más claro de la orientación a objetos en Python es la creación de clases y objetos. Una clase es una plantilla que define las propiedades y los métodos de un objeto y un objeto es una instancia de una clase. Por ejemplo, la clase Persona define las propiedades y los métodos de una persona y un objeto de la clase Persona es una persona en particular.\nEsto en Python se ve de la siguiente forma:\n\n\nCódigo\nclass Persona:\n    def __init__(self, nombre, edad, direccion):\n        self.nombre = nombre\n        self.edad = edad\n        self.direccion = direccion\n\n    def caminar(self):\n        print(f\"{self.nombre} está caminando\")\n\n    def hablar(self):\n        print(f\"{self.nombre} está hablando\")\n\n    def comer(self):\n        print(f\"{self.nombre} está comiendo\")\n\npersona = Persona(\"Juan\", 25, \"Calle 123\")\npersona.caminar()\npersona.hablar()\npersona.comer()\n\n\nJuan está caminando\nJuan está hablando\nJuan está comiendo",
    "crumbs": [
      "Fundamentos de Python",
      "Instalación de Python"
    ]
  },
  {
    "objectID": "intro_python/Cloud.html#semántica-dinámica",
    "href": "intro_python/Cloud.html#semántica-dinámica",
    "title": "Instalación de Python",
    "section": "",
    "text": "Python es un lenguaje de programación con semántica dinámica lo que significa que el tipo de una variable se determina en tiempo de ejecución y no en tiempo de compilación. Por ejemplo, en Python no es necesario declarar el tipo de una variable antes de utilizarla como en otros lenguajes de programación como C++ o Java. Por ejemplo, en Python se puede escribir:\n\n\nCódigo\na = 5\nb = \"Hola\"\nc = 3.14\n\n\nEn este caso, Python determina que a es un entero, b es una cadena de texto y c es un número de punto flotante en tiempo de ejecución.\nEn otro lenguaes de programación como C++ o Java se tendría que declarar el tipo de las variables antes de utilizarlas. Por ejemplo, en C++ se tendría que escribir:\nint a = 5;\nstd::string b = \"Hola\";\nfloat c = 3.14;\nEl tipo puede ser modificado en cualquier momento, por ejemplo:\n\n\nCódigo\na = 5\nprint(a)\na = \"Hola\"\nprint(a)\na = 3.14\nprint(a)\n\n\n5\nHola\n3.14",
    "crumbs": [
      "Fundamentos de Python",
      "Instalación de Python"
    ]
  },
  {
    "objectID": "intro_python/Cloud.html#macos",
    "href": "intro_python/Cloud.html#macos",
    "title": "Instalación de Python",
    "section": "MacOs ",
    "text": "MacOs",
    "crumbs": [
      "Fundamentos de Python",
      "Instalación de Python"
    ]
  },
  {
    "objectID": "intro_python/Cloud.html#windows",
    "href": "intro_python/Cloud.html#windows",
    "title": "Instalación de Python",
    "section": "Windows ",
    "text": "Windows",
    "crumbs": [
      "Fundamentos de Python",
      "Instalación de Python"
    ]
  },
  {
    "objectID": "intro_python/Cloud.html#instalar-visual-studio-code",
    "href": "intro_python/Cloud.html#instalar-visual-studio-code",
    "title": "Instalación de Python",
    "section": "Instalar Visual Studio Code ",
    "text": "Instalar Visual Studio Code \nLa instalación de Visual Studio Code es muy sencilla, solo debes de ir a la página oficial de Visual Studio Code y descargar el instalador correspondiente a tu sistema operativo, pero si prefieres ver un video que te guíe en la instalación aquí te dejo uno para MacOS y otro para Windows.\n\nMacOS \n\n\n\nWindows",
    "crumbs": [
      "Fundamentos de Python",
      "Instalación de Python"
    ]
  },
  {
    "objectID": "intro_python/functions_and_loops.html",
    "href": "intro_python/functions_and_loops.html",
    "title": "Funciones y Bucles de Control",
    "section": "",
    "text": "En esta lección, aprenderemos sobre funciones y estructuras de control en Python. Las funciones son bloques de código que se pueden reutilizar en diferentes partes de un programa, permitiendo que el código sea más modular y fácil de mantener. Las estructuras de control son bloques de código que permiten controlar el flujo de ejecución de un programa, como bucles y declaraciones condicionales. Son dos conceptos fundamentales en programación que nos permiten escribir código más eficiente y legible.",
    "crumbs": [
      "Fundamentos de Python",
      "Funciones y Bucles de Control"
    ]
  },
  {
    "objectID": "intro_python/functions_and_loops.html#funciones-lambda",
    "href": "intro_python/functions_and_loops.html#funciones-lambda",
    "title": "Funciones y Bucles de Control",
    "section": "Funciones Lambda",
    "text": "Funciones Lambda\nLas funciones lambda son funciones anónimas que se pueden definir en una sola línea. Se definen con la palabra clave lambda, seguida de una lista de parámetros y una expresión que se evalúa y devuelve el resultado. Por ejemplo, la siguiente función lambda calcula el cuadrado de un número:\n\n\nCódigo\ncuadrado_lamda = lambda x: x ** 2\n\nprint(cuadrado_lamda(5))\n\n\n25\n\n\nLas funciones lambda son útiles cuando se necesita una función simple y no es necesario definirla con una declaración def.\nAhora definamos una función lambda un poco más compleja para sumar dos números:\n\n\nCódigo\nsuma_lambda = lambda x, y: x + y\n\nprint(suma_lambda(3, 4))\n\n\n7",
    "crumbs": [
      "Fundamentos de Python",
      "Funciones y Bucles de Control"
    ]
  },
  {
    "objectID": "intro_python/functions_and_loops.html#paquetes-y-módulos",
    "href": "intro_python/functions_and_loops.html#paquetes-y-módulos",
    "title": "Funciones y Bucles de Control",
    "section": "Paquetes y Módulos",
    "text": "Paquetes y Módulos\nEn Python, las funciones se pueden organizar en módulos y paquetes para facilitar la reutilización y la organización del código. Un módulo es un archivo que contiene definiciones y declaraciones de Python, como funciones, clases y variables. Un paquete es una colección de módulos organizados en un directorio. Es decir, que un paquete es una carpeta que contiene archivos de python (módulos) que contienen funciones, clases y variables que puedes utilizar en tu programa.\nHay paqueterias y modulos muy útiles que se pueden importar en Python, por ejemplo, la paquetería math que contiene funciones matemáticas comunes, como sqrt para calcular la raíz cuadrada de un número. Para importar un módulo o una función de un módulo, se utiliza la palabra clave import seguida del nombre del módulo.\nimport math\nUna vez que se ha importado un módulo, se puede acceder a sus funciones y variables utilizando la notación de punto. Por ejemplo, para calcular la raíz cuadrada de un número, se puede utilizar la función sqrt del módulo math.\n\n\nCódigo\nimport math\nprint(math.sqrt(16))\n\n\n4.0\n\n\nTambién es posible importar funciones específicas de un módulo utilizando la palabra clave from. Por ejemplo, para importar solo la función sqrt del módulo math, se puede hacer lo siguiente:\n\n\nCódigo\nfrom math import sqrt\n\nprint(sqrt(16))\n\n\n4.0\n\n\nMuchas veces las paqueterias tienen nombres muy largos, por lo que se pueden importar con un alias utilizando la palabra clave as, las paqueterias más comunes en python tienen alias ya acordados en la comunidad, por ejemplo, pandas una paquetería muy utilizada para el análisis de datos se importa con el alias pd.\nimport pandas as pd\nDe esta forma, se puede acceder a las funciones y variables de la paquetería pandas utilizando el alias pd.\nExiste un gran número de paqueterías y módulos disponibles en Python que proporcionan funcionalidades adicionales para tareas específicas, como el análisis de datos, la visualización, el aprendizaje automático, etc. Estas paqueterías pueden ser muy útiles para acelerar el desarrollo de aplicaciones y realizar tareas complejas de manera eficiente.\nNo todas las paqueterias vienen preinstaladas con Python, por lo que es necesario instalarlas con un administrador de paquetes, el más común es pip. Para instalar una paquetería con pip, se utiliza el siguiente comando en la terminal:\npip install nombre_paqueteria\nO si se usa jupyter notebook o jupyter lab, se puede instalar una paquetería con el comando !pip install nombre_paqueteria.\nCada paqueteria tiene su propia documentación, por lo que es recomendable revisarla para conocer todas las funciones y características que ofrece.",
    "crumbs": [
      "Fundamentos de Python",
      "Funciones y Bucles de Control"
    ]
  },
  {
    "objectID": "intro_python/functions_and_loops.html#legibilidad-y-documentación-de-funciones",
    "href": "intro_python/functions_and_loops.html#legibilidad-y-documentación-de-funciones",
    "title": "Funciones y Bucles de Control",
    "section": "Legibilidad y Documentación de Funciones",
    "text": "Legibilidad y Documentación de Funciones\nEs importante que las funciones sean fáciles de leer y entender. Para ello, es recomendable seguir las siguientes buenas prácticas:\n\nNombres descriptivos: Usa nombres descriptivos para las funciones y los parámetros.\nComentarios: Añade comentarios para explicar lo que hace cada parte de la función.\nDocumentación: Usa docstrings para documentar la función y proporcionar información sobre su propósito, los parámetros que espera y el valor que devuelve.\nDividir en funciones más pequeñas: Si una función es demasiado larga o hace demasiadas cosas, divídela en funciones más pequeñas que realicen tareas específicas.\n\nVeamos un ejemplo de una función bien documentada y legible:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nEn este ejemplo, hemos definido una función sine_wave que genera una señal de onda sinusoidal con los parámetros de amplitud, frecuencia, fase, duración y tasa de muestreo especificados. Hemos documentado la función con un docstring que explica los parámetros que espera y el valor que devuelve. También hemos definido una función plot_signal que traza la señal generada. Esta función es más legible y fácil de entender gracias a los comentarios y la documentación.\nUn ejemplo profesional de documentación de funciones es la documentación de la paquetería numpy, que proporciona una descripción detallada de cada función, los parámetros que espera y el valor que devuelve. Puedes consultar la documentación de numpy en el siguiente enlace: https://numpy.org/doc/stable/.",
    "crumbs": [
      "Fundamentos de Python",
      "Funciones y Bucles de Control"
    ]
  },
  {
    "objectID": "intro_python/functions_and_loops.html#estrucutras-de-control-if-elif-y-else",
    "href": "intro_python/functions_and_loops.html#estrucutras-de-control-if-elif-y-else",
    "title": "Funciones y Bucles de Control",
    "section": "Estrucutras de Control: if, elif y else",
    "text": "Estrucutras de Control: if, elif y else\nLa declaración if se utiliza para ejecutar un bloque de código si una condición es verdadera. La declaración elif se utiliza para comprobar múltiples condiciones si la condición anterior es falsa. La declaración else se utiliza para ejecutar un bloque de código si ninguna de las condiciones anteriores es verdadera.\n\n\n\nEstructura de Control if\n\n\nLa sintaxis de la declaración if en python es la siguiente:\nif condicion:\n    # Código a ejecutar si la condición es verdadera\nelif otra_condicion:\n    # Código a ejecutar si la condición anterior es falsa y esta condición es verdadera\nelse:\n    # Código a ejecutar si ninguna de las condiciones anteriores es verdadera\n\nEjemplo\nSi tenemos varias condiciones que queremos comprobar, podemos usar la declaración elif para comprobarlas en orden. A continuación, se muestra un ejemplo de una declaración if con elif que comprueba si un número es positivo, negativo o cero:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Fundamentos de Python",
      "Funciones y Bucles de Control"
    ]
  },
  {
    "objectID": "intro_python/functions_and_loops.html#estrucutras-de-control-bucles-for",
    "href": "intro_python/functions_and_loops.html#estrucutras-de-control-bucles-for",
    "title": "Funciones y Bucles de Control",
    "section": "Estrucutras de Control: Bucles for",
    "text": "Estrucutras de Control: Bucles for\nEl bucle for se utiliza para iterar sobre una secuencia de elementos, como una lista, una tupla o un rango de números. La sintaxis de un bucle for en Python es la siguiente:\nfor elemento in secuencia:\n    # Código a ejecutar en cada iteración\n\n\n\nEstructura de Control for\n\n\nSon muy útiles cuando se necesita realizar una tarea repetitiva, como recorrer una lista de elementos o realizar una operación en cada elemento de una secuencia, serán vitales para el análisis de datos.\n\nEjemplo\nVeamos un ejemplo donde imprimimos una cuenta regresiva desde 10 hasta 1:\n\n\nCódigo\n# Ejemplo de bucle for\nfor i in range(10, 0, -1):\n    print(i)\n\n\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n\nLa función range genera una secuencia de números desde el primer argumento hasta el segundo argumento, con un paso especificado por el tercer argumento. En este caso, estamos generando una secuencia de números desde 10 hasta 1 con un paso de -1. Es muy común usar range en bucles for. Otra forma de usar range es con un solo argumento, que genera una secuencia de números desde 0 hasta el argumento especificado.\n\n\nCódigo\n# Ejemplo de bucle for con un solo argumento\nfor i in range(5):\n    print(i)\n\n\n0\n1\n2\n3\n4\n\n\nComo habras notado, la función range genera una secuencia de números desde 0 hasta 4, ya que el argumento especificado es 5, si cuentas los números, verás que son 5 números, si quieres hasta 5, debes poner 6.\nEn el siguiente tema abordaremos más los usos del ciclo for cuando trabajamos con listas y diccionarios o en general con objetos iterables.",
    "crumbs": [
      "Fundamentos de Python",
      "Funciones y Bucles de Control"
    ]
  },
  {
    "objectID": "intro_python/functions_and_loops.html#estrucutras-de-control-bucles-while",
    "href": "intro_python/functions_and_loops.html#estrucutras-de-control-bucles-while",
    "title": "Funciones y Bucles de Control",
    "section": "Estrucutras de Control: Bucles while",
    "text": "Estrucutras de Control: Bucles while\nEl bucle while se utiliza para ejecutar un bloque de código mientras una condición sea verdadera. La sintaxis de un bucle while en Python es la siguiente:\nwhile condicion:\n    # Código a ejecutar mientras la condición sea verdadera\n\n\n\nEstructura de Control while\n\n\nLos bucles while son útiles cuando no se sabe cuántas veces se debe ejecutar un bloque de código y se necesita comprobar una condición en cada iteración.\n\n ADVERTENCIA. Es importante tener cuidado al usar bucles while, ya que si la condición nunca se vuelve falsa, el bucle se ejecutará indefinidamente, lo que puede provocar un bucle infinito y bloquear el programa.\n\n\nEjemplo\nAhora intentemos algo más complicado, crearemos una función que nos diga cuantas vecces se repite una letra dada en una cadena de texto:\n\n\nCódigo\ndef contar_letra(cadena, letra):\n    contador = 0\n    i = 0\n    while i &lt; len(cadena):\n        if cadena[i] == letra:\n            contador += 1\n        i += 1\n    return contador\n\n# Primer verso y coro de \"Army of One\" de Coldplay \ncadena = \"\"\"\nBeen around the world, wonders to view\nBeen around the world looking for someone like you\nPyramids try, Babylon too\nBut the beautifulest treasures lie in the deepest blue\n\nSo I never say die, aim never untrue\nI'm never so high as when I'm with you\nAnd there isn't a fire that I wouldn't walk through\nMy army of one is gonna fight for you\n\"\"\"\n\nletra = 'a'\nprint(f'La letra \"{letra}\" se repite {contar_letra(cadena, letra)} veces en la cadena de texto.')\n\n\nLa letra \"a\" se repite 14 veces en la cadena de texto.\n\n\nHemos combinado el uso de un bucle while con una declaración if para contar cuántas veces se repite una letra en una cadena de texto. En general siempre podemos usar un bucle for en lugar de un while, pero en este caso, queríamos mostrar cómo se puede usar un bucle while para realizar la misma tarea.\nCombinar funciones, bucles y estructuras de control es una de las formas más poderosas de programar en cualquier lenguaje, y Python no es la excepción.",
    "crumbs": [
      "Fundamentos de Python",
      "Funciones y Bucles de Control"
    ]
  },
  {
    "objectID": "intro_python/functions_and_loops.html#declaración-break",
    "href": "intro_python/functions_and_loops.html#declaración-break",
    "title": "Funciones y Bucles de Control",
    "section": "Declaración break",
    "text": "Declaración break\nLa declaración break se utiliza para salir de un bucle antes de que se complete. Cuando se encuentra una declaración break, el bucle se detiene y la ejecución continúa con la siguiente instrucción después del bucle. La declaración break se puede utilizar en bucles for y while. Se suele utilizar para salir de un bucle cuando se cumple una condición específica utilizando una declaración if.\nSupongamos que queremos encontrar el primer número divisible entre \\(7\\) y \\(5\\) en un rango de números del \\(1\\) al \\(100\\). Podemos usar un bucle for y la declaración break para salir del bucle cuando se encuentre el número deseado.\n\n\nCódigo\nfor i in range(1, 101):\n    if i % 7 == 0 and i % 5 == 0:\n        print(f'El primer número divisible entre 7 y 5 es {i}')\n        break\n    else:\n        print(f'{i} no es divisible entre 7 y 5')\n\n\n1 no es divisible entre 7 y 5\n2 no es divisible entre 7 y 5\n3 no es divisible entre 7 y 5\n4 no es divisible entre 7 y 5\n5 no es divisible entre 7 y 5\n6 no es divisible entre 7 y 5\n7 no es divisible entre 7 y 5\n8 no es divisible entre 7 y 5\n9 no es divisible entre 7 y 5\n10 no es divisible entre 7 y 5\n11 no es divisible entre 7 y 5\n12 no es divisible entre 7 y 5\n13 no es divisible entre 7 y 5\n14 no es divisible entre 7 y 5\n15 no es divisible entre 7 y 5\n16 no es divisible entre 7 y 5\n17 no es divisible entre 7 y 5\n18 no es divisible entre 7 y 5\n19 no es divisible entre 7 y 5\n20 no es divisible entre 7 y 5\n21 no es divisible entre 7 y 5\n22 no es divisible entre 7 y 5\n23 no es divisible entre 7 y 5\n24 no es divisible entre 7 y 5\n25 no es divisible entre 7 y 5\n26 no es divisible entre 7 y 5\n27 no es divisible entre 7 y 5\n28 no es divisible entre 7 y 5\n29 no es divisible entre 7 y 5\n30 no es divisible entre 7 y 5\n31 no es divisible entre 7 y 5\n32 no es divisible entre 7 y 5\n33 no es divisible entre 7 y 5\n34 no es divisible entre 7 y 5\nEl primer número divisible entre 7 y 5 es 35\n\n\nPodemos ver que el bucle se detiene en el número \\(35\\), que es el primer número divisible entre \\(7\\) y \\(5\\) en el rango de números del \\(1\\) al \\(100\\).",
    "crumbs": [
      "Fundamentos de Python",
      "Funciones y Bucles de Control"
    ]
  },
  {
    "objectID": "intro_python/functions_and_loops.html#declaración-continue",
    "href": "intro_python/functions_and_loops.html#declaración-continue",
    "title": "Funciones y Bucles de Control",
    "section": "Declaración continue",
    "text": "Declaración continue\nLa declaración continue se utiliza para saltar a la siguiente iteración de un bucle sin ejecutar el resto del código en el bloque de bucle. Se suele utilizar para omitir ciertas iteraciones de un bucle basándose en una condición.\nPor ejemplo, supongamos que queremos imprimir todos los números del \\(1\\) al \\(10\\) excepto el número \\(5\\). Podemos usar un bucle for y la declaración continue para saltar la iteración cuando el número es \\(5\\).\n\n\nCódigo\nfor i in range(1, 11):\n    if i == 5:\n        continue\n    print(i)\n\n\n1\n2\n3\n4\n6\n7\n8\n9\n10\n\n\nOtra declaración que se puede usar en bucles es pass, que no hace nada y se utiliza como marcador de posición cuando no se necesita ejecutar ninguna instrucción en un bloque de código. Pero el resultado no es el mismo que continue, ya que pass no salta a la siguiente iteración, simplemente no hace nada.\n\n\nCódigo\nfor i in range(1, 11):\n    if i == 5:\n        pass\n    print(i)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nEn este caso, el número \\(5\\) se imprimirá, pero no se ejecutará ninguna instrucción cuando i == 5. Es una forma de evitar errores de sintaxis cuando se necesita un bloque de código vacío (por ejemplo, en una declaración if o una función que aún no se ha implementado).",
    "crumbs": [
      "Fundamentos de Python",
      "Funciones y Bucles de Control"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ciencia de Datos con Python",
    "section": "",
    "text": "Este curso tiene como meta introducir a los estudiantes de la Facultad de Psicología al mundo de la ciencia de datos utilizando Python. A lo largo del curso se abordarán temas como la programación en Python, la estadística descriptiva e inferencial, el análisis y visualización de datos, y la creación de redes neuronales.\nEl curso no supone ningún conocimiento previo de programación, pero se espera que los estudiantes tengan una actitud proactiva y estén dispuestos a aprender y a poner en práctica los conceptos que se presenten en clase.\nAunque se repasarán de manera breve conceptos de estadística y probabilidad, es recomendable tener un conocimiento básico de estos temas para poder aprovechar al máximo el curso. Otro requisito importante es tener una computadora personal con acceso a internet y disponibilidad de llevar al salón, ya que la mayor parte de las actividades requerirán de una computadora personal.\nUna parte importante del curso es crear un ambiente de aprendizaje colaborativo, por lo que se espera que los estudiantes participen activamente en las clases, realicen las tareas y actividades que se les asignen, y que estén dispuestos a ayudar a sus compañeros en caso de que lo necesiten. Además se crearán equipos permanentes de trabajo, por lo que es importante que los estudiantes se comprometan a trabajar en equipo dado que todas las actividades se realizarán en equipo fomentando un método de trabajo llamado “pair programming”.",
    "crumbs": [
      "Inicio"
    ]
  },
  {
    "objectID": "index.html#acerca-de-este-curso",
    "href": "index.html#acerca-de-este-curso",
    "title": "Ciencia de Datos con Python",
    "section": "",
    "text": "Este curso tiene como meta introducir a los estudiantes de la Facultad de Psicología al mundo de la ciencia de datos utilizando Python. A lo largo del curso se abordarán temas como la programación en Python, la estadística descriptiva e inferencial, el análisis y visualización de datos, y la creación de redes neuronales.\nEl curso no supone ningún conocimiento previo de programación, pero se espera que los estudiantes tengan una actitud proactiva y estén dispuestos a aprender y a poner en práctica los conceptos que se presenten en clase.\nAunque se repasarán de manera breve conceptos de estadística y probabilidad, es recomendable tener un conocimiento básico de estos temas para poder aprovechar al máximo el curso. Otro requisito importante es tener una computadora personal con acceso a internet y disponibilidad de llevar al salón, ya que la mayor parte de las actividades requerirán de una computadora personal.\nUna parte importante del curso es crear un ambiente de aprendizaje colaborativo, por lo que se espera que los estudiantes participen activamente en las clases, realicen las tareas y actividades que se les asignen, y que estén dispuestos a ayudar a sus compañeros en caso de que lo necesiten. Además se crearán equipos permanentes de trabajo, por lo que es importante que los estudiantes se comprometan a trabajar en equipo dado que todas las actividades se realizarán en equipo fomentando un método de trabajo llamado “pair programming”.",
    "crumbs": [
      "Inicio"
    ]
  },
  {
    "objectID": "index.html#objetivos-del-curso",
    "href": "index.html#objetivos-del-curso",
    "title": "Ciencia de Datos con Python",
    "section": "Objetivos del curso",
    "text": "Objetivos del curso\n\nAprender y poner en práctica los conceptos básicos de la programación en Python.\nComprender los conceptos básicos de la estadística descriptiva e inferencial a través de ejemplos prácticos en Python.\nAprender a utilizar las librerías más populares de Python para el análisis y la visualización de datos.\nComprender los conceptos básicos de las redes neuronales y aprender a crear redes neuronales utilizando Python.\nAprender a presentar y comunicar los resultados de un análisis de datos de manera efectiva y con código reproducible.\n\n\n\nInstructor\n\n   Christian Francisco Badillo Hernández\n   Lab 25, Facultad de Psicología, UNAM\n   cris.badillo1408@gmail.com\n\n\n\nDetalles del curso\n\n   Lunes - Viernes\n   10 de junio - 28 de junio de 2024\n   13:30 - 15:30 hrs\n   A209\n   Classroom",
    "crumbs": [
      "Inicio"
    ]
  },
  {
    "objectID": "index.html#temario",
    "href": "index.html#temario",
    "title": "Ciencia de Datos con Python",
    "section": "Temario",
    "text": "Temario\n\nIntroducción a Python y la Ciencia de Datos.\n\nTipos, variables y operaciones matemáticas básicas.\n\nStrings (cadenas de texto).\nInteger (Números enteros).\nFloats (Décimales)\nBooleanos\nExpresiones y declaraciones.\nOperaciones.\n\nDefinición de funciones.\n\nDefinición de funciones en python\nUso de funciones.\nFunciones lambda\n\nFlujos de control.\n\nFlujo de control If, else, elif\nFlujo de control while\nFlujo de control for\nKeywords de flujos de control: pass, continue, break\n\nEstructuras de datos.\n\nSet.\nTuplas.\nDiccionarios.\nListas.\nMatrices.\nNdarrays.\nData Frames.\n\nMódulos y paquetes.\n\nDefinición de módulos y paquetes.\n\n\nLimpieza, manejo y creación de bases de datos con pandas.\n\nTipos de bases de datos.\n\nBases de datos relacionales.\nBase de datos no relacionales.\n\nIntroducción al manejo de bases de datos con pandas.\n\nCarga de bases de datos locales o de la nube.\nBúsqueda de datos faltantes.\nDescripción de bases de datos.\nModificación de bases de datos.\nCreación de bases de datos.\n\n\nAnálisis descriptivo y visualización de datos.\n\nVisualización de datos con Matplotlib y Seaborn.\n\nIntroducción a la visualización de datos.\nMatriz de correlación.\nVisualizaciones básicas (histogramas, cajas de bigotes, etc.).\nVisualizaciones en 3D y combinaciones de gráficos.\n\n\nEstadística Inferencial con Python.\n\nPaqueterías de Análisis estadístico.\n\nStatsmodels.\nScikit-learn.\nPaqueterías bayesianas (PyMC3, PyJAGS, PySTAN).\n\nRegresión lineal.\nRegresión múltiple.\nModelos lineales generalizados (regresión logística, ANOVA, T-Student, etc).\nEstimación Bayesiana (Uso de algoritmos MCMC).\n\nIntroducción a Machine learning: Redes Neuronales.\n\nConceptos básicos de Inteligencia artificial y redes neuronales.\nPaqueterías para redes neuronales.\n\nTensorflow.\nScikit-learn.\nKeras.\n\nProgramación del perceptrón multicapa.\nAnalizando datos con un red neuronal multicapa.\n\n\n\nPuedes descargar el temario completo en el siguiente enlace:\n Descarga el temario",
    "crumbs": [
      "Inicio"
    ]
  },
  {
    "objectID": "neural_neworks/classes_python.html",
    "href": "neural_neworks/classes_python.html",
    "title": "Programación Orientada a Objetos en Python",
    "section": "",
    "text": "La programación orientada a objetos (POO) es un paradigma de programación que se basa en el uso de clases y objetos. En este paradigma, los objetos son entidades que tienen atributos y métodos. Las clases son plantillas que definen la estructura de los objetos.\nPodemos pensar en las clases como moldes para crear objetos. Por ejemplo, si tenemos una clase Gato, podemos crear varios objetos de tipo Gato a partir de esa clase. Cada objeto de tipo Gato tendrá sus propios atributos y métodos.\nLos atributos de una clase son variables que almacenan información sobre el objeto. Por ejemplo, un gato puede tener atributos como nombre, edad y color. Son caracterticas que definen al objeto y pocas veces cambian.\nLos métodos de una clase son funciones que definen el comportamiento del objeto. Por ejemplo, un gato puede tener métodos como maullar, dormir y comer. Son acciones que el objeto puede realizar. Pueden modificar los atributos del objeto o realizar alguna acción en el objeto.\nLas características de la POO son:\n\nEncapsulamiento: Las clases encapsulan los datos y los métodos que operan sobre esos datos. Esto significa que los datos y los métodos están juntos en un solo lugar.\nAbstracción: Las clases permiten abstraer los detalles de implementación de un objeto. Esto significa que podemos usar un objeto sin necesidad de conocer cómo está implementado.\nHerencia: Las clases pueden heredar atributos y métodos de otras clases. Esto permite reutilizar código y crear jerarquías de clases.\nPolimorfismo: Las clases pueden tener métodos con el mismo nombre pero con diferentes implementaciones. Esto permite que un objeto pueda comportarse de diferentes maneras dependiendo del contexto.\nModularidad: Las clases permiten dividir un programa en módulos más pequeños. Esto facilita la reutilización de código y el mantenimiento del programa.\n\nLos objetos son instancias de una clase. Por ejemplo, si tenemos una clase Gato, un objeto de tipo Gato sería un gato en particular. Podemos crear varios objetos de tipo Gato a partir de la misma clase. Ya hemos visto esto una gran cantidad de veces, por ejemplo, cuando creamos una lista vacía con [] estamos creando un objeto de la clase list que tiene sus propios métodos y atributos.\nLa sintaxis en Python para definir una clase es la siguiente:\nclass NombreDeLaClase(object):\n    def __init__(self, atributo1, atributo2):\n        self.atributo1 = atributo1\n        self.atributo2 = atributo2\n\n    def metodo1(self, parametro1, parametro2):\n        pass\n\n    def metodo2(self):\n        pass\nSe usa la palabra reservada class seguida del nombre de la clase y entre parentesis la clase de la que es heredera, sino se especifica la clase no hereda nada. Cada método se define como una función dentro de la clase. El método __init__ es el constructor de la clase y se llama cuando se crea un objeto de la clase. El primer argumento de los métodos es self, que es una referencia al objeto en sí mismo, esto indica que el método pertenece a la clase. Además del parámetro self, los métodos pueden tener otros parámetros que se pasan al llamar al método. Para acceder a los atributos de la clase se usa la notación self.atributo.\nPara documentar una clase se usa el siguiente formato:\nclass NombreDeLaClase(objecto):\n    \"\"\"\n    Documentación de la clase\n    \"\"\"\n\n    def __init__(self, atributo1, atributo2):\n        \"\"\"\n        Documentación del método __init__\n        \"\"\"\n        self.atributo1 = atributo1\n        self.atributo2 = atributo2\n\n    def metodo1(self, parametro1, parametro2):\n        \"\"\"\n        Documentación del método metodo1\n        \"\"\"\n        pass\n\n    def metodo2(self):\n        \"\"\"\n        Documentación del método metodo2\n        \"\"\"\n        pass\nPara instanciar un objeto de una clase se usa la siguiente sintaxis:\nobjeto = NombreDeLaClase(init_param1, init_param2)\nAsignamos el objeto a una variable y llamamos a la clase con los parámetros que recibe el método __init__.\n\n\nVamos a crear una clase llamada Gato que tiene los atributos nombre, edad y color y los métodos maullar, dormir y comer.\n\n\nCódigo\nclass Gato(object):\n    \"\"\"\n    Clase que representa un gato\n    \"\"\"\n\n    def __init__(self, nombre, edad, color):\n        \"\"\"\n        Constructor de la clase Gato\n        \"\"\"\n        self.nombre = nombre\n        self.edad = edad\n        self.color = color\n\n    def maullar(self):\n        \"\"\"\n        Método que hace que el gato maúlle\n        \"\"\"\n        print(f\"{self.nombre} está maullando\")\n\n    def dormir(self):\n        \"\"\"\n        Método que hace que el gato duerma\n        \"\"\"\n        print(f\"{self.nombre} está durmiendo\")\n\n    def comer(self):\n        \"\"\"\n        Método que hace que el gato coma\n        \"\"\"\n        print(f\"{self.nombre} está comiendo\")\n\n\nAhora vamos a crear un objeto de tipo Gato y llamar a sus métodos.\n\n\nCódigo\ngato = Gato(\"Tom\", 3, \"gris\")\ngato.maullar()\n\ngato.dormir()\n\ngato.comer()\n\n\nTom está maullando\nTom está durmiendo\nTom está comiendo\n\n\nAhora creemos una nueva clase que herede de la clase Gato, en este caso crearemos un gato naranjoso.\n\n\nCódigo\nclass Naranjoso(Gato):\n    \"\"\"\n    Clase que representa un gato naranjoso\n    \"\"\"\n\n    def __init__(self, nombre, edad):\n        \"\"\"\n        Constructor de la clase Naranjoso\n        \"\"\"\n        super().__init__(nombre, edad, \"naranja\")\n\n    def romper_cosas(self):\n        \"\"\"\n        Método que hace que el gato naranjoso rompa cosas\n        \"\"\"\n        print(f\"{self.nombre} está rompiendo cosas\")\n    \n    def travesura(self):\n        \"\"\"\n        Método que hace que el gato naranjoso haga travesuras\n        \"\"\"\n        print(f\"{self.nombre} está haciendo travesuras\")\n\n    def comer_lasaña(self):\n        \"\"\"\n        Método que hace que el gato naranjoso coma lasaña\n        \"\"\"\n        print(f\"{self.nombre} está comiendo lasaña\")\n\n\nAhora vamos a crear un objeto de tipo Naranjoso y llamar a sus métodos.\n\n\nCódigo\nnaranjoso = Naranjoso(\"Garfield\", 5)\n\nnaranjoso.maullar()\n\nnaranjoso.dormir()\n\nnaranjoso.romper_cosas()\n\nnaranjoso.travesura()\n\nnaranjoso.comer_lasaña()\n\n\nGarfield está maullando\nGarfield está durmiendo\nGarfield está rompiendo cosas\nGarfield está haciendo travesuras\nGarfield está comiendo lasaña\n\n\nComo hemos visto podemos usar los métodos de la clase Gato en la clase Naranjoso ya que Naranjoso hereda de Gato y hemos creado nuevos métodos para la clase Naranjoso que no están en la clase Gato sin modificar la clase Gato.",
    "crumbs": [
      "Redes Neuronales",
      "Programación Orientada a Objetos en Python"
    ]
  },
  {
    "objectID": "neural_neworks/classes_python.html#ejemplo",
    "href": "neural_neworks/classes_python.html#ejemplo",
    "title": "Programación Orientada a Objetos en Python",
    "section": "",
    "text": "Vamos a crear una clase llamada Gato que tiene los atributos nombre, edad y color y los métodos maullar, dormir y comer.\n\n\nCódigo\nclass Gato(object):\n    \"\"\"\n    Clase que representa un gato\n    \"\"\"\n\n    def __init__(self, nombre, edad, color):\n        \"\"\"\n        Constructor de la clase Gato\n        \"\"\"\n        self.nombre = nombre\n        self.edad = edad\n        self.color = color\n\n    def maullar(self):\n        \"\"\"\n        Método que hace que el gato maúlle\n        \"\"\"\n        print(f\"{self.nombre} está maullando\")\n\n    def dormir(self):\n        \"\"\"\n        Método que hace que el gato duerma\n        \"\"\"\n        print(f\"{self.nombre} está durmiendo\")\n\n    def comer(self):\n        \"\"\"\n        Método que hace que el gato coma\n        \"\"\"\n        print(f\"{self.nombre} está comiendo\")\n\n\nAhora vamos a crear un objeto de tipo Gato y llamar a sus métodos.\n\n\nCódigo\ngato = Gato(\"Tom\", 3, \"gris\")\ngato.maullar()\n\ngato.dormir()\n\ngato.comer()\n\n\nTom está maullando\nTom está durmiendo\nTom está comiendo\n\n\nAhora creemos una nueva clase que herede de la clase Gato, en este caso crearemos un gato naranjoso.\n\n\nCódigo\nclass Naranjoso(Gato):\n    \"\"\"\n    Clase que representa un gato naranjoso\n    \"\"\"\n\n    def __init__(self, nombre, edad):\n        \"\"\"\n        Constructor de la clase Naranjoso\n        \"\"\"\n        super().__init__(nombre, edad, \"naranja\")\n\n    def romper_cosas(self):\n        \"\"\"\n        Método que hace que el gato naranjoso rompa cosas\n        \"\"\"\n        print(f\"{self.nombre} está rompiendo cosas\")\n    \n    def travesura(self):\n        \"\"\"\n        Método que hace que el gato naranjoso haga travesuras\n        \"\"\"\n        print(f\"{self.nombre} está haciendo travesuras\")\n\n    def comer_lasaña(self):\n        \"\"\"\n        Método que hace que el gato naranjoso coma lasaña\n        \"\"\"\n        print(f\"{self.nombre} está comiendo lasaña\")\n\n\nAhora vamos a crear un objeto de tipo Naranjoso y llamar a sus métodos.\n\n\nCódigo\nnaranjoso = Naranjoso(\"Garfield\", 5)\n\nnaranjoso.maullar()\n\nnaranjoso.dormir()\n\nnaranjoso.romper_cosas()\n\nnaranjoso.travesura()\n\nnaranjoso.comer_lasaña()\n\n\nGarfield está maullando\nGarfield está durmiendo\nGarfield está rompiendo cosas\nGarfield está haciendo travesuras\nGarfield está comiendo lasaña\n\n\nComo hemos visto podemos usar los métodos de la clase Gato en la clase Naranjoso ya que Naranjoso hereda de Gato y hemos creado nuevos métodos para la clase Naranjoso que no están en la clase Gato sin modificar la clase Gato.",
    "crumbs": [
      "Redes Neuronales",
      "Programación Orientada a Objetos en Python"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "",
    "section": "",
    "text": "Ver código\n\n\n\n\nMIT License\nCopyright (c) 2024 Christian Francisco Badillo Hernández\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n Volver arriba"
  },
  {
    "objectID": "statistics/Linear_models.html",
    "href": "statistics/Linear_models.html",
    "title": "Modelos Lineales",
    "section": "",
    "text": "Los modelos lineales son una clase de modelos estadísticos que asumen que la relación entre las variables dependientes e independientes es lineal. En este tutorial, aprenderemos cómo ajustar un modelo lineal a un conjunto de datos y cómo interpretar los resultados.\nEntre los modelos lineales más comunes se encuentran la regresión lineal simple y la regresión lineal múltiple. Se pueden utilizar para predecir una variable dependiente a partir de una o más variables independientes.\nOtros modelos lineales incluyen algunas pruebas de hipótesis, como el análisis de varianza (ANOVA) y la comparación de medias (prueba t).\nTodos estos modelos caen en la categoría de modelos lineales generalizados (GLM), que son una extensión de los modelos lineales tradicionales que permiten una mayor flexibilidad en la especificación de la distribución de los errores y la función de enlace.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Modelos Lineales"
    ]
  },
  {
    "objectID": "statistics/Linear_models.html#regresión-lineal-simple-por-mínimos-cuadrados",
    "href": "statistics/Linear_models.html#regresión-lineal-simple-por-mínimos-cuadrados",
    "title": "Modelos Lineales",
    "section": "Regresión Lineal Simple por Mínimos Cuadrados",
    "text": "Regresión Lineal Simple por Mínimos Cuadrados\nEl método más común para ajustar un modelo de regresión lineal simple es el método de mínimos cuadrados. Este método consiste en encontrar los valores de \\(\\beta_0\\) y \\(\\beta_1\\) que minimizan la suma de los cuadrados de los errores. La fórmula para calcular los valores de \\(\\beta_0\\) y \\(\\beta_1\\) es la siguiente:\n\\[\\beta_1 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}\\]\n\\[\\beta_0 = \\bar{Y} - \\beta_1\\bar{X}\\]\nDonde:\n\n\\(\\bar{X}\\) es la media de la variable independiente\n\\(\\bar{Y}\\) es la media de la variable dependiente\n\\(n\\) es el número de observaciones\n\nUsemos los datos de la altura y el peso de un grupo de personas para ajustar un modelo de regresión lineal simple y predecir el peso de una persona en función de su altura.\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Datos\naltura = np.array([150, 160, 170, 180, 190])\npeso = np.array([50, 60, 70, 80, 90])\n\n# Calcular medias\naltura_media = np.mean(altura)\npeso_media = np.mean(peso)\n\n# Calcular beta_1\nbeta_1 = np.sum((altura - altura_media) * (peso - peso_media)) / np.sum((altura - altura_media) ** 2)\n\n# Calcular beta_0\nbeta_0 = peso_media - beta_1 * altura_media\n\n# Imprimir coeficientes\nprint(f'beta_0: {beta_0}')\nprint(f'beta_1: {beta_1}')\n\n\nbeta_0: -100.0\nbeta_1: 1.0\n\n\nPodemos graficar los datos y la línea de regresión para visualizar la relación entre la altura y el peso.\n\n\nCódigo\n# Graficar datos\nplt.scatter(altura, peso, color='blue')\nplt.plot(altura, beta_0 + beta_1 * altura, color='red')\nplt.xlabel('Altura')\nplt.ylabel('Peso')\nplt.title('Regresión Lineal Simple')\nplt.show()\n\n\n\n\n\n\n\n\n\nAhora podemos crear datos datos mas realistas y ajustar un modelo de regresión lineal simple.\n\n\nCódigo\n# Crear datos\nnp.random.seed(0)\n\naltura = np.random.normal(170, 10, 100)\npeso = 50 + 0.05 * altura + np.random.normal(0, 5, 100)\n\n# Calcular medias\naltura_media = np.mean(altura)\npeso_media = np.mean(peso)\n\n# Calcular beta_1\nbeta_1 = np.sum((altura - altura_media) * (peso - peso_media)) / np.sum((altura - altura_media) ** 2)\n\n# Calcular beta_0\nbeta_0 = peso_media - beta_1 * altura_media\n\n# Imprimir coeficientes\nprint(f'beta_0: {beta_0}')\nprint(f'beta_1: {beta_1}')\n\n# Graficar datos\nplt.figure(figsize=(10, 6))\nplt.scatter(altura, peso, color='blue')\nplt.plot(altura, beta_0 + beta_1 * altura, color='red')\nplt.xlabel('Altura')\nplt.ylabel('Peso')\nplt.title('Regresión Lineal Simple')\nplt.show()\n\n\nbeta_0: 40.626398573820204\nbeta_1: 0.10734921677319048\n\n\n\n\n\n\n\n\n\nUsando seaborn podemos ajustar un modelo de regresión lineal simple y visualizar los resultados.\n\n\nCódigo\nimport seaborn as sns\nimport pandas as pd\n\n# Crear DataFrame\ndf = pd.DataFrame({'altura': altura, 'peso': peso})\n\n# Ajustar modelo\nplt.figure(figsize=(10, 6))\nsns.lmplot(x='altura', y='peso', data=df)\n\nplt.show()\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nNos muestra la gráfica de la regresión lineal simple y su intervalo de confianza.\nLa libreria statsmodels nos permite ajustar un modelo de regresión lineal simple y obtener un resumen de los resultados.\nPuedes instalar la libreria usando el siguiente comando:\n!pip install statsmodels\n\n\nCódigo\nimport statsmodels.api as sm\n\n# Ajustar modelo\nX = sm.add_constant(altura)\nmodel = sm.OLS(peso, X).fit()\n\n# Imprimir resumen\nprint(model.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.042\nModel:                            OLS   Adj. R-squared:                  0.033\nMethod:                 Least Squares   F-statistic:                     4.341\nDate:                Sat, 22 Jun 2024   Prob (F-statistic):             0.0398\nTime:                        13:41:19   Log-Likelihood:                -305.62\nNo. Observations:                 100   AIC:                             615.2\nDf Residuals:                      98   BIC:                             620.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         40.6264      8.805      4.614      0.000      23.152      58.100\nx1             0.1073      0.052      2.083      0.040       0.005       0.210\n==============================================================================\nOmnibus:                        5.184   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.075   Jarque-Bera (JB):                3.000\nSkew:                           0.210   Prob(JB):                        0.223\nKurtosis:                       2.262   Cond. No.                     2.90e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.9e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nPara ajustar el modelo primero agregamos una columna de unos a la matriz de características X y luego ajustamos el modelo usando la función OLS de statsmodels. Finalmente, imprimimos un resumen de los resultados del modelo.\nImprimamos el valor de X\n\n\nCódigo\nX\n\n\narray([[  1.        , 187.64052346],\n       [  1.        , 174.00157208],\n       [  1.        , 179.78737984],\n       [  1.        , 192.40893199],\n       [  1.        , 188.6755799 ],\n       [  1.        , 160.2272212 ],\n       [  1.        , 179.50088418],\n       [  1.        , 168.48642792],\n       [  1.        , 168.96781148],\n       [  1.        , 174.10598502],\n       [  1.        , 171.44043571],\n       [  1.        , 184.54273507],\n       [  1.        , 177.61037725],\n       [  1.        , 171.21675016],\n       [  1.        , 174.43863233],\n       [  1.        , 173.33674327],\n       [  1.        , 184.94079073],\n       [  1.        , 167.94841736],\n       [  1.        , 173.13067702],\n       [  1.        , 161.45904261],\n       [  1.        , 144.47010184],\n       [  1.        , 176.53618595],\n       [  1.        , 178.64436199],\n       [  1.        , 162.5783498 ],\n       [  1.        , 192.69754624],\n       [  1.        , 155.45634325],\n       [  1.        , 170.45758517],\n       [  1.        , 168.1281615 ],\n       [  1.        , 185.32779214],\n       [  1.        , 184.6935877 ],\n       [  1.        , 171.54947426],\n       [  1.        , 173.7816252 ],\n       [  1.        , 161.12214252],\n       [  1.        , 150.19203532],\n       [  1.        , 166.52087851],\n       [  1.        , 171.56348969],\n       [  1.        , 182.30290681],\n       [  1.        , 182.02379849],\n       [  1.        , 166.12673183],\n       [  1.        , 166.97697249],\n       [  1.        , 159.51447035],\n       [  1.        , 155.79982063],\n       [  1.        , 152.93729809],\n       [  1.        , 189.50775395],\n       [  1.        , 164.90347818],\n       [  1.        , 165.61925698],\n       [  1.        , 157.4720464 ],\n       [  1.        , 177.77490356],\n       [  1.        , 153.86102152],\n       [  1.        , 167.8725972 ],\n       [  1.        , 161.04533439],\n       [  1.        , 173.86902498],\n       [  1.        , 164.89194862],\n       [  1.        , 158.19367816],\n       [  1.        , 169.71817772],\n       [  1.        , 174.28331871],\n       [  1.        , 170.66517222],\n       [  1.        , 173.02471898],\n       [  1.        , 163.65677906],\n       [  1.        , 166.37258834],\n       [  1.        , 163.27539552],\n       [  1.        , 166.40446838],\n       [  1.        , 161.86853718],\n       [  1.        , 152.73717398],\n       [  1.        , 171.77426142],\n       [  1.        , 165.98219064],\n       [  1.        , 153.69801653],\n       [  1.        , 174.62782256],\n       [  1.        , 160.92701636],\n       [  1.        , 170.51945396],\n       [  1.        , 177.29090562],\n       [  1.        , 171.28982911],\n       [  1.        , 181.39400685],\n       [  1.        , 157.6517418 ],\n       [  1.        , 174.02341641],\n       [  1.        , 163.15189909],\n       [  1.        , 161.29202851],\n       [  1.        , 164.21150335],\n       [  1.        , 166.88447468],\n       [  1.        , 170.56165342],\n       [  1.        , 158.34850159],\n       [  1.        , 179.00826487],\n       [  1.        , 174.6566244 ],\n       [  1.        , 154.63756314],\n       [  1.        , 184.88252194],\n       [  1.        , 188.95889176],\n       [  1.        , 181.78779571],\n       [  1.        , 168.20075164],\n       [  1.        , 159.29247378],\n       [  1.        , 180.54451727],\n       [  1.        , 165.96823053],\n       [  1.        , 182.2244507 ],\n       [  1.        , 172.08274978],\n       [  1.        , 179.76639036],\n       [  1.        , 173.56366397],\n       [  1.        , 177.06573168],\n       [  1.        , 170.10500021],\n       [  1.        , 187.85870494],\n       [  1.        , 171.26912093],\n       [  1.        , 174.01989363]])\n\n\nQue corresponde a lo que se denomina como matriz de diseño. Que es una matriz que contiene las variables independientes y una columna de unos que representa la intersección. Podemos usar esta matriz para ajustar el modelo de regresión lineal simple por medio de la siguiente fórmula:\n\\[Y = X\\beta + \\epsilon\\]\nY usando el método de mínimos cuadrados para encontrar los valores de \\(\\beta\\) que minimizan la suma de los cuadrados de los errores, nos da la siguiente fórmula:\n\\[\\beta = (X^TX)^{-1}X^TY\\]\nDonde:\n\n\\(Y\\) es el vector de la variable dependiente\n\\(X\\) es la matriz de diseño\n\\(\\beta\\) es el vector de coeficientes\n\nHagamos esto en Python.\n\n\nCódigo\n# Ajustar modelo\nX = np.column_stack((np.ones(len(altura)), altura))\n\nprint(X)\n\nbeta = np.linalg.inv(X.T @ X) @ X.T @ peso\n\n# Imprimir coeficientes\nprint(f'beta_0: {beta[0]}')\nprint(f'beta_1: {beta[1]}')\n\n\n[[  1.         187.64052346]\n [  1.         174.00157208]\n [  1.         179.78737984]\n [  1.         192.40893199]\n [  1.         188.6755799 ]\n [  1.         160.2272212 ]\n [  1.         179.50088418]\n [  1.         168.48642792]\n [  1.         168.96781148]\n [  1.         174.10598502]\n [  1.         171.44043571]\n [  1.         184.54273507]\n [  1.         177.61037725]\n [  1.         171.21675016]\n [  1.         174.43863233]\n [  1.         173.33674327]\n [  1.         184.94079073]\n [  1.         167.94841736]\n [  1.         173.13067702]\n [  1.         161.45904261]\n [  1.         144.47010184]\n [  1.         176.53618595]\n [  1.         178.64436199]\n [  1.         162.5783498 ]\n [  1.         192.69754624]\n [  1.         155.45634325]\n [  1.         170.45758517]\n [  1.         168.1281615 ]\n [  1.         185.32779214]\n [  1.         184.6935877 ]\n [  1.         171.54947426]\n [  1.         173.7816252 ]\n [  1.         161.12214252]\n [  1.         150.19203532]\n [  1.         166.52087851]\n [  1.         171.56348969]\n [  1.         182.30290681]\n [  1.         182.02379849]\n [  1.         166.12673183]\n [  1.         166.97697249]\n [  1.         159.51447035]\n [  1.         155.79982063]\n [  1.         152.93729809]\n [  1.         189.50775395]\n [  1.         164.90347818]\n [  1.         165.61925698]\n [  1.         157.4720464 ]\n [  1.         177.77490356]\n [  1.         153.86102152]\n [  1.         167.8725972 ]\n [  1.         161.04533439]\n [  1.         173.86902498]\n [  1.         164.89194862]\n [  1.         158.19367816]\n [  1.         169.71817772]\n [  1.         174.28331871]\n [  1.         170.66517222]\n [  1.         173.02471898]\n [  1.         163.65677906]\n [  1.         166.37258834]\n [  1.         163.27539552]\n [  1.         166.40446838]\n [  1.         161.86853718]\n [  1.         152.73717398]\n [  1.         171.77426142]\n [  1.         165.98219064]\n [  1.         153.69801653]\n [  1.         174.62782256]\n [  1.         160.92701636]\n [  1.         170.51945396]\n [  1.         177.29090562]\n [  1.         171.28982911]\n [  1.         181.39400685]\n [  1.         157.6517418 ]\n [  1.         174.02341641]\n [  1.         163.15189909]\n [  1.         161.29202851]\n [  1.         164.21150335]\n [  1.         166.88447468]\n [  1.         170.56165342]\n [  1.         158.34850159]\n [  1.         179.00826487]\n [  1.         174.6566244 ]\n [  1.         154.63756314]\n [  1.         184.88252194]\n [  1.         188.95889176]\n [  1.         181.78779571]\n [  1.         168.20075164]\n [  1.         159.29247378]\n [  1.         180.54451727]\n [  1.         165.96823053]\n [  1.         182.2244507 ]\n [  1.         172.08274978]\n [  1.         179.76639036]\n [  1.         173.56366397]\n [  1.         177.06573168]\n [  1.         170.10500021]\n [  1.         187.85870494]\n [  1.         171.26912093]\n [  1.         174.01989363]]\nbeta_0: 40.62639857381858\nbeta_1: 0.10734921677318345\n\n\n\nDemostración de la Fórmula de Mínimos Cuadrados*\nTenemos el modelo de regresión lineal general:\n\\[Y = \\beta_0 + \\beta_1X + ... + \\beta_nX_n + \\epsilon\\]\nLa función de costo que queremos minimizar es el error cuadrático residual (RSS), que se define como:\n\\[RSS = \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})^2\\]\nEn forma matricial, la función de costo se puede expresar como:\n\\[RSS = (Y - X\\beta)^T(Y - X\\beta)\\]\nQue se conoce como la forma cuadrática de la función de costo. Para minimizar la función de costo, tomamos la derivada de la función de costo con respecto a \\(\\beta\\):\n\\[\\begin{align*}\n\n\\frac{\\partial RSS}{\\partial \\beta} & = \\frac{\\partial}{\\partial \\beta} (Y - X\\beta)^T(Y - X\\beta) \\\\\n& = \\frac{\\partial}{\\partial \\beta} (Y^TY - Y^TX\\beta - (X\\beta)^TY + (X\\beta)^TX\\beta) \\\\\n& = \\frac{\\partial}{\\partial \\beta} (Y^TY - Y^TX\\beta - \\beta^TX^TY + \\beta^TX^TX\\beta) \\\\\n& = \\frac{\\partial}{\\partial \\beta} Y^TY - \\frac{\\partial}{\\partial \\beta} Y^TX\\beta - \\frac{\\partial}{\\partial \\beta} \\beta^TX^TY + \\frac{\\partial}{\\partial \\beta} \\beta^TX^TX\\beta \\\\\n& = 0 - X^TY - X^TY + 2X^TX\\beta \\\\\n& = -2X^TY + 2X^TX\\beta \\\\\n& = 2X^T(X\\beta - Y) \\\\\n\\end{align*}\\]\nIgualamos la derivada de la función de costo a cero para encontrar el mínimo:\n\\[\\frac{\\partial RSS}{\\partial \\beta} = 0\\]\n\\[2X^T(X\\beta - Y) = 0\\]\nResolviendo para \\(\\beta\\):\n\\[\\begin{align*}\n2X^T(X\\beta - Y) & = 0 \\\\\nX^T(X\\beta - Y) & = 0 \\\\\nX^TX\\beta - X^TY & = 0 \\\\\nX^TX\\beta & = X^TY \\\\\n\\beta & = (X^TX)^{-1}X^TY \\\\\n\\end{align*}\\]",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Modelos Lineales"
    ]
  },
  {
    "objectID": "statistics/Linear_models.html#regresión-lineal-simple-por-descenso-del-gradiente",
    "href": "statistics/Linear_models.html#regresión-lineal-simple-por-descenso-del-gradiente",
    "title": "Modelos Lineales",
    "section": "Regresión Lineal Simple por Descenso del Gradiente",
    "text": "Regresión Lineal Simple por Descenso del Gradiente\nOtra forma de ajustar un modelo de regresión lineal simple es mediante el descenso del gradiente. Este método consiste en ajustar los valores de \\(\\beta_0\\) y \\(\\beta_1\\) iterativamente para minimizar una función de costo. La función de costo que se utiliza en la regresión lineal simple es el error cuadrático medio (MSE), que se define como:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})^2\\]\nDonde:\n\n\\(Y_i\\) es el valor observado de la variable dependiente\n\\(\\hat{Y_i}\\) es el valor predicho de la variable dependiente\n\\(n\\) es el número de observaciones\n\nEl descenso de gradiente utiliza la derivada de la función de costo con respecto a los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) para ajustar los valores de los parámetros en la dirección que minimiza la función de costo. La regla de actualización de los parámetros es la siguiente:\n\\[\\beta_0 = \\beta_0 - \\alpha \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})\\]\n\\[\\beta_1 = \\beta_1 - \\alpha \\frac{1}{n} \\sum_{i=1}^{n} (Y_i -\\hat{Y_i})X_i\\]\nDonde:\n\n\\(\\alpha\\) es la tasa de aprendizaje\n\\(\\hat{Y_i}\\) es el valor predicho de la variable dependiente\n\\(Y_i\\) es el valor observado de la variable dependiente\n\\(X_i\\) es el valor de la variable independiente\n\\(n\\) es el número de observaciones\n\\(\\alpha\\) es la tasa de aprendizaje que controla el tamaño de los pasos de actualización de los parámetros\n\n\nDerivación de las Ecuaciones de Descenso del Gradiente*\nAquí está la demostración de la fórmula de actualización de los parámetros en el descenso del gradiente para la regresión lineal simple.\nLa función de costo para la regresión lineal simple es el error cuadrático medio (MSE), que se define como:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})^2\\]\nReemplazando \\(\\hat{Y_i}\\) con la ecuación de regresión lineal simple, obtenemos:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1X_i)^2\\]\nDerivamos respecto a \\(\\beta_0\\) y \\(\\beta_1\\) para obtener las derivadas parciales de la función de costo:\n\\[\\frac{\\partial MSE}{\\partial \\beta_0} = \\frac{-2}{n} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1X_i)\\]\n\\[\\frac{\\partial MSE}{\\partial \\beta_1} = \\frac{-2}{n} \\sum_{i=1}^{n} X_i (Y_i - \\beta_0 - \\beta_1X_i)\\]\nUsamos las derivadas parciales para actualizar los parámetros en la dirección que minimiza la función de costo:\n\\[\\beta_0 = \\beta_0 - \\alpha \\frac{\\partial MSE}{\\partial \\beta_0}\\]\n\\[\\beta_1 = \\beta_1 - \\alpha \\frac{\\partial MSE}{\\partial \\beta_1}\\]\nReemplazando las derivadas parciales, obtenemos la fórmula de actualización de los parámetros en el descenso del gradiente para la regresión lineal simple:\n\\[\\beta_0 = \\beta_0 + \\alpha \\frac{2}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})\\]\n\\[\\beta_1 = \\beta_1 + \\alpha \\frac{2}{n} \\sum_{i=1}^{n} X_i (Y _i - \\hat{Y_i})\\]\nEl vector de coeficientes \\(\\beta\\) se puede actualizar iterativamente hasta que se alcance un criterio de convergencia, como un número máximo de iteraciones o una tolerancia en el cambio de los parámetros.\n\n\nImplementación del Descenso del Gradiente\nVamos a implementar el descenso del gradiente para ajustar un modelo de regresión lineal simple.\n\n\nCódigo\n# Normalización de los datos\nX = (altura - np.mean(altura)) / np.std(altura)\nY = (peso - np.mean(peso)) / np.std(peso)\nn = len(X)\n\n# Valores iniciales de los parámetros\nb0 = 0\nb1 = 0\n\n# Hiperparámetros\nalpha = 1e-3\nepochs = 10000\n\n# Descenso del gradiente\nfor i in range(epochs):\n    y_pred = b0 + b1 * X # Predicción\n\n    db0 = (-2/n) * sum(Y - y_pred) # Derivada parcial de b0\n    db1 = (-2/n) * sum(X * (Y - y_pred)) # Derivada parcial de b1\n\n    b0 = b0 - alpha * db0 # Actualizar b0\n    b1 = b1 - alpha * db1 # Actualizar b1\n\n    if i % 500 == 0: # Imprimir resultados cada 500 iteraciones\n        print(f\"Epoch {i}: b0 = {b0 * np.std(peso) + np.mean(peso) - b1 * np.mean(altura)}, b1 = {b1 * (np.std(peso) / np.std(altura))}\")\n\n# Desnormalizar los parámetros\nb1 = b1 * (np.std(peso) / np.std(altura))\nb0 = b0 * np.std(peso) + np.mean(peso) - b1 * np.mean(altura)\n\n# Imprimir coeficientes\nprint(f\"Los valores óptimos son: b0 = {b0}, b1 = {b1}\")\n\n\nEpoch 0: b0 = 58.86970065046024, b1 = 0.00021469843354638102\nEpoch 500: b0 = 36.69221818906457, b1 = 0.06797607549967058\nEpoch 1000: b0 = 28.541743780104426, b1 = 0.09287914421609095\nEpoch 1500: b0 = 25.546352702386315, b1 = 0.10203130224985668\nEpoch 2000: b0 = 24.44551276862618, b1 = 0.10539482333349699\nEpoch 2500: b0 = 24.040941703173104, b1 = 0.10663095518768421\nEpoch 3000: b0 = 23.89225728322957, b1 = 0.1070852475565854\nEpoch 3500: b0 = 23.83761408547845, b1 = 0.10725220511515218\nEpoch 4000: b0 = 23.817532095303484, b1 = 0.10731356389700908\nEpoch 4500: b0 = 23.81015173789428, b1 = 0.10733611393992575\nEpoch 5000: b0 = 23.8074393734815, b1 = 0.10734440133449522\nEpoch 5500: b0 = 23.80644254903254, b1 = 0.1073474470452729\nEpoch 6000: b0 = 23.806076204828443, b1 = 0.10734856637826272\nEpoch 6500: b0 = 23.80594156921029, b1 = 0.10734897774573438\nEpoch 7000: b0 = 23.80589208910532, b1 = 0.10734912892791008\nEpoch 7500: b0 = 23.805873904609804, b1 = 0.10734918448906133\nEpoch 8000: b0 = 23.805867221603073, b1 = 0.10734920490840964\nEpoch 8500: b0 = 23.805864765522912, b1 = 0.10734921241274988\nEpoch 9000: b0 = 23.80586386288578, b1 = 0.10734921517067947\nEpoch 9500: b0 = 23.805863531156447, b1 = 0.10734921618424971\nLos valores óptimos son: b0 = 40.62639861081885, b1 = 0.1073492165563143\n\n\nLa normalización de los datos es importante para que el descenso del gradiente converja más rápido. En este caso, normalizamos las variables independientes y dependientes restando la media y dividiendo por la desviación estándar. Después de ajustar el modelo, desnormalizamos los parámetros para obtener los valores en la escala original.\nOtro factor importante es la tasa de aprendizaje \\(\\alpha\\), que controla el tamaño de los pasos de actualización de los parámetros. Si la tasa de aprendizaje es demasiado pequeña, el algoritmo puede converger lentamente. Si es demasiado grande, el algoritmo puede divergir, encontrar la tasa de aprendizaje adecuada es crucial para el éxito del descenso del gradiente y muchos algoritmos adaptativos ajustan automáticamente la tasa de aprendizaje durante el entrenamiento para mejorar la convergencia.\nVeremos de nuevo el método de descenso del gradiente cuando hablemos de redes neuronales.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Modelos Lineales"
    ]
  },
  {
    "objectID": "statistics/Linear_models.html#regresión-lineal-múltiple-por-mínimos-cuadrados",
    "href": "statistics/Linear_models.html#regresión-lineal-múltiple-por-mínimos-cuadrados",
    "title": "Modelos Lineales",
    "section": "Regresión Lineal Múltiple por Mínimos Cuadrados",
    "text": "Regresión Lineal Múltiple por Mínimos Cuadrados\nDebido a que ahora tratamos con múltiples variables independientes, la fórmula para calcular los valores de \\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n\\) esta en forma matricial:\n\\[\\beta = (X^TX)^{-1}X^TY\\]\nQué es idénctico a la fórmula de la regresión lineal simple, solo que ahora \\(X\\) es una matriz que contiene las variables independientes y una columna de unos que representa la intersección.\nUsemos datos de ejemplo para ajustar un modelo de regresión lineal múltiple y predecir la variable dependiente. Generaremos datos de 5 variables independientes y una variable dependiente.\n\n\nCódigo\n# Crear datos\nnp.random.seed(1014)\n\n# Variables independientes\nX1 = np.random.normal(0, 1, 100)\nX2 = np.random.normal(10, 5, 100)\nX3 = np.random.normal(-5, 2, 100)\nX4 = np.random.normal(3, 1, 100)\nX5 = np.random.normal(2, 0.5, 100)\n\n# Efecto de cada variable independiente\nbetas = np.array([5, 10, 3, -2, -1, 4])\n\n# Error\nepsilon = np.random.normal(0, 2.5, 100)\n\n# Variable dependiente\nY = betas[0] + betas[1] * X1 + betas[2] * X2 + betas[3] * X3 + betas[4] * X4 + betas[5] * X5 + epsilon\n\ndf = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'X4': X4, 'X5': X5, 'Y': Y})\n\ndf.head()\n\n\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nY\n\n\n\n\n0\n0.759433\n10.731036\n-6.699854\n2.615701\n1.670699\n61.504776\n\n\n1\n-1.007260\n16.029547\n-3.969604\n1.957694\n2.076248\n55.376552\n\n\n2\n-0.644990\n10.329352\n-7.744539\n2.040599\n2.300200\n52.803668\n\n\n3\n-0.266741\n14.039842\n-7.208872\n3.020797\n2.237179\n62.968653\n\n\n4\n0.291256\n13.169584\n-5.827498\n3.217659\n1.341268\n61.591722\n\n\n\n\n\n\n\n\nAhora ajustaremos un modelo de regresión lineal múltiple a los datos y obtendremos un resumen de los resultados.\n\n\nCódigo\n# Ajustar modelo\nX = sm.add_constant(df[['X1', 'X2', 'X3', 'X4', 'X5']])\nmodel = sm.OLS(df['Y'], X).fit()\n\n# Imprimir resumen\nprint(model.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.976\nModel:                            OLS   Adj. R-squared:                  0.975\nMethod:                 Least Squares   F-statistic:                     758.3\nDate:                Sat, 22 Jun 2024   Prob (F-statistic):           2.63e-74\nTime:                        13:41:20   Log-Likelihood:                -243.14\nNo. Observations:                 100   AIC:                             498.3\nDf Residuals:                      94   BIC:                             513.9\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.4953      1.737      3.165      0.002       2.047       8.943\nX1             9.9913      0.310     32.255      0.000       9.376      10.606\nX2             2.8923      0.058     49.999      0.000       2.777       3.007\nX3            -1.9544      0.142    -13.789      0.000      -2.236      -1.673\nX4            -0.7991      0.281     -2.843      0.005      -1.357      -0.241\nX5             4.0200      0.613      6.553      0.000       2.802       5.238\n==============================================================================\nOmnibus:                        2.713   Durbin-Watson:                   2.227\nProb(Omnibus):                  0.258   Jarque-Bera (JB):                1.890\nSkew:                           0.134   Prob(JB):                        0.389\nKurtosis:                       2.383   Cond. No.                         73.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nPodemos imprimir los coeficientes del modelo para ver cómo se relacionan con los valores reales.\n\n\nCódigo\n# Imprimir coeficientes\nprint(f'Intercepto: {model.params[0]}')\nprint(f'Coeficientes: {model.params[1:]}')\n\n\nIntercepto: 5.495327623636475\nCoeficientes: X1    9.991312\nX2    2.892250\nX3   -1.954408\nX4   -0.799148\nX5    4.020035\ndtype: float64\n\n\n/tmp/ipykernel_11165/3979803380.py:2: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\nEl resumen del modelo nos proporciona información sobre la calidad del ajuste, los coeficientes de las variables independientes, los errores estándar de los coeficientes, los valores p, el coeficiente de determinación \\(R^2\\), entre otros.\nGráfiquemos los parámetros estimados y los valores reales.\n\n\nCódigo\n# Graficar parámetros estimados y valores reales\nplt.figure(figsize=(10, 6))\nplt.plot(betas, label='Real', marker='o', markersize=10)\nplt.plot(model.params, label='Estimado', marker='x', markersize=10)\nplt.xlabel('Variables', fontsize=14)\nplt.ylabel('Coeficientes', fontsize=14)\nplt.title('Coeficientes Estimados vs. Coeficientes Reales', fontsize=16, fontweight='bold')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nLa paquetería statsmodels es muy completa y nos permite ajustar modelos de regresión lineal múltiple con facilidad. También podemos usar la función ols de statsmodels.formula.api para ajustar modelos de regresión lineal múltiple con fórmulas de estilo R.\n\n\nCódigo\nimport statsmodels.formula.api as smf\n\n# Ajustar modelo\nmodel = smf.ols('Y ~ X1 + X2 + X3 + X4 + X5', data=df).fit()\n\n# Imprimir resumen\nprint(model.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.976\nModel:                            OLS   Adj. R-squared:                  0.975\nMethod:                 Least Squares   F-statistic:                     758.3\nDate:                Sat, 22 Jun 2024   Prob (F-statistic):           2.63e-74\nTime:                        13:41:20   Log-Likelihood:                -243.14\nNo. Observations:                 100   AIC:                             498.3\nDf Residuals:                      94   BIC:                             513.9\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      5.4953      1.737      3.165      0.002       2.047       8.943\nX1             9.9913      0.310     32.255      0.000       9.376      10.606\nX2             2.8923      0.058     49.999      0.000       2.777       3.007\nX3            -1.9544      0.142    -13.789      0.000      -2.236      -1.673\nX4            -0.7991      0.281     -2.843      0.005      -1.357      -0.241\nX5             4.0200      0.613      6.553      0.000       2.802       5.238\n==============================================================================\nOmnibus:                        2.713   Durbin-Watson:                   2.227\nProb(Omnibus):                  0.258   Jarque-Bera (JB):                1.890\nSkew:                           0.134   Prob(JB):                        0.389\nKurtosis:                       2.383   Cond. No.                         73.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nDos resultados importantes del resumen es la prueba Durbin-Watson y la prueba de Jarque-Bera. La prueba Durbin-Watson se utiliza para detectar la presencia de autocorrelación en los residuos del modelo. Un valor de Durbin-Watson cercano a 2 indica que no hay autocorrelación. La prueba de Jarque-Bera se utiliza para detectar la normalidad de los residuos del modelo. Un valor de Jarque-Bera cercano a 0 indica que los residuos son normales.\nDetectar la presencia de autocorrelación y no normalidad en los residuos es importante para evaluar la calidad del ajuste del modelo y tomar decisiones informadas sobre su uso. Si se obtienen resultados significativos en estas pruebas, es posible que sea necesario realizar ajustes adicionales al modelo para mejorar su rendimiento.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Modelos Lineales"
    ]
  },
  {
    "objectID": "statistics/Linear_models.html#gráfico-q-q",
    "href": "statistics/Linear_models.html#gráfico-q-q",
    "title": "Modelos Lineales",
    "section": "Gráfico Q-Q",
    "text": "Gráfico Q-Q\nEl gráfico Q-Q (quantile-quantile) es una forma visual de evaluar si una muestra de datos proviene de una distribución normal. En un gráfico Q-Q, los cuantiles de la muestra se comparan con los cuantiles de una distribución normal teórica. Si los puntos en el gráfico Q-Q siguen una línea recta, entonces los datos se ajustan a una distribución normal. Evidentemente, si los puntos se alejan de la línea recta, entonces los datos no se ajustan a una distribución normal.\nVamos a generar datos de una distribución normal y una distribución no normal para comparar los gráficos Q-Q.\n\n\nCódigo\n# Datos de una distribución normal\nnp.random.seed(0)\nnormal_data = np.random.normal(0, 1, 1000)\n\n# Datos de una distribución no normal\nnon_normal_data = np.random.exponential(1, 1000)\n\n# Gráfico Q-Q\nfig, ax = plt.subplots(1, 2, figsize=(10, 6))\n\n# Distribución normal\nax[0].set_title('Distribución Normal')\nsm.qqplot(normal_data, line='s', ax=ax[0])\nax[0].set_aspect('equal')\nax[0].set_xlabel('Cuantiles Teóricos')\nax[0].set_ylabel('Cuantiles de los Datos')\n\n# Distribución no normal\nax[1].set_title('Distribución No Normal')\nsm.qqplot(non_normal_data, line='s', ax=ax[1])\nax[1].set_aspect('equal')\nax[1].set_xlabel('Cuantiles Teóricos')\nax[1].set_ylabel('Cuantiles de los Datos')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nEn el gráfico Q-Q de la distribución normal, los puntos siguen una línea recta, lo que indica que los datos se ajustan a una distribución normal. En el gráfico Q-Q de la distribución no normal, los puntos se alejan de la línea recta, lo que indica que los datos no se ajustan a una distribución normal.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Modelos Lineales"
    ]
  },
  {
    "objectID": "statistics/Linear_models.html#prueba-de-shapiro-wilk",
    "href": "statistics/Linear_models.html#prueba-de-shapiro-wilk",
    "title": "Modelos Lineales",
    "section": "Prueba de Shapiro-Wilk",
    "text": "Prueba de Shapiro-Wilk\nLa prueba de Shapiro-Wilk es una prueba estadística que se utiliza para determinar si una muestra de datos proviene de una distribución normal.\nHipótesis:\n\n\\(H_0\\): Los datos provienen de una distribución normal\n\\(H_1\\): Los datos no provienen de una distribución normal\n\nVamos a aplicar la prueba de Shapiro-Wilk a los datos de las distribuciones normal y no normal.\n\n\nCódigo\nfrom scipy.stats import shapiro\n\n# Prueba de Shapiro-Wilk\nstat, p = shapiro(normal_data)\nprint(f'Distribución Normal: Estadístico = {stat}, p-valor = {p}')\n\nstat, p = shapiro(non_normal_data)\nprint(f'Distribución No Normal: Estadístico = {stat}, p-valor = {p}')\n\n\nDistribución Normal: Estadístico = 0.9985554728235057, p-valor = 0.5912267898687746\nDistribución No Normal: Estadístico = 0.8336406754912049, p-valor = 5.034540538267324e-31\n\n\nEl valor p de los datos normales fue de \\(0.5912267898687746\\) y el valor p de los datos no normales fue de \\(5.034540538267324e-31\\). Por lo que podemos rechazar la hipótesis nula en el segundo caso.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Modelos Lineales"
    ]
  },
  {
    "objectID": "statistics/Linear_models.html#regresión-logística",
    "href": "statistics/Linear_models.html#regresión-logística",
    "title": "Modelos Lineales",
    "section": "Regresión Logística",
    "text": "Regresión Logística\nPodemos derivar el modelo de regresión logística a partir del modelo lineal que ya vimos. En el caso de la regresión logística, la variable dependiente \\(y\\) es binaria, es decir, \\(y \\in \\{0, 1\\}\\). Por tanto podemos pensar en modelar la probabilidad de que \\(y = 1\\) en función de las variables independientes \\(x_1, x_2, \\ldots, x_p\\). El complemento de la probabilidad de que \\(y = 1\\) es la probabilidad de que \\(y = 0\\).\nAhora calculemos los “odds” de que \\(y = 1\\):\n\\[ \\text{odds} = \\frac{P(y = 1)}{P(y = 0)} \\]\nLos “odds” son la razón de la probabilidad de que \\(y = 1\\) entre la probabilidad de que \\(y = 0\\). Si los “odds” son mayores a 1, entonces la probabilidad de que \\(y = 1\\) es mayor que la probabilidad de que $y = 0. Si los “odds” son menores a 1, entonces la probabilidad de que \\(y = 0\\) es mayor que la probabilidad de que \\(y = 1\\).\nAhora vamos a suponer que los “odds” de que \\(y = 1\\) son una función lineal de las variables independientes \\(x_1, x_2, \\ldots, x_p\\):\n\\[\\text{odds} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\\]\nUn problema que tienen los “odds” es que están desbalanceados. Por ejemplo, si los “odds” son 10, entonces la probabilidad de que \\(y = 1\\) es 10 veces mayor que la probabilidad de que \\(y = 0\\). Pero si los “odds” son 0.1, entonces la probabilidad de que \\(y = 1\\) es 10 veces menor que la probabilidad de que \\(y = 0\\). Para resolver este problema, vamos a tomar el logaritmo de los “odds” y vamos a modelar el logaritmo de los “odds” en función de las variables independientes \\(x_1, x_2, \\ldots, x_p\\):\n\\[\\log\\left(\\frac{P(y = 1)}{P(y = 0)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\\]\n\\[\\log\\left(\\frac{P(y = 1)}{1 - P(y = 1)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\\]\nAhora solo tenemos que despejar \\(P(y = 1)\\) paso a paso:\n\\[\\log\\left(\\frac{P(y = 1)}{1 - P(y = 1)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\\]\n\\[\\frac{P(y = 1)}{1 - P(y = 1)} = e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}\\]\n\\[P(y = 1) = (1 - P(y = 1))e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}\\]\n\\[P(y = 1) = e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p} - P(y = 1)e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}\\]\n\\[P(y = 1) + P(y = 1)e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p} = e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}\\]\n\\[P(y = 1)(1 + e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}) = e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}\\]\n\\[P(y = 1) = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p}}\\]\n\\[P(y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p)}}\\]\nEsta última ecuación es la función logística. La función logística es una función sigmoide que toma valores entre 0 y 1. Gráficamente, la función logística se ve de la siguiente forma:\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 1000)\ny = 1 / (1 + np.exp(-x))\n\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Función Logística')\nplt.show()\n\n\n\n\n\n\n\n\n\nAhora veamos como se ve la función logística dandole valor a dos coeficientes \\(\\beta_0 = 0\\) y \\(\\beta_1 = 1\\):\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 1000)\nb0 = 5\nb1 = 10\n\ny = 1 / (1 + np.exp(-(b0 + b1 * x)))\n\nplt.plot(x, y, label=r'$y = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}$')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Función Logística')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nAlgunas propiedades de la función logística son:\n\nLa función logística es simétrica respecto al punto \\((0, 0.5)\\).\nLa función logística es monótona creciente.\nLa función logística es acotada entre 0 y 1.\n\n\nEstimar los coeficientes de la regresión logística\nPara estimar los coeficientes de la regresión logística, podemos utilizar el método de máxima verosimilitud. La función de verosimilitud es la probabilidad de observar los datos dados los coeficientes del modelo. La función de verosimilitud de la regresión logística es la siguiente:\n\\[L(\\beta_0, \\beta_1, \\ldots, \\beta_p) = \\prod_{i=1}^{n} P(y_i = 1)^{y_i} (1 - P(y_i = 1))^{1 - y_i}\\]\nDonde \\(n\\) es el número de observaciones, \\(y_i\\) es la variable dependiente de la observación \\(i\\) y \\(P(y_i = 1)\\) es la probabilidad de que la observación \\(i\\) tome el valor 1. La función de verosimilitud es el producto de las probabilidades de que cada observación tome el valor 1 o 0.\nDado que los datos son independientes e idénticamente distribuidos, podemos tomar el logaritmo de la función de verosimilitud para simplificar los cálculos:\n\\[\\log L(\\beta_0, \\beta_1, \\ldots, \\beta_p) = \\sum_{i=1}^{n} y_i \\log P(y_i = 1) + (1 - y_i) \\log (1 - P(y_i = 1))\\]\nAhora solo tenemos que maximizar la función de verosimilitud logarítmica para encontrar los coeficientes del modelo. Para maximizar la función de verosimilitud logarítmica, podemos utilizar el método de Newton-Raphson o con descenso de gradiente. En la práctica, la mayoría de los paquetes de software utilizan el método de Newton-Raphson.\nDado que la explicación de la estimación de los coeficientes de la regresión logística es un poco extensa, vamos a ver un ejemplo en Python utilizando el paquete statsmodels.\n\n\nCódigo\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# Generar datos\nnp.random.seed(0)\nn = 1000\nx1 = np.random.normal(0, 1, n)\nx2 = np.random.normal(0, 1, n)\n\n# Definir coeficientes\nb0 = 5\nb1 = 3\nb2 = 2\n\n# Generar variable dependiente\np = 1 / (1 + np.exp(-(b0 + b1 * x1 + b2 * x2)))\n\ny = np.random.binomial(1, p)\n\n# Crear DataFrame\ndf = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2})\n\n# Muestra de los datos\ndf.head(10)\n\n\n\n\n\n\n\n\n\n\ny\nx1\nx2\n\n\n\n\n0\n1\n1.764052\n0.555963\n\n\n1\n1\n0.400157\n0.892474\n\n\n2\n1\n0.978738\n-0.422315\n\n\n3\n1\n2.240893\n0.104714\n\n\n4\n1\n1.867558\n0.228053\n\n\n5\n1\n-0.977278\n0.201480\n\n\n6\n1\n0.950088\n0.540774\n\n\n7\n1\n-0.151357\n-1.818078\n\n\n8\n1\n-0.103219\n-0.049324\n\n\n9\n1\n0.410599\n0.239034\n\n\n\n\n\n\n\n\nAhora estimemos los coeficientes del modelo de regresión logística:\n\n\nCódigo\n# Estimar modelo\ndata = sm.add_constant(df[['x1', 'x2']])\nmodel = sm.GLM(df['y'], data, family=sm.families.Binomial())\nresult = model.fit()\n\n# Mostrar resultados\nprint(result.summary())\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1000\nModel:                            GLM   Df Residuals:                      997\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -143.21\nDate:                Sat, 22 Jun 2024   Deviance:                       286.43\nTime:                        13:41:21   Pearson chi2:                 1.41e+03\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.2988\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.4077      0.442     12.228      0.000       4.541       6.274\nx1             3.2868      0.317     10.383      0.000       2.666       3.907\nx2             2.0070      0.248      8.093      0.000       1.521       2.493\n==============================================================================\n\n\nComo antes tenemos que añaadir una columna de unos al DataFrame para poder estimar el intercepto del modelo. Después utilizamos la función GLM de statsmodels que estima distribuciones de la familia exponencial. En este caso, estamos utilizando la distribución binomial, con el parámetro family=sm.families.Binomial(). Finalmente, utilizamos el método fit para estimar los coeficientes del modelo.\nPodemos estimar la regresión logística con scikit-learn:\n\n\nCódigo\nfrom sklearn.linear_model import LogisticRegression\n\n# Estimar modelo\nmodel = LogisticRegression()\nmodel.fit(df[['x1', 'x2']], df['y'])\n\n# Imprimir coeficientes\nprint(f'Intercepto: {model.intercept_}')\nprint(f'Coeficientes: {model.coef_}')\n\n\nIntercepto: [4.94144067]\nCoeficientes: [[2.94011   1.7773892]]\n\n\nPodemos probar que tan bueno es el modelo al verificar que predice con los datos que ya tenemos y compararlo con los datos reales:\n\n\nCódigo\n# Predicción\ny_pred = model.predict(df[['x1', 'x2']])\n\n# Comparar predicciones con datos reales\ndf['y_pred'] = y_pred\ndf.head(10)\n\n\n\n\n\n\n\n\n\n\ny\nx1\nx2\ny_pred\n\n\n\n\n0\n1\n1.764052\n0.555963\n1\n\n\n1\n1\n0.400157\n0.892474\n1\n\n\n2\n1\n0.978738\n-0.422315\n1\n\n\n3\n1\n2.240893\n0.104714\n1\n\n\n4\n1\n1.867558\n0.228053\n1\n\n\n5\n1\n-0.977278\n0.201480\n1\n\n\n6\n1\n0.950088\n0.540774\n1\n\n\n7\n1\n-0.151357\n-1.818078\n1\n\n\n8\n1\n-0.103219\n-0.049324\n1\n\n\n9\n1\n0.410599\n0.239034\n1\n\n\n\n\n\n\n\n\nPara comparar modelos de regresión logística, podemos utilizar métricas como la precisión, la sensibilidad, la especificidad, el valor predictivo positivo y el valor predictivo negativo. Estas métricas nos permiten evaluar la capacidad del modelo para predecir correctamente los valores positivos y negativos y se basan en la matriz de confusión.\n\n\nCódigo\nfrom sklearn.metrics import confusion_matrix\n\n# Matriz de confusión\ncm = confusion_matrix(df['y'], df['y_pred'])\nprint(cm)\n\n\n[[ 52  46]\n [ 11 891]]\n\n\nLa matriz de confusión es una tabla que muestra el número de verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos del modelo. A partir de la matriz de confusión, podemos calcular métricas como la precisión, la sensibilidad, la especificidad, el valor predictivo positivo y el valor predictivo negativo.\nLa diagonal de la matriz de confusión contiene los valores correctos, es decir, los verdaderos positivos y los verdaderos negativos. Los valores fuera de la diagonal contienen los valores incorrectos, es decir, los falsos positivos y los falsos negativos.\n\\[\\begin{array}{|c|c|}\n\\hline\n\\text{Verdaderos Positivos (TP)} & \\text{Falsos Positivos (FP)} \\\\\n\\hline\n\\text{Falsos Negativos (FN)} & \\text{Verdaderos Negativos (TN)} \\\\\n\\hline\n\\end{array}\\]\nLa precisión es la proporción de predicciones correctas entre todas las predicciones realizadas por el modelo:\n\\[\\text{Precisión} = \\frac{TP + TN}{TP + FP + FN + TN}\\]\nLa sensibilidad es la proporción de verdaderos positivos entre todos los valores positivos reales:\n\\[\\text{Sensibilidad} = \\frac{TP}{TP + FN}\\]\nLa especificidad es la proporción de verdaderos negativos entre todos los valores negativos reales:\n\\[\\text{Especificidad} = \\frac{TN}{TN + FP}\\]\nEl valor predictivo positivo es la proporción de verdaderos positivos entre todas las predicciones positivas realizadas por el modelo:\n\\[\\text{Valor Predictivo Positivo} = \\frac{TP}{TP + FP}\\]\nEl valor predictivo negativo es la proporción de verdaderos negativos entre todas las predicciones negativas realizadas por el modelo:\n\\[\\text{Valor Predictivo Negativo} = \\frac{TN}{TN + FN}\\]\nEn nuestro modelo de regresión logística tenemos los siguientes valores:\n\n\nCódigo\n# Precisión\nprecision = (cm[0, 0] + cm[1, 1]) / np.sum(cm)\nprint(f'Precisión: {precision}')\n\n# Especificidad\nspecificity = cm[1, 1] / (cm[1, 1] + cm[0, 1])\nprint(f'Especificidad: {specificity}')\n\n# Sensibilidad\nsensitivity = cm[0, 0] / (cm[0, 0] + cm[1, 0])\nprint(f'Sensibilidad: {sensitivity}')\n\n# Valor Predictivo Positivo\nppv = cm[0, 0] / (cm[0, 0] + cm[0, 1])\nprint(f'Valor Predictivo Positivo: {ppv}')\n\n# Valor Predictivo Negativo\nnpv = cm[1, 1] / (cm[1, 1] + cm[1, 0])\nprint(f'Valor Predictivo Negativo: {npv}')\n\n\nPrecisión: 0.943\nEspecificidad: 0.9509071504802561\nSensibilidad: 0.8253968253968254\nValor Predictivo Positivo: 0.5306122448979592\nValor Predictivo Negativo: 0.9878048780487805\n\n\nEstas métricas son usadas también en la evaluación de modelos de clasificación como lo son las redes neuronales.\nOtras métricas comunes para evaluar modelos de clasificación son el área bajo la curva ROC (AUC-ROC) y el área bajo la curva PR (AUC-PR). El AUC-ROC mide la capacidad del modelo para distinguir entre las clases positiva y negativa, mientras que el AUC-PR mide la precisión del modelo en la clase positiva.\n\n\nCódigo\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n\n# Área bajo la curva ROC\nroc_auc = roc_auc_score(df['y'], df['y_pred'])\n\n# Área bajo la curva PR\npr_auc = average_precision_score(df['y'], df['y_pred'])\n\nprint(f'Área bajo la curva ROC: {roc_auc}')\nprint(f'Área bajo la curva PR: {pr_auc}')\n\n\nÁrea bajo la curva ROC: 0.7592085614733698\nÁrea bajo la curva PR: 0.9503107218158627\n\n\nEl AUC-ROC y el AUC-PR son valores entre 0 y 1, donde un valor de 1 indica un modelo perfecto y un valor de 0.5 indica un modelo aleatorio.\nPodemos graficar la curva ROC y la curva PR para visualizar la calidad del modelo de regresión logística.\n\n\nCódigo\nfrom sklearn.metrics import roc_curve, precision_recall_curve\n\n# Curva ROC\nfpr, tpr, _ = roc_curve(df['y'], df['y_pred'])\n\n# Curva PR\nprecision, recall, _ = precision_recall_curve(df['y'], df['y_pred'])\n\n# Graficar curva ROC\nfig, ax = plt.subplots(1, 2, figsize=(10, 6))\n\nax[0].plot(fpr, tpr)\nax[0].plot([0, 1], [0, 1], linestyle='--', color='gray')\nax[0].set_xlabel('Tasa de Falsos Positivos')\nax[0].set_ylabel('Tasa de Verdaderos Positivos')\nax[0].set_title('Curva ROC')\n\n# Graficar curva PR\nax[1].plot(recall, precision)\nax[1].set_xlabel('Recall')\nax[1].set_ylabel('Precision')\nax[1].set_title('Curva PR')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nLa forma de la curva ROC y la curva PR nos permite evaluar la calidad del modelo de regresión logística. Una curva ROC que se acerca al punto (0, 1) y una curva PR que se acerca al punto (1, 1) indican un modelo de alta calidad. Cada punto en la curva ROC representa una combinación de la tasa de verdaderos positivos y la tasa de falsos positivos del modelo, mientras que cada punto en la curva PR representa una combinación de la precisión y el recall del modelo. El recall es la sensibilidad del modelo, es decir, la proporción de verdaderos positivos entre todos los valores positivos reales.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Modelos Lineales"
    ]
  },
  {
    "objectID": "statistics/data-visualization.html",
    "href": "statistics/data-visualization.html",
    "title": "Visualización de datos",
    "section": "",
    "text": "La visualización de datos es una herramienta fundamental en el análisis de datos. Nos permite entender de manera rápida y sencilla la información que tenemos a nuestra disposición. En este tutorial vamos a ver cómo podemos visualizar datos en Python utilizando la librería matplotlib y seaborn, que son las librerías más populares para visualización de datos en Python.\nEn Google Colab, podemos visualizar datos utilizando las librerías matplotlib y seaborn ya que vienen instaladas por defecto. Para instalar estas librerías de forma local, podemos utilizar el siguiente comando en una celda de Jupyter Notebook:\n!pip install matplotlib seaborn\nSi estas usando un archivo con extensión .py, puedes instalar las librerías utilizando el comando en la terminal:\npip install matplotlib seaborn\n\n\nMatplotlib es una librería de visualización de datos en 2D que produce gráficos de alta calidad en una variedad de formatos y entornos. En este tutorial vamos a ver cómo podemos utilizar matplotlib para visualizar datos en Python.\nUtilizaremos los datos de hongos que vimos en el tutorial anterior para hacer una visualización de los datos. Primero, vamos a cargar los datos.\n\n\nCódigo\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/main/data/archive/mushroom_cleaned.csv\"\ndata = pd.read_csv(url)\ndata.head()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nstem-color\nseason\nclass\n\n\n\n\n0\n1372\n2\n2\n10\n3.807467\n1545\n11\n1.804273\n1\n\n\n1\n1461\n2\n2\n10\n3.807467\n1557\n11\n1.804273\n1\n\n\n2\n1371\n2\n2\n10\n3.612496\n1566\n11\n1.804273\n1\n\n\n3\n1261\n6\n2\n10\n3.787572\n1566\n11\n1.804273\n1\n\n\n4\n1305\n6\n2\n10\n3.711971\n1464\n11\n0.943195\n1\n\n\n\n\n\n\n\n\nUna gran ventaja de usar pandas y matplotlib juntos es que podemos visualizar los datos directamente desde un DataFrame. Por ejemplo, podemos hacer un histograma de la variable cap-shape.\n\n\nCódigo\nimport matplotlib.pyplot as plt\n\ndata[\"cap-shape\"].value_counts().plot(kind=\"bar\")\nplt.title(\"Distribución de la forma del sombrero\")\nplt.show()\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el método value_counts() para contar el número de valores únicos en la columna cap-shape y luego utilizamos el método plot() para hacer un histograma de los valores. Finalmente, utilizamos el método show() para mostrar el gráfico.\nEl parametro kind nos permite especificar el tipo de gráfico que queremos hacer. En este caso, utilizamos bar para hacer un histograma de barras. De acuerdo a la documentación de plot(), los valores válidos para el parametro kind son:\n\nline: Gráfico de línea.\nbar: Gráfico de barras.\nbarh: Gráfico de barras horizontales.\nhist: Histograma.\nbox: Diagrama de caja.\nkde: Estimación de densidad de kernel.\ndensity: Estimación de densidad.\narea: Gráfico de área.\npie: Gráfico de pastel.\nscatter: Gráfico de dispersión.\nhexbin.: Gráfico que puede considerarse una mezcla de un histograma y un gráfico de dispersión.\n\n\nAdemás, podemos personalizar el gráfico utilizando los métodos de matplotlib. Por ejemplo, podemos cambiar el color de las barras y agregar etiquetas a los ejes. A pesar de la utilidad de pandas y matplotlib, a veces es necesario utilizar matplotlib directamente para hacer gráficos más complejos.\nEl primer paso para crear un gráfico con matplotlib es crear una figura y un eje. Luego, podemos utilizar los métodos de matplotlib para personalizar el gráfico. Por ejemplo, podemos hacer un gráfico de dispersión de las variables cap-diameter y stem-height.\n\n\nCódigo\nfig, ax = plt.subplots()\n\nax.scatter(data[\"cap-diameter\"], data[\"stem-height\"])\nax.set_title(\"Gráfico de dispersión de diámetro del sombrero y altura del tallo\")\nax.set_xlabel(\"Diámetro del sombrero\")\nax.set_ylabel(\"Altura del tallo\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el método subplots() para crear una figura y un eje. Luego, utilizamos el método scatter() para hacer un gráfico de dispersión de las variables cap-diameter y stem-height. Finalmente, utilizamos los métodos set_title(), set_xlabel() y set_ylabel() para personalizar el gráfico.\nEditemos aún más el gráfico para que sea más informativo. Podemos cambiar el color de los puntos, agregar una leyenda y cambiar el tamaño de los puntos.\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.scatter(data[\"cap-diameter\"], data[\"stem-height\"], c=data[\"class\"], s=50, alpha=0.25)\nax.set_title(\"Gráfico de dispersión de diámetro del sombrero y altura del tallo\")\nax.set_xlabel(\"Diámetro del sombrero\")\nax.set_ylabel(\"Altura del tallo\")\nax.legend(labels=[\"Comestible\", \"Venenoso\"], title=\"Clase\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el parametro c para cambiar el color de los puntos de acuerdo a la variable class, el parametro s para cambiar el tamaño de los puntos y el parametro alpha para cambiar la transparencia de los puntos. Además, utilizamos el método legend() para agregar una leyenda al gráfico.\nAhora veamos como hacer un histograma de las variables cap-diameter usando los métodos de matplotlib.\n\n\nCódigo\nfig, ax = plt.subplots()\n\nax.hist(data[\"cap-diameter\"], bins=20, color=\"skyblue\", edgecolor=\"black\")\nax.set_title(\"Histograma del diámetro del sombrero\")\nax.set_xlabel(\"Diámetro del sombrero\")\nax.set_ylabel(\"Frecuencia\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el método hist() para hacer un histograma de la variable cap-diameter. El parametro bins nos permite especificar el número de contenedores en el histograma. Además, utilizamos los parametros color y edgecolor para cambiar el color de las barras y el color de los bordes de las barras.\nAhora filtremos los datos para los hongos comestibles.\n\n\nCódigo\nedible = data[data[\"class\"] == 1]\n\nedible = edible[[\"cap-diameter\", \"stem-height\"]]\n\nedible.head()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\nstem-height\n\n\n\n\n0\n1372\n3.807467\n\n\n1\n1461\n3.807467\n\n\n2\n1371\n3.612496\n\n\n3\n1261\n3.787572\n\n\n4\n1305\n3.711971\n\n\n\n\n\n\n\n\nHagamos un gráfico de hexbin de las variables cap-diameter y stem-height para los hongos comestibles.\n\n\nCódigo\nfig = plt.figure(figsize=(10, 6))\n\nax = edible.plot.hexbin(x=\"cap-diameter\", y=\"stem-height\", gridsize=20, cmap=\"coolwarm\")\n\nax.set_title(\"Gráfico de hexbin de diámetro del sombrero y altura del tallo para hongos comestibles\")\nax.set_xlabel(\"Diámetro del sombrero\")\nax.set_ylabel(\"Altura del tallo\")\n\nplt.show()\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nEn un gráfico de hexbin, los puntos se agrupan en hexágonos y el color de los hexágonos se basa en el número de puntos en cada hexágono. En este caso, utilizamos el parametro gridsize para especificar el tamaño de los hexágonos y el parametro cmap para cambiar el mapa de colores.\nLa sintaxis que usamos aquí es otra forma de hacer los gráficos. Primerop creamos la figura con plt.figure() y luego utilizamos el método plot.hexbin() para hacer el gráfico de hexbin que nos ofrece la clase DataFrame de pandas. Finalmente, utilizamos los métodos set_title(), set_xlabel() y set_ylabel() para personalizar el gráfico.\n\n\n\nSeaborn es una librería de visualización de datos en Python que se basa en matplotlib y proporciona una interfaz de alto nivel para crear gráficos atractivos y informativos. En este tutorial vamos a ver cómo podemos utilizar seaborn para visualizar datos en Python.\nPrimero, hagamos un gráfico de cajas de las variables cap-diameter para los hongos comestibles y venenosos.\n\n\nCódigo\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.boxplot(x=\"class\", y=\"cap-diameter\", data=data, ax=ax)\nax.set_title(\"Diagrama de caja del diámetro del sombrero por clase\")\nax.set_xlabel(\"Clase\")\nax.set_ylabel(\"Diámetro del sombrero\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nPersonalicemos el gráfico para que sea más informativo. Podemos cambiar el color de las cajas y los bigotes, agregar una leyenda y cambiar el tamaño de las cajas.\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.boxplot(hue=\"class\", x=\"class\", y=\"cap-diameter\", data=data, ax=ax, palette=\"Set2\", linewidth=2)\n\nax.set_title(\"Diagrama de caja del diámetro del sombrero por clase\")\nax.set_xlabel(\"Clase\")\nax.set_ylabel(\"Diámetro del sombrero\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAhora aumentemos el tamaño de la letra de las etiquetas de los ejes, agreguemos que los valores atípicos se muestren como puntos y cambiemos el estilo de las cajas.\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.boxplot(hue=\"class\", x=\"class\", y=\"cap-diameter\", data=data, ax=ax, \n            palette=\"Set2\", linewidth=2, fliersize=5, notch=True)\n\nax.set_title(\"Diagrama de caja del diámetro del sombrero por clase\", fontsize=16)\nax.set_xlabel(\"Clase\", fontsize=14)\nax.set_ylabel(\"Diámetro del sombrero\", fontsize=14)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nUsemos los mismos datos para crear un gráfico de violín de las variables cap-diameter para los hongos comestibles y venenosos.\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.violinplot(x=\"class\", y=\"cap-diameter\", data=data, ax=ax, hue=\"class\", palette=\"Set1\")\n\nax.set_title(\"Gráfico de violín del diámetro del sombrero por clase\", fontsize=16, fontweight=\"bold\")\nax.set_xlabel(\"Clase\", fontsize=14)\nax.set_ylabel(\"Diámetro del sombrero\", fontsize=14)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nEn un gráfico de violín, la forma del violín representa la distribución de los datos. En este caso, utilizamos el parametro hue para cambiar el color de los violines de acuerdo a la variable class y el parametro palette para cambiar la paleta de colores. Nos incluye un gráfico de caja en el interior del violín, que nos permite ver la distribución de los datos de una manera más detallada. Modifiquemos el gráfico para que sea más informativo.\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.violinplot(x=\"class\", y=\"cap-diameter\", data=data, ax=ax, \n                hue=\"class\", palette=\"Set1\", inner=\"box\", fill=False,\n                inner_kws={\"linewidth\": 1.5, \"box_width\": 17, \"whis_width\":2})\n\nax.set_title(\"Gráfico de violín del diámetro del sombrero por clase\", fontsize=16, fontweight=\"bold\")\nax.set_xlabel(\"Clase\", fontsize=14)\nax.set_ylabel(\"Diámetro del sombrero\", fontsize=14)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAquí hemos especificado que el interior del violín sea un gráfico de caja con el parametro inner=\"box\". Además, hemos cambiado el ancho de las cajas con el parametro inner_kws={\"box_width\": 17} y el ancho de los bigotes con el parametro inner_kws={\"whis_width\": 2}. Con el parametro fill=False, hemos eliminado el relleno del violín.\nUn gráfico útil a la hora de analizar datos es la matriz de correlación. La matriz de correlación nos permite ver cómo se relacionan las variables entre sí. En seaborn, podemos hacer una matriz de correlación utilizando el método heatmap().\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncorr = data.corr()\n\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", ax=ax, vmax=1, vmin=-1)\n\nax.set_title(\"Matriz de correlación de las variables\", fontsize=16, fontweight=\"bold\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el método corr() para calcular la matriz de correlación de los datos y luego utilizamos el método heatmap() para hacer una matriz de correlación. El parametro annot=True nos permite mostrar los valores de la correlación en los cuadros y el parametro cmap=\"coolwarm\" nos permite cambiar el mapa de colores. Los parametros vmax y vmin nos permiten especificar los valores máximos y mínimos de la escala de colores, sino se dan, se toman los valores máximos y mínimos de la matriz de correlación.\nUna buena forma de presentar los datos de dos variables es juntar un gráfico de dispersión con un histograma. En seaborn, podemos hacer un gráfico de dispersión con histogramas utilizando el método jointplot().\n\n\nCódigo\nfig = plt.figure(figsize=(10, 6))\n\nsns.jointplot(x=\"cap-diameter\", y=\"stem-height\", data=data, kind=\"scatter\", color=\"skyblue\")\n\nplt.show()\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el parametro kind=\"scatter\" para hacer un gráfico de dispersión. Los valores válidos para el parametro kind son:\n\nscatter: Gráfico de dispersión.\nkde: Estimación de densidad de kernel.\nhist: Histograma.\nhex: Gráfico de hexbin.\nreg: Gráfico de regresión.\nresid: Gráfico de residuos.\n\nAdemás con el parámetro hue podemos cambiar el color de los puntos de acuerdo a una variable categórica.\n\n\nCódigo\nfig = plt.figure(figsize=(10, 6))\n\nax = sns.jointplot(x=\"cap-diameter\", y=\"stem-height\", data=data, \n                    kind=\"scatter\", color=\"skyblue\", hue=\"class\")\n\nax.set_axis_labels(\"Diámetro del sombrero\", \"Altura del tallo\", fontsize=12)\nax.fig.suptitle(\"Gráfico de dispersión de diámetro del sombrero y altura del tallo\", \n                fontsize=14, fontweight=\"bold\", y = 1.02)\n\nplt.show()\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nEste gráfico nos permite ver la relación entre dos variables y la distribución de cada variable de forma simultánea. Vemos que no hay diferencias claras entre las variables cap-diameter y stem-height para los hongos comestibles y venenosos.\nSeaborn tiene una función para mostrar gráficos con la relación entre todas las variables de un DataFrame. Esta función se llama pairplot().\n\n\nCódigo\nfig = plt.figure(figsize=(10, 6))\n\nsns.pairplot(data, hue=\"class\", palette=\"Set1\")\n\nplt.show()\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nSi son muchas variables, podemos seleccionar solo algunas para hacer el gráfico. En este caso, seleccionamos las variables cap-diameter, stem-height, cap-surface y cap-color.\n\n\nCódigo\nfig = plt.figure(figsize=(10, 6))\n\nsns.pairplot(data[[\"cap-diameter\", \"stem-height\", \"class\"]], hue=\"class\", palette=\"Set1\")\n\nplt.show()\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nNos mostrará un gráfico de dispersión de todas las variables seleccionadas y un histograma de cada variable en la diagonal. Además, podemos cambiar el color de los puntos de acuerdo a una variable categórica con el parametro hue.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Visualización de datos"
    ]
  },
  {
    "objectID": "statistics/data-visualization.html#matplotlib",
    "href": "statistics/data-visualization.html#matplotlib",
    "title": "Visualización de datos",
    "section": "",
    "text": "Matplotlib es una librería de visualización de datos en 2D que produce gráficos de alta calidad en una variedad de formatos y entornos. En este tutorial vamos a ver cómo podemos utilizar matplotlib para visualizar datos en Python.\nUtilizaremos los datos de hongos que vimos en el tutorial anterior para hacer una visualización de los datos. Primero, vamos a cargar los datos.\n\n\nCódigo\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/Christian-F-Badillo/Ciencia-de-datos-con-Python-de-estadistica-descriptiva-a-redes-neuronales/main/data/archive/mushroom_cleaned.csv\"\ndata = pd.read_csv(url)\ndata.head()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\ncap-shape\ngill-attachment\ngill-color\nstem-height\nstem-width\nstem-color\nseason\nclass\n\n\n\n\n0\n1372\n2\n2\n10\n3.807467\n1545\n11\n1.804273\n1\n\n\n1\n1461\n2\n2\n10\n3.807467\n1557\n11\n1.804273\n1\n\n\n2\n1371\n2\n2\n10\n3.612496\n1566\n11\n1.804273\n1\n\n\n3\n1261\n6\n2\n10\n3.787572\n1566\n11\n1.804273\n1\n\n\n4\n1305\n6\n2\n10\n3.711971\n1464\n11\n0.943195\n1\n\n\n\n\n\n\n\n\nUna gran ventaja de usar pandas y matplotlib juntos es que podemos visualizar los datos directamente desde un DataFrame. Por ejemplo, podemos hacer un histograma de la variable cap-shape.\n\n\nCódigo\nimport matplotlib.pyplot as plt\n\ndata[\"cap-shape\"].value_counts().plot(kind=\"bar\")\nplt.title(\"Distribución de la forma del sombrero\")\nplt.show()\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el método value_counts() para contar el número de valores únicos en la columna cap-shape y luego utilizamos el método plot() para hacer un histograma de los valores. Finalmente, utilizamos el método show() para mostrar el gráfico.\nEl parametro kind nos permite especificar el tipo de gráfico que queremos hacer. En este caso, utilizamos bar para hacer un histograma de barras. De acuerdo a la documentación de plot(), los valores válidos para el parametro kind son:\n\nline: Gráfico de línea.\nbar: Gráfico de barras.\nbarh: Gráfico de barras horizontales.\nhist: Histograma.\nbox: Diagrama de caja.\nkde: Estimación de densidad de kernel.\ndensity: Estimación de densidad.\narea: Gráfico de área.\npie: Gráfico de pastel.\nscatter: Gráfico de dispersión.\nhexbin.: Gráfico que puede considerarse una mezcla de un histograma y un gráfico de dispersión.\n\n\nAdemás, podemos personalizar el gráfico utilizando los métodos de matplotlib. Por ejemplo, podemos cambiar el color de las barras y agregar etiquetas a los ejes. A pesar de la utilidad de pandas y matplotlib, a veces es necesario utilizar matplotlib directamente para hacer gráficos más complejos.\nEl primer paso para crear un gráfico con matplotlib es crear una figura y un eje. Luego, podemos utilizar los métodos de matplotlib para personalizar el gráfico. Por ejemplo, podemos hacer un gráfico de dispersión de las variables cap-diameter y stem-height.\n\n\nCódigo\nfig, ax = plt.subplots()\n\nax.scatter(data[\"cap-diameter\"], data[\"stem-height\"])\nax.set_title(\"Gráfico de dispersión de diámetro del sombrero y altura del tallo\")\nax.set_xlabel(\"Diámetro del sombrero\")\nax.set_ylabel(\"Altura del tallo\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el método subplots() para crear una figura y un eje. Luego, utilizamos el método scatter() para hacer un gráfico de dispersión de las variables cap-diameter y stem-height. Finalmente, utilizamos los métodos set_title(), set_xlabel() y set_ylabel() para personalizar el gráfico.\nEditemos aún más el gráfico para que sea más informativo. Podemos cambiar el color de los puntos, agregar una leyenda y cambiar el tamaño de los puntos.\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.scatter(data[\"cap-diameter\"], data[\"stem-height\"], c=data[\"class\"], s=50, alpha=0.25)\nax.set_title(\"Gráfico de dispersión de diámetro del sombrero y altura del tallo\")\nax.set_xlabel(\"Diámetro del sombrero\")\nax.set_ylabel(\"Altura del tallo\")\nax.legend(labels=[\"Comestible\", \"Venenoso\"], title=\"Clase\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el parametro c para cambiar el color de los puntos de acuerdo a la variable class, el parametro s para cambiar el tamaño de los puntos y el parametro alpha para cambiar la transparencia de los puntos. Además, utilizamos el método legend() para agregar una leyenda al gráfico.\nAhora veamos como hacer un histograma de las variables cap-diameter usando los métodos de matplotlib.\n\n\nCódigo\nfig, ax = plt.subplots()\n\nax.hist(data[\"cap-diameter\"], bins=20, color=\"skyblue\", edgecolor=\"black\")\nax.set_title(\"Histograma del diámetro del sombrero\")\nax.set_xlabel(\"Diámetro del sombrero\")\nax.set_ylabel(\"Frecuencia\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el método hist() para hacer un histograma de la variable cap-diameter. El parametro bins nos permite especificar el número de contenedores en el histograma. Además, utilizamos los parametros color y edgecolor para cambiar el color de las barras y el color de los bordes de las barras.\nAhora filtremos los datos para los hongos comestibles.\n\n\nCódigo\nedible = data[data[\"class\"] == 1]\n\nedible = edible[[\"cap-diameter\", \"stem-height\"]]\n\nedible.head()\n\n\n\n\n\n\n\n\n\n\ncap-diameter\nstem-height\n\n\n\n\n0\n1372\n3.807467\n\n\n1\n1461\n3.807467\n\n\n2\n1371\n3.612496\n\n\n3\n1261\n3.787572\n\n\n4\n1305\n3.711971\n\n\n\n\n\n\n\n\nHagamos un gráfico de hexbin de las variables cap-diameter y stem-height para los hongos comestibles.\n\n\nCódigo\nfig = plt.figure(figsize=(10, 6))\n\nax = edible.plot.hexbin(x=\"cap-diameter\", y=\"stem-height\", gridsize=20, cmap=\"coolwarm\")\n\nax.set_title(\"Gráfico de hexbin de diámetro del sombrero y altura del tallo para hongos comestibles\")\nax.set_xlabel(\"Diámetro del sombrero\")\nax.set_ylabel(\"Altura del tallo\")\n\nplt.show()\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nEn un gráfico de hexbin, los puntos se agrupan en hexágonos y el color de los hexágonos se basa en el número de puntos en cada hexágono. En este caso, utilizamos el parametro gridsize para especificar el tamaño de los hexágonos y el parametro cmap para cambiar el mapa de colores.\nLa sintaxis que usamos aquí es otra forma de hacer los gráficos. Primerop creamos la figura con plt.figure() y luego utilizamos el método plot.hexbin() para hacer el gráfico de hexbin que nos ofrece la clase DataFrame de pandas. Finalmente, utilizamos los métodos set_title(), set_xlabel() y set_ylabel() para personalizar el gráfico.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Visualización de datos"
    ]
  },
  {
    "objectID": "statistics/data-visualization.html#seaborn",
    "href": "statistics/data-visualization.html#seaborn",
    "title": "Visualización de datos",
    "section": "",
    "text": "Seaborn es una librería de visualización de datos en Python que se basa en matplotlib y proporciona una interfaz de alto nivel para crear gráficos atractivos y informativos. En este tutorial vamos a ver cómo podemos utilizar seaborn para visualizar datos en Python.\nPrimero, hagamos un gráfico de cajas de las variables cap-diameter para los hongos comestibles y venenosos.\n\n\nCódigo\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.boxplot(x=\"class\", y=\"cap-diameter\", data=data, ax=ax)\nax.set_title(\"Diagrama de caja del diámetro del sombrero por clase\")\nax.set_xlabel(\"Clase\")\nax.set_ylabel(\"Diámetro del sombrero\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nPersonalicemos el gráfico para que sea más informativo. Podemos cambiar el color de las cajas y los bigotes, agregar una leyenda y cambiar el tamaño de las cajas.\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.boxplot(hue=\"class\", x=\"class\", y=\"cap-diameter\", data=data, ax=ax, palette=\"Set2\", linewidth=2)\n\nax.set_title(\"Diagrama de caja del diámetro del sombrero por clase\")\nax.set_xlabel(\"Clase\")\nax.set_ylabel(\"Diámetro del sombrero\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAhora aumentemos el tamaño de la letra de las etiquetas de los ejes, agreguemos que los valores atípicos se muestren como puntos y cambiemos el estilo de las cajas.\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.boxplot(hue=\"class\", x=\"class\", y=\"cap-diameter\", data=data, ax=ax, \n            palette=\"Set2\", linewidth=2, fliersize=5, notch=True)\n\nax.set_title(\"Diagrama de caja del diámetro del sombrero por clase\", fontsize=16)\nax.set_xlabel(\"Clase\", fontsize=14)\nax.set_ylabel(\"Diámetro del sombrero\", fontsize=14)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nUsemos los mismos datos para crear un gráfico de violín de las variables cap-diameter para los hongos comestibles y venenosos.\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.violinplot(x=\"class\", y=\"cap-diameter\", data=data, ax=ax, hue=\"class\", palette=\"Set1\")\n\nax.set_title(\"Gráfico de violín del diámetro del sombrero por clase\", fontsize=16, fontweight=\"bold\")\nax.set_xlabel(\"Clase\", fontsize=14)\nax.set_ylabel(\"Diámetro del sombrero\", fontsize=14)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nEn un gráfico de violín, la forma del violín representa la distribución de los datos. En este caso, utilizamos el parametro hue para cambiar el color de los violines de acuerdo a la variable class y el parametro palette para cambiar la paleta de colores. Nos incluye un gráfico de caja en el interior del violín, que nos permite ver la distribución de los datos de una manera más detallada. Modifiquemos el gráfico para que sea más informativo.\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.violinplot(x=\"class\", y=\"cap-diameter\", data=data, ax=ax, \n                hue=\"class\", palette=\"Set1\", inner=\"box\", fill=False,\n                inner_kws={\"linewidth\": 1.5, \"box_width\": 17, \"whis_width\":2})\n\nax.set_title(\"Gráfico de violín del diámetro del sombrero por clase\", fontsize=16, fontweight=\"bold\")\nax.set_xlabel(\"Clase\", fontsize=14)\nax.set_ylabel(\"Diámetro del sombrero\", fontsize=14)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAquí hemos especificado que el interior del violín sea un gráfico de caja con el parametro inner=\"box\". Además, hemos cambiado el ancho de las cajas con el parametro inner_kws={\"box_width\": 17} y el ancho de los bigotes con el parametro inner_kws={\"whis_width\": 2}. Con el parametro fill=False, hemos eliminado el relleno del violín.\nUn gráfico útil a la hora de analizar datos es la matriz de correlación. La matriz de correlación nos permite ver cómo se relacionan las variables entre sí. En seaborn, podemos hacer una matriz de correlación utilizando el método heatmap().\n\n\nCódigo\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncorr = data.corr()\n\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", ax=ax, vmax=1, vmin=-1)\n\nax.set_title(\"Matriz de correlación de las variables\", fontsize=16, fontweight=\"bold\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el método corr() para calcular la matriz de correlación de los datos y luego utilizamos el método heatmap() para hacer una matriz de correlación. El parametro annot=True nos permite mostrar los valores de la correlación en los cuadros y el parametro cmap=\"coolwarm\" nos permite cambiar el mapa de colores. Los parametros vmax y vmin nos permiten especificar los valores máximos y mínimos de la escala de colores, sino se dan, se toman los valores máximos y mínimos de la matriz de correlación.\nUna buena forma de presentar los datos de dos variables es juntar un gráfico de dispersión con un histograma. En seaborn, podemos hacer un gráfico de dispersión con histogramas utilizando el método jointplot().\n\n\nCódigo\nfig = plt.figure(figsize=(10, 6))\n\nsns.jointplot(x=\"cap-diameter\", y=\"stem-height\", data=data, kind=\"scatter\", color=\"skyblue\")\n\nplt.show()\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nEn este caso, utilizamos el parametro kind=\"scatter\" para hacer un gráfico de dispersión. Los valores válidos para el parametro kind son:\n\nscatter: Gráfico de dispersión.\nkde: Estimación de densidad de kernel.\nhist: Histograma.\nhex: Gráfico de hexbin.\nreg: Gráfico de regresión.\nresid: Gráfico de residuos.\n\nAdemás con el parámetro hue podemos cambiar el color de los puntos de acuerdo a una variable categórica.\n\n\nCódigo\nfig = plt.figure(figsize=(10, 6))\n\nax = sns.jointplot(x=\"cap-diameter\", y=\"stem-height\", data=data, \n                    kind=\"scatter\", color=\"skyblue\", hue=\"class\")\n\nax.set_axis_labels(\"Diámetro del sombrero\", \"Altura del tallo\", fontsize=12)\nax.fig.suptitle(\"Gráfico de dispersión de diámetro del sombrero y altura del tallo\", \n                fontsize=14, fontweight=\"bold\", y = 1.02)\n\nplt.show()\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nEste gráfico nos permite ver la relación entre dos variables y la distribución de cada variable de forma simultánea. Vemos que no hay diferencias claras entre las variables cap-diameter y stem-height para los hongos comestibles y venenosos.\nSeaborn tiene una función para mostrar gráficos con la relación entre todas las variables de un DataFrame. Esta función se llama pairplot().\n\n\nCódigo\nfig = plt.figure(figsize=(10, 6))\n\nsns.pairplot(data, hue=\"class\", palette=\"Set1\")\n\nplt.show()\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nSi son muchas variables, podemos seleccionar solo algunas para hacer el gráfico. En este caso, seleccionamos las variables cap-diameter, stem-height, cap-surface y cap-color.\n\n\nCódigo\nfig = plt.figure(figsize=(10, 6))\n\nsns.pairplot(data[[\"cap-diameter\", \"stem-height\", \"class\"]], hue=\"class\", palette=\"Set1\")\n\nplt.show()\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nNos mostrará un gráfico de dispersión de todas las variables seleccionadas y un histograma de cada variable en la diagonal. Además, podemos cambiar el color de los puntos de acuerdo a una variable categórica con el parametro hue.",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Visualización de datos"
    ]
  },
  {
    "objectID": "statistics/data-visualization.html#conclusiones",
    "href": "statistics/data-visualization.html#conclusiones",
    "title": "Visualización de datos",
    "section": "Conclusiones",
    "text": "Conclusiones\nEn este tutorial, hemos visto cómo podemos visualizar datos en Python utilizando las librerías matplotlib y seaborn. Hemos visto cómo hacer gráficos de barras, gráficos de dispersión, histogramas, diagramas de caja, gráficos de violín, matrices de correlación, gráficos de dispersión con histogramas y gráficos de pares. Estas son solo algunas de las muchas formas en que podemos visualizar datos en Python. La visualización de datos es una herramienta poderosa que nos permite entender de manera rápida y sencilla la información que tenemos a nuestra disposición.\nPara más información sobre matplotlib y seaborn, puedes consultar la documentación oficial:\n\nDocumentación de Matplotlib\nDocumentación de Seaborn",
    "crumbs": [
      "Manejo de Datos y Estadística con Python",
      "Visualización de datos"
    ]
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Pyodide in Quarto HTML Documents",
    "section": "",
    "text": "This is a pyodide-enabled code cell in a Quarto HTML document.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n Volver arriba"
  }
]