{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Introducción a Redes Neuronales\"\n",
        "author: \"Christian Badillo\"\n",
        "format: html\n",
        "date: today\n",
        "order: 2\n",
        "---"
      ],
      "id": "a7b6940f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Redes Neuronales Artificiales\n",
        "\n",
        "Las redes neuronales artificiales son un modelo computacional inspirado en el cerebro humano. Están compuestas por nodos llamados neuronas que están conectados entre sí. Cada conexión entre neuronas tiene un peso asociado que se ajusta durante el entrenamiento del modelo. Estos pesos son los parámetros que se ajustan para que el modelo pueda realizar predicciones, es decir, son la memoria del modelo y representan la importancia de cada conexión.\n",
        "\n",
        "Las redes neuronales artificiales se dividen en capas, cada capa está compuesta por un conjunto de neuronas. La primera capa se llama **capa de entrada**, la última capa se llama **capa de salida** y las capas intermedias se llaman **capas ocultas**. La capa de entrada recibe los datos de entrada, la capa de salida produce la predicción y las capas ocultas procesan la información.\n",
        "\n",
        "![Red Neuronal](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/500px-Artificial_neural_network.svg.png)\n",
        "\n",
        "## Comportamiento de una Neurona\n",
        "\n",
        "Cada neurona recibe una serie de entradas, las multiplica por los pesos asociados a cada conexión y aplica una función de activación. La **función de activación** es una función no lineal que se encarga de introducir no linealidades en el modelo. \n",
        "\n",
        "Matemáticamente, el comportamiento de una neurona se puede expresar de la siguiente forma:\n",
        "\n",
        "$$y = f(\\sum_{i=1}^{n} x_i \\cdot w_i + b)$$\n",
        "\n",
        "Donde $x_i$ son las entradas, $w_i$ son los pesos asociados a cada conexión, $b$ es el sesgo y $f$ es la función de activación. Si usaramos una función de activación lineal, la red neuronal sería equivalente a un modelo de regresión lineal. \n",
        "\n",
        "Entonces podemos usar una red neuronal para regresión lineal:\n",
        "\n",
        "\\begin{align*}\n",
        "y &= f(\\sum_{i=1}^{n} x_i \\cdot w_i + b) \\hspace{1cm} \\text{Donde } f(x) = x \\\\\n",
        "y &= \\sum_{i=1}^{n} x_i \\cdot w_i + b\n",
        "\\end{align*}\n",
        "\n",
        "Existen diversas funciones de activación, algunas de las más comunes son:\n",
        "\n",
        "- **Función Sigmoide**.\n",
        "  $$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "  \n",
        "- **Función ReLU**.\n",
        "  $$f(x) = \\max(0, x)$$\n",
        "\n",
        "- **Función Tangente Hiperbólica**.\n",
        "    $$f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
        "\n",
        "- **Función Softmax**.\n",
        "    $$f(x) = \\frac{e^x}{\\sum_{i=1}^{n} e^{x_i}}$$\n",
        "\n",
        "- **Función de Identidad**.\n",
        "    $$f(x) = x$$\n",
        "\n",
        "Cada función de activación tiene sus propias características y se utiliza en diferentes contextos. Por ejemplo, la función sigmoide se utiliza en la capa de salida de una red neuronal para clasificación binaria, la función ReLU se utiliza en las capas ocultas y la función softmax se utiliza en la capa de salida para clasificación multiclase.\n",
        "\n",
        "Visualicemos el comportamiento de algunas funciones de activación:\n"
      ],
      "id": "d971ec92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(2, 2, figsize=(8, 4), sharex=True)\n",
        "\n",
        "x = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Funciones de activación\n",
        "sigmoid = 1 / (1 + np.exp(-x))\n",
        "relu = np.maximum(0, x)\n",
        "tanh = np.tanh(x)\n",
        "softmax = np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "# Gráficas\n",
        "ax[0, 0].plot(x, sigmoid)\n",
        "ax[0, 0].set_title(\"Función Sigmoide\")\n",
        "\n",
        "ax[0, 1].plot(x, relu)\n",
        "ax[0, 1].set_title(\"Función ReLU\")\n",
        "\n",
        "ax[1, 0].plot(x, tanh)\n",
        "ax[1, 0].set_title(\"Función Tangente Hiperbólica\")\n",
        "\n",
        "ax[1, 1].plot(x, softmax)\n",
        "ax[1, 1].set_title(\"Función Softmax\")\n",
        "\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax[i, j].grid( linestyle='--', linewidth=0.5, alpha=0.5, color='grey')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "f2d042ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hagamos lo que hace una neurona con una función de activación sigmoide.\n"
      ],
      "id": "04f27c1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Función de activación sigmoide\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Datos de entrada\n",
        "X = np.random.randn(10, 1)\n",
        "\n",
        "# Pesos y sesgo\n",
        "np.random.seed(1014)\n",
        "weights = np.random.randn(1, 10)\n",
        "bias = np.random.randn(1)\n",
        "\n",
        "# Salida de la neurona\n",
        "y = sigmoid(np.dot(X.T, weights.T) + bias)\n",
        "\n",
        "print(y)"
      ],
      "id": "96c77193",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estamos realizando la siguiente operación:\n",
        "\n",
        "$$y = f(\\sum_{i=1}^{n} x_i \\cdot w_i + b)$$\n",
        "\n",
        "Donde $f(x) = \\frac{1}{1 + e^{-x}}$ es la función sigmoide. En este caso, estamos utilizando una neurona con 10 entradas y una salida.\n",
        "\n",
        "Lo que busca simular o modelar el comportamiento de una neurona biológica. La neurona biológica recibe señales eléctricas de otras neuronas a través de las dendritas, las procesa en el cuerpo celular y envía una señal eléctrica a través del axón. La señal eléctrica se transmite a través de las sinapsis, que son las conexiones entre las neuronas.\n",
        "\n",
        "Ahora veamos cómo se comporta una red neuronal con una capa oculta y una capa de salida. Para esto, vamos a implementar una red neuronal para regresión lineal con pesos y sesgos aleatorios.\n"
      ],
      "id": "982b3472"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, input_size, hidden_size, output_size, seed=1014):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # Pesos y sesgos\n",
        "        np.random.seed(seed)\n",
        "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "        self.bias_input_hidden = np.random.randn(hidden_size)\n",
        "        \n",
        "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "        self.bias_hidden_output = np.random.randn(output_size)\n",
        "        \n",
        "    def forward(self, X, activation):\n",
        "        # Capa oculta\n",
        "        hidden = np.dot(X, self.weights_input_hidden) + self.bias_input_hidden\n",
        "        hidden = activation(hidden)\n",
        "        \n",
        "        # Capa de salida\n",
        "        output = np.dot(hidden, self.weights_hidden_output) + self.bias_hidden_output\n",
        "        \n",
        "        return output"
      ],
      "id": "2b4b6eab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usemos la red neuronal para predecir un conjunto de datos y con una función de activación identidad.\n"
      ],
      "id": "6d2fdbd0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Datos de entrada\n",
        "X = np.random.randn(10, 1)\n",
        "\n",
        "# Parámetros de la red neuronal\n",
        "input_size = 1\n",
        "hidden_size = 10\n",
        "output_size = 1\n",
        "\n",
        "# Red Neuronal\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "\n",
        "# Función de activación identidad\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "# Predicciones\n",
        "y_pred = nn.forward(X, identity)\n",
        "\n",
        "print(y_pred)"
      ],
      "id": "467d7d95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este caso tenemos una red neuronal con una capa oculta de 10 neuronas y una capa de salida de 1 neurona. La función de activación de la capa oculta es la función identidad y la función de activación de la capa de salida también es la función identidad.\n",
        "\n",
        "Los pesos de la red son:\n"
      ],
      "id": "f4c9f9a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Pesos capa oculta: {nn.weights_input_hidden}\\n\")\n",
        "print(f\"Sesgos capa oculta: {nn.bias_input_hidden}\\n\")\n",
        "print(f\"Pesos capa de salida: {nn.weights_hidden_output.T}\\n\")\n",
        "print(f\"Sesgos capa de salida: {nn.bias_hidden_output}\\n\")"
      ],
      "id": "1062c04c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos dibujar la red neuronal con los pesos y sesgos asociados a cada conexión.\n"
      ],
      "id": "402ab5b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Colores para las capas\n",
        "subset_colors = ['blue', 'red', 'green']\n",
        "\n",
        "def multilayered_graph(input_size, hidden_size, output_size, weights_input_hidden, weights_hidden_output):\n",
        "    # Crear los rangos para las capas\n",
        "    subset_sizes = [input_size, hidden_size, output_size]\n",
        "    extents = nx.utils.pairwise(itertools.accumulate((0,) + tuple(subset_sizes)))\n",
        "    layers = [range(start, end) for start, end in extents]\n",
        "    \n",
        "    # Crear el gráfico\n",
        "    G = nx.Graph()\n",
        "    for i, layer in enumerate(layers):\n",
        "        G.add_nodes_from(layer, layer=i)\n",
        "        \n",
        "    # Añadir los bordes con pesos para capa de entrada a oculta\n",
        "    for i, j in itertools.product(layers[0], layers[1]):\n",
        "        G.add_edge(i, j, weight=round(weights_input_hidden[i, j - layers[1][0]], 3))\n",
        "        \n",
        "    # Añadir los bordes con pesos para capa oculta a salida\n",
        "    for i, j in itertools.product(layers[1], layers[2]):\n",
        "        G.add_edge(i, j, weight=round(weights_hidden_output[i - layers[1][0], j - layers[2][0]], 3))\n",
        "    \n",
        "    return G\n",
        "\n",
        "# Crear el gráfico con los pesos\n",
        "G = multilayered_graph(input_size, hidden_size, output_size, nn.weights_input_hidden, nn.weights_hidden_output)\n",
        "\n",
        "# Colores para los nodos según su capa\n",
        "color = [subset_colors[data[\"layer\"]] for node, data in G.nodes(data=True)]\n",
        "\n",
        "# Posición de los nodos\n",
        "pos = nx.multipartite_layout(G, subset_key=\"layer\")\n",
        "\n",
        "# Dibujar el gráfico\n",
        "plt.figure(figsize=(10, 6))\n",
        "nx.draw(G, pos, with_labels=False, node_color=color, node_size=1500, font_size=10, font_weight='bold')\n",
        "\n",
        "# Dibujar los bordes con los pesos\n",
        "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)\n",
        "\n",
        "plt.show()"
      ],
      "id": "d100b2d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cada una de las neuronas realiza la operación que hemos visto anteriormente. Sin embargo, aquí solo hemos decidido los pesos y sesgos de la red neuronal de forma aleatoria. En la práctica, estos pesos y sesgos se ajustan durante el entrenamiento de la red neuronal, es decir, la red neuronal aprende a partir de los datos.\n",
        "\n",
        "## Entrenamiento de una Red Neuronal\n",
        "\n",
        "El entrenamiento de una red neuronal consiste en ajustar los pesos y sesgos de la red para minimizar una función de pérdida. La función de pérdida mide la diferencia entre las predicciones del modelo y los valores reales. Durante el entrenamiento, los pesos y sesgos se ajustan iterativamente utilizando un algoritmo de optimización.\n",
        "\n",
        "Existen diversos algoritmos de optimización, algunos de los más comunes son:\n",
        "\n",
        "- **Descenso del Gradiente** : Actualiza los pesos en la dirección opuesta al gradiente de la función de pérdida.\n",
        "- **Adam**: Utiliza una combinación de descenso del gradiente y adaptación de la tasa de aprendizaje.\n",
        "- **RMSprop**: Se adapta a la tasa de aprendizaje para cada parámetro.\n",
        "- **Adagrad**: Ajusta la tasa de aprendizaje para cada parámetro en función de la magnitud de los gradientes.\n",
        "\n",
        "El algoritmo tipico es el descenso del gradiente. La idea es ajustar los pesos y sesgos de la red neuronal en la dirección opuesta al gradiente de la función de pérdida. El gradiente de la función de pérdida se calcula utilizando la regla de la cadena y el algoritmo de retropropagación. Pero en la práctica, se utiliza una variante del descenso del gradiente llamada **descenso del gradiente estocástico** o el algoritmo **Adam**.\n",
        "\n",
        "Algunas funciones de pérdida comunes son:\n",
        "\n",
        "- **Error Cuadrático Medio**.\n",
        "  $$L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "- **Entropía Cruzada**.\n",
        "    $$L(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$$\n",
        "\n",
        "- **Error Absoluto Medio**.\n",
        "    $$L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
        "\n",
        "El algoritmo de entrenamiento de una red neuronal se puede resumir en los siguientes pasos:\n",
        "\n",
        "1. Inicializar los pesos y sesgos de la red neuronal.\n",
        "2. Calcular la salida de la red neuronal.\n",
        "3. Calcular la función de pérdida.\n",
        "4. Calcular el gradiente de la función de pérdida.\n",
        "5. Actualizar los pesos y sesgos utilizando un algoritmo de optimización.\n",
        "6. Repetir los pasos 2-5 hasta que se alcance un número de iteraciones o se cumpla un criterio de parada.\n",
        "\n",
        "### Aprendizaje de pesos y sesgos en una neurona\n",
        "\n",
        "Vamos a ver cómo se actualizan los pesos y sesgos de una neurona durante el entrenamiento. Para esto, vamos a implementar una neurona con una función de activación sigmoide y vamos a entrenar la neurona para realizar una regresión lineal y usar la función de pérdida de error cuadrático medio.\n",
        "\n",
        "Para obtener el gradiente de la función de pérdida, vamos a utilizar la regla de la cadena y el algoritmo de retropropagación. La regla de la cadena se utiliza para calcular el gradiente de una función compuesta y el algoritmo de retropropagación se utiliza para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal.\n",
        "\n",
        "#### Regla de la Cadena para el Gradiente\n",
        "\n",
        "La regla de la cadena se utiliza para calcular el gradiente de una función compuesta. Si tenemos una función $f(g(x))$, el gradiente de $f$ con respecto a $x$ se puede calcular como:\n",
        "\n",
        "$$\\frac{\\partial f(g(x))}{\\partial x} = \\frac{\\partial f(g(x))}{\\partial g(x)} \\cdot \\frac{\\partial g(x)}{\\partial x}$$\n",
        "\n",
        "En nuestro caso tenemos una función de pérdida $L(y, \\hat{y})$ y una función de activación $f(x)$. Entonces, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal se puede calcular como:\n",
        "\n",
        "$$\\frac{\\partial L(y, \\hat{y})}{\\partial w_i} = \\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i}$$\n",
        "\n",
        "Donde $z = \\sum_{i=1}^{n} x_i \\cdot w_i + b$ es la entrada de la neurona y $\\hat{y} = f(z)$ es la salida de la neurona.\n",
        "\n",
        "Hagamos la primera parte de la regla de la cadena, es decir, el gradiente de la función de pérdida con respecto a la salida de la neurona.\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} &= \\frac{\\partial}{\\partial \\hat{y}} \\left( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\right) \\\\\n",
        "&= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\hat{y}} (y_i - \\hat{y}_i)^2 \\\\\n",
        "&= \\frac{1}{n} \\sum_{i=1}^{n} -2(y_i - \\hat{y}_i) \\\\\n",
        "&= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i)\n",
        "\\end{align*}\n",
        "\n",
        "La segunda parte de la regla de la cadena es el gradiente de la salida de la neurona con respecto a la entrada de la neurona.\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\hat{y}}{\\partial z} &= \\frac{\\partial}{\\partial z} \\left( \\frac{1}{1 + e^{-z}} \\right) \\\\\n",
        "&= \\frac{e^{-z}}{(1 + e^{-z})^2} \\\\\n",
        "&= \\frac{1}{1 + e^{-z}} \\cdot \\left(1 - \\frac{1}{1 + e^{-z}} \\right) \\\\\n",
        "&= \\hat{y} \\cdot (1 - \\hat{y})\n",
        "\\end{align*}\n",
        "\n",
        "La tercera parte de la regla de la cadena es el gradiente de la entrada de la neurona con respecto a los pesos.\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial z}{\\partial w_i} &= \\frac{\\partial}{\\partial w_i} \\left( \\sum_{i=1}^{n} x_i \\cdot w_i + b \\right) \\\\\n",
        "&= x_i\n",
        "\\end{align*}\n",
        "\n",
        "Ahora para el sesgo.\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial z}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left( \\sum_{i=1}^{n} x_i \\cdot w_i + b \\right) \\\\\n",
        "&= 1\n",
        "\\end{align*}\n",
        "\n",
        "Entonces, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal se puede calcular como:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L(y, \\hat{y})}{\\partial w_i} &= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i) \\cdot \\hat{y}_i \\cdot (1 - \\hat{y}_i) \\cdot x_i \\\\\n",
        "\\frac{\\partial L(y, \\hat{y})}{\\partial b} &= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i) \\cdot \\hat{y}_i \\cdot (1 - \\hat{y}_i)\n",
        "\\end{align*}\n",
        "\n",
        "##### Actualización de los pesos y sesgos\n",
        "\n",
        "Para actualizar los pesos y sesgos de la red neuronal, utilizamos el algoritmo de descenso del gradiente. La actualización de los pesos y sesgos se realiza de la siguiente forma:\n",
        "\n",
        "\\begin{align*}\n",
        "w_i &:= w_i - \\alpha \\cdot \\frac{\\partial L(y, \\hat{y})}{\\partial w_i} \\\\\n",
        "b &:= b - \\alpha \\cdot \\frac{\\partial L(y, \\hat{y})}{\\partial b}\n",
        "\\end{align*}\n",
        "\n",
        "Donde $\\alpha$ es la tasa de aprendizaje, que es un hiperparámetro del modelo. La tasa de aprendizaje controla la magnitud de la actualización de los pesos y sesgos. Si la tasa de aprendizaje es muy pequeña, el modelo puede tardar mucho tiempo en converger. Si la tasa de aprendizaje es muy grande, el modelo puede divergir.\n",
        "\n",
        "#### Retropropagación del Gradiente.\n",
        "\n",
        "La retropropagación es un algoritmo que se utiliza para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal, usamos la regla de la cadena para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal y lo propagamos hacia atrás a través de la red neuronal.\n",
        "\n",
        "Para cada capa de la red neuronal, calculamos el gradiente de la función de pérdida con respecto a los pesos y sesgos de la capa utilizando la regla de la cadena y el gradiente de la capa anterior. Luego, actualizamos los pesos y sesgos de la capa. Este proceso se repite para todas las capas de la red neuronal. Un hermoso gif creado por [Michael Pyrcz](https://x.com/GeostatsGuy/status/1802719780282982832) muestra cómo funciona la retropropagación.\n",
        "\n",
        "![Retropropagación del Gradiente](/img/ann/backpropagation.gif){width=100%}\n",
        "\n",
        "### Implementación de Backpropagation\n",
        "\n",
        "Hagamos una red neuronal con 1 neurona en la capa oculta y 5 neuronas en la capa de salida, usaremos la función de activación sigmoide para la capa oculta y la función de activación identidad para la capa de salida. Vamos a entrenar la red neuronal para realizar una regresión lineal y usaremos la función de pérdida de error cuadrático medio.\n",
        "\n",
        "#### Datos de Entrada\n",
        "\n",
        "Simulemos datos de entrada y salida para entrenar la red neuronal. \n"
      ],
      "id": "ca5be6f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Datos de entrada\n",
        "X = np.random.normal(0, 5, (40, 1))\n",
        "\n",
        "# Datos de salida\n",
        "y = 2 * X + 3 + np.random.normal(0, 1, (40, 1))\n",
        "\n",
        "df = pd.DataFrame(np.concatenate([X, y], axis=1), columns=[\"X\", \"y\"])\n",
        "df.head()"
      ],
      "id": "5773cd79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Funciones auxiliares\n",
        "\n",
        "Vamos a implementar algunas funciones auxiliares para la red neuronal.\n"
      ],
      "id": "dae2ea9f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "# Función de activación sigmoide\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Función de pérdida de error cuadrático medio\n",
        "def mse(y, y_pred):\n",
        "    return np.mean((y - y_pred) ** 2)\n",
        "\n",
        "def weight_derivative_hidden(X, y, y_pred):\n",
        "    \"\"\"\n",
        "    Derivada de los pesos de la capa oculta\n",
        "    \"\"\"\n",
        "    return -2 * np.mean((y - y_pred) * y_pred * (1 - y_pred) * X)\n",
        "\n",
        "def bias_derivative_hidden(y, y_pred):\n",
        "    \"\"\"\n",
        "    Derivada del sesgo de la capa oculta\n",
        "    \"\"\"\n",
        "    return -2 * np.mean((y - y_pred) * y_pred * (1 - y_pred))\n",
        "\n",
        "def weight_derivative_output(hidden, y, y_pred):\n",
        "    \"\"\"\n",
        "    Derivada de los pesos de la capa de salida para la función de pérdida de error cuadrático medio y función de activación identidad\n",
        "    \"\"\"\n",
        "    return -2 * np.mean((y - y_pred) * X)\n",
        "\n",
        "def bias_derivative_output(y, y_pred):\n",
        "    \"\"\"\n",
        "    Derivada del sesgo de la capa de salida para la función de pérdida de error cuadrático medio y función de activación identidad\n",
        "    \"\"\"\n",
        "    return -2 * np.mean(y - y_pred)\n",
        "\n",
        "# Inicialización de los pesos y sesgos\n",
        "def initialize_weights(input_size, hidden_size, output_size, seed=1014):\n",
        "    np.random.seed(seed)\n",
        "    weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "    bias_input_hidden = np.random.randn(hidden_size)\n",
        "    \n",
        "    weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "    bias_hidden_output = np.random.randn(output_size)\n",
        "    \n",
        "    return weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output"
      ],
      "id": "d5814073",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Entrenamiento de la Red Neuronal\n",
        "\n",
        "Vamos a entrenar la red neuronal utilizando el algoritmo de retropropagación. Durante el entrenamiento, vamos a calcular la función de pérdida, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal y vamos a actualizar los pesos y sesgos utilizando el algoritmo de descenso del gradiente.\n"
      ],
      "id": "2ccf1480"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def learning( X, y, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, learning_rate=0.01):\n",
        "    # Capa oculta\n",
        "    hidden = np.dot(X, weights_input_hidden) + bias_input_hidden\n",
        "    hidden = sigmoid(hidden)\n",
        "    \n",
        "    # Capa de salida\n",
        "    output = np.dot(hidden, weights_hidden_output) + bias_hidden_output\n",
        "    \n",
        "    # Función de pérdida\n",
        "    loss = mse(y, output)\n",
        "    \n",
        "    # Gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal\n",
        "    weight_der_hidden = weight_derivative_hidden(X, y, output)\n",
        "    bias_der_hidden = bias_derivative_hidden(y, output)\n",
        "    \n",
        "    weight_der_output = weight_derivative_output(hidden, y, output)\n",
        "    bias_der_output = bias_derivative_output(y, output)\n",
        "    \n",
        "    # Actualización de los pesos y sesgos\n",
        "    weights_input_hidden -= learning_rate * weight_der_hidden\n",
        "    bias_input_hidden -= learning_rate * bias_der_hidden\n",
        "    \n",
        "    weights_hidden_output -= learning_rate * weight_der_output\n",
        "    bias_hidden_output -= learning_rate * bias_der_output\n",
        "    \n",
        "    return weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, loss\n",
        "\n",
        "# Creamos un bucle para entrenar la red neuronal\n",
        "\n",
        "input_size = 1\n",
        "hidden_size = 5\n",
        "output_size = 1\n",
        "\n",
        "weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output = initialize_weights(input_size, hidden_size, output_size)\n",
        "\n",
        "learning_rate = 0.0001\n",
        "epochs = 1500\n",
        "\n",
        "X_normalized = (X - X.mean()) / X.std()\n",
        "Y_normalized = (y - y.mean()) / y.std()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, loss = learning(\n",
        "    X_normalized, Y_normalized, \n",
        "    weights_input_hidden, \n",
        "    bias_input_hidden, \n",
        "    weights_hidden_output, \n",
        "    bias_hidden_output, \n",
        "    learning_rate)\n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss {loss}\")\n",
        "    elif epoch == epochs - 1:\n",
        "        print(f\"Epoch {epoch}: Loss {loss}\")"
      ],
      "id": "8def5bd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predicciones de la Red Neuronal\n",
        "\n",
        "Una ves que hemos entrenado la red neuronal, podemos hacer predicciones con la red neuronal que es el objetivo de crear un modelo de aprendizaje automático."
      ],
      "id": "4e16f123"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def predict(X, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output):\n",
        "    hidden = np.dot(X, weights_input_hidden) + bias_input_hidden\n",
        "    hidden = sigmoid(hidden)\n",
        "    \n",
        "    output = np.dot(hidden, weights_hidden_output) + bias_hidden_output\n",
        "    \n",
        "    return output\n",
        "\n",
        "y_pred = predict(X_normalized, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output)\n",
        "\n",
        "df_normalized = pd.DataFrame(np.concatenate([X_normalized, Y_normalized, y_pred], axis=1), columns=[\"X\", \"y\", \"y_pred\"])\n",
        "\n",
        "df_normalized.head()"
      ],
      "id": "e500b199",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A nivel profesional, se utilizan librerías como TensorFlow, PyTorch o Keras para implementar redes neuronales. Estas librerías proporcionan una interfaz de alto nivel para construir y entrenar redes neuronales de forma eficiente. Sin embargo, es importante entender cómo funcionan las redes neuronales a nivel de bajo nivel para poder depurar y optimizar los modelos.\n",
        "\n",
        "# Otros tipos de Redes Neuronales\n",
        "\n",
        "Hasta ahora hemos visto cómo funcionan las redes neuronales artificiales y cómo se entrenan utilizando el algoritmo de retropropagación. Sin embargo, existen otros tipos de redes neuronales que se utilizan para diferentes tareas de aprendizaje automático. Algunos de los tipos de redes neuronales más comunes son:\n",
        "\n",
        "- **Redes Neuronales Convolucionales (CNN)**: Se utilizan para tareas de visión por computadora, como clasificación de imágenes, detección de objetos y segmentación semántica.\n",
        "- **Redes Neuronales Recurrentes (RNN)**: Se utilizan para tareas de procesamiento de lenguaje natural, como traducción automática, generación de texto y análisis de sentimientos.\n",
        "- **Redes Neuronales Generativas Adversarias (GAN)**: Se utilizan para generar datos sintéticos, como imágenes, texto y audio.\n",
        "- **Transformers**: Se utilizan para tareas de procesamiento de lenguaje natural, como traducción automática, generación de texto y análisis de sentimientos.\n",
        "- **Encoders y Decoders**: Se utilizan para tareas de traducción automática, generación de texto y resumen de texto.\n",
        "\n",
        "Cada tipo de red neuronal tiene sus propias características y se utiliza en diferentes contextos. Algunas redes son más sencillos de entrenar y otras son más complejas. Por ejemplo, las redes neuronales convolucionales son más sencillas de entrenar que las redes neuronales recurrentes, ya que las redes convolucionales no tienen dependencias temporales."
      ],
      "id": "b4909684"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}